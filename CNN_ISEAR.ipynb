{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "ISEAR=./data/isear.csv\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "TWEETS=./data/text_emotion.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "# =================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "# tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "# tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "# tf.flags.DEFINE_string(\"tweets\",\"./data/text_emotion.csv\", \"Data source from crowdflower\")\n",
    "tf.flags.DEFINE_string(\"isear\",\"./data/isear.csv\", \"Data source from ISEAR\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "# tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "# tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "# tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "# tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "# tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "# tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(FLAGS.isear,sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COUN</th>\n",
       "      <th>SUBJ</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>RELI</th>\n",
       "      <th>PRAC</th>\n",
       "      <th>FOCC</th>\n",
       "      <th>MOCC</th>\n",
       "      <th>...</th>\n",
       "      <th>STATE</th>\n",
       "      <th>Unnamed: 42</th>\n",
       "      <th>Unnamed: 43</th>\n",
       "      <th>Unnamed: 44</th>\n",
       "      <th>Unnamed: 45</th>\n",
       "      <th>Unnamed: 46</th>\n",
       "      <th>Unnamed: 47</th>\n",
       "      <th>Unnamed: 48</th>\n",
       "      <th>Unnamed: 49</th>\n",
       "      <th>Unnamed: 50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  CITY  COUN  SUBJ  SEX  AGE  RELI  PRAC  FOCC  MOCC     ...       \\\n",
       "0  11001     1     1     1    1   33     1     2     6     1     ...        \n",
       "1  11001     1     1     1    1   33     1     2     6     1     ...        \n",
       "2  11001     1     1     1    1   33     1     2     6     1     ...        \n",
       "3  11001     1     1     1    1   33     1     2     6     1     ...        \n",
       "4  11001     1     1     1    1   33     1     2     6     1     ...        \n",
       "\n",
       "   STATE  Unnamed: 42  Unnamed: 43  Unnamed: 44  Unnamed: 45  Unnamed: 46  \\\n",
       "0      1          NaN          NaN          NaN          NaN          NaN   \n",
       "1      1          NaN          NaN          NaN          NaN          NaN   \n",
       "2      1          NaN          NaN          NaN          NaN          NaN   \n",
       "3      1          NaN          NaN          NaN          NaN          NaN   \n",
       "4      1          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 47  Unnamed: 48  Unnamed: 49  Unnamed: 50  \n",
       "0          NaN          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text = data[\"SIT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Field1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    data['Field1'] = data['Field1'].map({'joy':[1,0,0,0,0,0,0], 'fear':[0,1,0,0,0,0,0], 'anger':[0,0,1,0,0,0,0], 'sadness':[0,0,0,1,0,0,0], 'disgust':[0,0,0,0,1,0,0], 'shame':[0,0,0,0,0,1,0], 'guilt':[0,0,0,0,0,0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['Field1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During the period of falling in love, each time that we met and \\xc3\\xa1 especially when we had not met for a long time.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text=[data_helpers.clean_str(sent) for sent in x_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'during the period of falling in love , each time that we met and especially when we had not met for a long time'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text) ))\n",
    "y = np.array(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3, ...,   0,   0,   0],\n",
       "       [ 15,  21,  22, ...,   0,   0,   0],\n",
       "       [ 15,  21,  22, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [ 21,  22,  38, ...,   0,   0,   0],\n",
       "       [ 21,  72,  17, ...,   0,   0,   0],\n",
       "       [ 21,  16, 254, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15,  116,  949, ...,    0,    0,    0],\n",
       "       [ 116, 3968,  115, ...,    0,    0,    0],\n",
       "       [  33,   22,   19, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [  15,   21,   22, ...,    0,    0,    0],\n",
       "       [ 165,   37,   22, ...,    0,    0,    0],\n",
       "       [  95, 1012,  111, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "Vocabulary Size: 8927\n",
      "Train/Dev split: 6900/766\n"
     ]
    }
   ],
   "source": [
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print x_train.shape[1]\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data helper function\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ubuntu/cnn-text-classification-tf/runs/1491237569\n",
      "\n",
      "2017-04-03T22:09:30.417450: step 1, loss 6.31757, acc 0.171875\n",
      "2017-04-03T22:09:30.612363: step 2, loss 5.52489, acc 0.140625\n",
      "2017-04-03T22:09:30.811297: step 3, loss 4.40588, acc 0.09375\n",
      "2017-04-03T22:09:31.006075: step 4, loss 5.29336, acc 0.125\n",
      "2017-04-03T22:09:31.203784: step 5, loss 5.12952, acc 0.140625\n",
      "2017-04-03T22:09:31.394841: step 6, loss 5.03943, acc 0.15625\n",
      "2017-04-03T22:09:31.592158: step 7, loss 4.94861, acc 0.125\n",
      "2017-04-03T22:09:31.783684: step 8, loss 4.86245, acc 0.109375\n",
      "2017-04-03T22:09:31.980101: step 9, loss 4.46962, acc 0.171875\n",
      "2017-04-03T22:09:32.172149: step 10, loss 5.34941, acc 0.109375\n",
      "2017-04-03T22:09:32.369979: step 11, loss 4.29757, acc 0.1875\n",
      "2017-04-03T22:09:32.557327: step 12, loss 4.24104, acc 0.140625\n",
      "2017-04-03T22:09:32.755123: step 13, loss 4.99609, acc 0.140625\n",
      "2017-04-03T22:09:32.947285: step 14, loss 3.9522, acc 0.171875\n",
      "2017-04-03T22:09:33.145156: step 15, loss 4.98548, acc 0.109375\n",
      "2017-04-03T22:09:33.336801: step 16, loss 4.60483, acc 0.140625\n",
      "2017-04-03T22:09:33.531460: step 17, loss 5.1359, acc 0.09375\n",
      "2017-04-03T22:09:33.723232: step 18, loss 4.77246, acc 0.125\n",
      "2017-04-03T22:09:33.920091: step 19, loss 4.90526, acc 0.125\n",
      "2017-04-03T22:09:34.115825: step 20, loss 4.16124, acc 0.203125\n",
      "2017-04-03T22:09:34.312664: step 21, loss 3.46177, acc 0.25\n",
      "2017-04-03T22:09:34.508094: step 22, loss 3.94947, acc 0.140625\n",
      "2017-04-03T22:09:34.708110: step 23, loss 4.99162, acc 0.09375\n",
      "2017-04-03T22:09:34.905052: step 24, loss 4.65177, acc 0.125\n",
      "2017-04-03T22:09:35.106446: step 25, loss 5.05608, acc 0.140625\n",
      "2017-04-03T22:09:35.301833: step 26, loss 4.54154, acc 0.15625\n",
      "2017-04-03T22:09:35.500426: step 27, loss 4.43382, acc 0.125\n",
      "2017-04-03T22:09:35.694274: step 28, loss 4.32089, acc 0.171875\n",
      "2017-04-03T22:09:35.895874: step 29, loss 4.49447, acc 0.140625\n",
      "2017-04-03T22:09:36.109747: step 30, loss 4.51076, acc 0.15625\n",
      "2017-04-03T22:09:36.318242: step 31, loss 3.39538, acc 0.1875\n",
      "2017-04-03T22:09:36.521494: step 32, loss 4.06895, acc 0.234375\n",
      "2017-04-03T22:09:36.729411: step 33, loss 4.26241, acc 0.1875\n",
      "2017-04-03T22:09:36.938779: step 34, loss 4.02316, acc 0.203125\n",
      "2017-04-03T22:09:37.143992: step 35, loss 4.47394, acc 0.0625\n",
      "2017-04-03T22:09:37.352152: step 36, loss 4.37727, acc 0.203125\n",
      "2017-04-03T22:09:37.559353: step 37, loss 4.63078, acc 0.125\n",
      "2017-04-03T22:09:37.756498: step 38, loss 4.18932, acc 0.171875\n",
      "2017-04-03T22:09:37.955221: step 39, loss 4.03498, acc 0.15625\n",
      "2017-04-03T22:09:38.150600: step 40, loss 4.04702, acc 0.1875\n",
      "2017-04-03T22:09:38.350806: step 41, loss 4.11856, acc 0.140625\n",
      "2017-04-03T22:09:38.541822: step 42, loss 4.01535, acc 0.125\n",
      "2017-04-03T22:09:38.748289: step 43, loss 4.06309, acc 0.109375\n",
      "2017-04-03T22:09:38.944822: step 44, loss 3.85314, acc 0.15625\n",
      "2017-04-03T22:09:39.146439: step 45, loss 4.56804, acc 0.09375\n",
      "2017-04-03T22:09:39.338483: step 46, loss 4.34877, acc 0.1875\n",
      "2017-04-03T22:09:39.536957: step 47, loss 3.78363, acc 0.25\n",
      "2017-04-03T22:09:39.728947: step 48, loss 4.89148, acc 0.125\n",
      "2017-04-03T22:09:39.934555: step 49, loss 3.84067, acc 0.171875\n",
      "2017-04-03T22:09:40.130756: step 50, loss 4.89882, acc 0.09375\n",
      "2017-04-03T22:09:40.334471: step 51, loss 4.62946, acc 0.125\n",
      "2017-04-03T22:09:40.541396: step 52, loss 3.062, acc 0.21875\n",
      "2017-04-03T22:09:40.734511: step 53, loss 4.31597, acc 0.171875\n",
      "2017-04-03T22:09:40.928527: step 54, loss 3.4922, acc 0.1875\n",
      "2017-04-03T22:09:41.121577: step 55, loss 4.11308, acc 0.15625\n",
      "2017-04-03T22:09:41.319669: step 56, loss 4.10361, acc 0.25\n",
      "2017-04-03T22:09:41.509886: step 57, loss 3.65192, acc 0.1875\n",
      "2017-04-03T22:09:41.710931: step 58, loss 3.90012, acc 0.21875\n",
      "2017-04-03T22:09:41.902435: step 59, loss 3.70141, acc 0.203125\n",
      "2017-04-03T22:09:42.104049: step 60, loss 3.87753, acc 0.203125\n",
      "2017-04-03T22:09:42.304236: step 61, loss 3.82603, acc 0.265625\n",
      "2017-04-03T22:09:42.507850: step 62, loss 4.02697, acc 0.203125\n",
      "2017-04-03T22:09:42.697331: step 63, loss 3.72779, acc 0.21875\n",
      "2017-04-03T22:09:42.899881: step 64, loss 4.08793, acc 0.140625\n",
      "2017-04-03T22:09:43.090137: step 65, loss 3.71453, acc 0.15625\n",
      "2017-04-03T22:09:43.289553: step 66, loss 3.1659, acc 0.1875\n",
      "2017-04-03T22:09:43.477345: step 67, loss 3.99712, acc 0.15625\n",
      "2017-04-03T22:09:43.681771: step 68, loss 3.8316, acc 0.21875\n",
      "2017-04-03T22:09:43.883674: step 69, loss 3.65301, acc 0.140625\n",
      "2017-04-03T22:09:44.079233: step 70, loss 3.55398, acc 0.15625\n",
      "2017-04-03T22:09:44.274870: step 71, loss 3.58243, acc 0.125\n",
      "2017-04-03T22:09:44.482761: step 72, loss 3.12885, acc 0.171875\n",
      "2017-04-03T22:09:44.683721: step 73, loss 3.30275, acc 0.203125\n",
      "2017-04-03T22:09:44.878481: step 74, loss 3.97013, acc 0.09375\n",
      "2017-04-03T22:09:45.076906: step 75, loss 3.3903, acc 0.1875\n",
      "2017-04-03T22:09:45.271208: step 76, loss 3.86791, acc 0.140625\n",
      "2017-04-03T22:09:45.472145: step 77, loss 3.19507, acc 0.140625\n",
      "2017-04-03T22:09:45.664916: step 78, loss 3.27527, acc 0.15625\n",
      "2017-04-03T22:09:45.863921: step 79, loss 3.61758, acc 0.234375\n",
      "2017-04-03T22:09:46.056861: step 80, loss 3.91107, acc 0.15625\n",
      "2017-04-03T22:09:46.255085: step 81, loss 3.37214, acc 0.140625\n",
      "2017-04-03T22:09:46.449234: step 82, loss 4.08145, acc 0.171875\n",
      "2017-04-03T22:09:46.654533: step 83, loss 3.67399, acc 0.265625\n",
      "2017-04-03T22:09:46.848197: step 84, loss 3.3521, acc 0.25\n",
      "2017-04-03T22:09:47.054271: step 85, loss 4.62798, acc 0.140625\n",
      "2017-04-03T22:09:47.245687: step 86, loss 3.84552, acc 0.15625\n",
      "2017-04-03T22:09:47.442460: step 87, loss 3.23449, acc 0.21875\n",
      "2017-04-03T22:09:47.636524: step 88, loss 3.15002, acc 0.234375\n",
      "2017-04-03T22:09:47.833504: step 89, loss 3.26863, acc 0.203125\n",
      "2017-04-03T22:09:48.026749: step 90, loss 3.05911, acc 0.234375\n",
      "2017-04-03T22:09:48.225015: step 91, loss 2.93255, acc 0.25\n",
      "2017-04-03T22:09:48.422019: step 92, loss 3.13216, acc 0.171875\n",
      "2017-04-03T22:09:48.618077: step 93, loss 3.09252, acc 0.28125\n",
      "2017-04-03T22:09:48.810537: step 94, loss 3.50793, acc 0.21875\n",
      "2017-04-03T22:09:49.009804: step 95, loss 3.13347, acc 0.25\n",
      "2017-04-03T22:09:49.198649: step 96, loss 3.79611, acc 0.125\n",
      "2017-04-03T22:09:49.393409: step 97, loss 3.30461, acc 0.1875\n",
      "2017-04-03T22:09:49.586168: step 98, loss 3.25877, acc 0.140625\n",
      "2017-04-03T22:09:49.791487: step 99, loss 3.28596, acc 0.21875\n",
      "2017-04-03T22:09:49.984901: step 100, loss 3.46526, acc 0.203125\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:09:50.643114: step 100, loss 1.84679, acc 0.292428\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-100\n",
      "\n",
      "2017-04-03T22:09:50.894739: step 101, loss 3.50848, acc 0.078125\n",
      "2017-04-03T22:09:51.085009: step 102, loss 3.02701, acc 0.25\n",
      "2017-04-03T22:09:51.276806: step 103, loss 3.25633, acc 0.265625\n",
      "2017-04-03T22:09:51.473327: step 104, loss 3.86918, acc 0.203125\n",
      "2017-04-03T22:09:51.669674: step 105, loss 3.03386, acc 0.234375\n",
      "2017-04-03T22:09:51.860621: step 106, loss 3.58037, acc 0.203125\n",
      "2017-04-03T22:09:52.057551: step 107, loss 3.69763, acc 0.171875\n",
      "2017-04-03T22:09:52.232321: step 108, loss 3.78935, acc 0.115385\n",
      "2017-04-03T22:09:52.435060: step 109, loss 2.72014, acc 0.25\n",
      "2017-04-03T22:09:52.634444: step 110, loss 3.03373, acc 0.25\n",
      "2017-04-03T22:09:52.838275: step 111, loss 3.03759, acc 0.171875\n",
      "2017-04-03T22:09:53.036547: step 112, loss 3.85321, acc 0.109375\n",
      "2017-04-03T22:09:53.241469: step 113, loss 2.92257, acc 0.25\n",
      "2017-04-03T22:09:53.445579: step 114, loss 2.8692, acc 0.25\n",
      "2017-04-03T22:09:53.648252: step 115, loss 3.04458, acc 0.21875\n",
      "2017-04-03T22:09:53.849830: step 116, loss 3.09537, acc 0.265625\n",
      "2017-04-03T22:09:54.041415: step 117, loss 3.05506, acc 0.265625\n",
      "2017-04-03T22:09:54.238786: step 118, loss 2.6366, acc 0.28125\n",
      "2017-04-03T22:09:54.430898: step 119, loss 3.02313, acc 0.203125\n",
      "2017-04-03T22:09:54.625577: step 120, loss 2.84284, acc 0.28125\n",
      "2017-04-03T22:09:54.817880: step 121, loss 3.01382, acc 0.203125\n",
      "2017-04-03T22:09:55.026303: step 122, loss 2.75157, acc 0.328125\n",
      "2017-04-03T22:09:55.226808: step 123, loss 3.24024, acc 0.1875\n",
      "2017-04-03T22:09:55.419010: step 124, loss 3.06422, acc 0.203125\n",
      "2017-04-03T22:09:55.608188: step 125, loss 2.96716, acc 0.234375\n",
      "2017-04-03T22:09:55.801336: step 126, loss 3.2546, acc 0.21875\n",
      "2017-04-03T22:09:55.991986: step 127, loss 3.22572, acc 0.25\n",
      "2017-04-03T22:09:56.188020: step 128, loss 2.76678, acc 0.296875\n",
      "2017-04-03T22:09:56.378773: step 129, loss 2.64049, acc 0.28125\n",
      "2017-04-03T22:09:56.573931: step 130, loss 2.17884, acc 0.3125\n",
      "2017-04-03T22:09:56.763724: step 131, loss 2.30213, acc 0.34375\n",
      "2017-04-03T22:09:56.969995: step 132, loss 2.56292, acc 0.34375\n",
      "2017-04-03T22:09:57.171513: step 133, loss 3.1777, acc 0.1875\n",
      "2017-04-03T22:09:57.379538: step 134, loss 2.76471, acc 0.234375\n",
      "2017-04-03T22:09:57.581137: step 135, loss 2.46698, acc 0.28125\n",
      "2017-04-03T22:09:57.788381: step 136, loss 2.50118, acc 0.28125\n",
      "2017-04-03T22:09:57.985685: step 137, loss 3.1988, acc 0.21875\n",
      "2017-04-03T22:09:58.182500: step 138, loss 3.12859, acc 0.234375\n",
      "2017-04-03T22:09:58.388236: step 139, loss 3.4944, acc 0.1875\n",
      "2017-04-03T22:09:58.593202: step 140, loss 2.96601, acc 0.328125\n",
      "2017-04-03T22:09:58.792101: step 141, loss 3.07695, acc 0.15625\n",
      "2017-04-03T22:09:58.994763: step 142, loss 2.94745, acc 0.203125\n",
      "2017-04-03T22:09:59.203003: step 143, loss 3.06668, acc 0.1875\n",
      "2017-04-03T22:09:59.405328: step 144, loss 2.17597, acc 0.34375\n",
      "2017-04-03T22:09:59.607733: step 145, loss 2.73631, acc 0.265625\n",
      "2017-04-03T22:09:59.805412: step 146, loss 2.94467, acc 0.203125\n",
      "2017-04-03T22:10:00.011174: step 147, loss 2.39974, acc 0.265625\n",
      "2017-04-03T22:10:00.209471: step 148, loss 3.00829, acc 0.21875\n",
      "2017-04-03T22:10:00.401757: step 149, loss 2.7445, acc 0.234375\n",
      "2017-04-03T22:10:00.594593: step 150, loss 2.36896, acc 0.296875\n",
      "2017-04-03T22:10:00.793706: step 151, loss 2.73277, acc 0.203125\n",
      "2017-04-03T22:10:00.993214: step 152, loss 2.69337, acc 0.265625\n",
      "2017-04-03T22:10:01.200884: step 153, loss 2.75521, acc 0.28125\n",
      "2017-04-03T22:10:01.395402: step 154, loss 2.64282, acc 0.265625\n",
      "2017-04-03T22:10:01.589078: step 155, loss 2.51116, acc 0.25\n",
      "2017-04-03T22:10:01.783828: step 156, loss 2.39079, acc 0.359375\n",
      "2017-04-03T22:10:01.980703: step 157, loss 3.09865, acc 0.21875\n",
      "2017-04-03T22:10:02.169592: step 158, loss 2.47897, acc 0.28125\n",
      "2017-04-03T22:10:02.373438: step 159, loss 2.67046, acc 0.1875\n",
      "2017-04-03T22:10:02.570505: step 160, loss 2.20875, acc 0.328125\n",
      "2017-04-03T22:10:02.770326: step 161, loss 2.76118, acc 0.1875\n",
      "2017-04-03T22:10:02.960351: step 162, loss 2.84211, acc 0.3125\n",
      "2017-04-03T22:10:03.157564: step 163, loss 2.81729, acc 0.21875\n",
      "2017-04-03T22:10:03.355184: step 164, loss 3.04823, acc 0.203125\n",
      "2017-04-03T22:10:03.550539: step 165, loss 2.74923, acc 0.28125\n",
      "2017-04-03T22:10:03.745425: step 166, loss 2.5444, acc 0.28125\n",
      "2017-04-03T22:10:03.946983: step 167, loss 2.66171, acc 0.296875\n",
      "2017-04-03T22:10:04.143684: step 168, loss 2.68766, acc 0.203125\n",
      "2017-04-03T22:10:04.342315: step 169, loss 2.04497, acc 0.265625\n",
      "2017-04-03T22:10:04.539549: step 170, loss 2.75882, acc 0.234375\n",
      "2017-04-03T22:10:04.738503: step 171, loss 2.63079, acc 0.265625\n",
      "2017-04-03T22:10:04.930788: step 172, loss 2.5093, acc 0.234375\n",
      "2017-04-03T22:10:05.129505: step 173, loss 2.72258, acc 0.28125\n",
      "2017-04-03T22:10:05.323645: step 174, loss 2.51124, acc 0.296875\n",
      "2017-04-03T22:10:05.517416: step 175, loss 2.69923, acc 0.21875\n",
      "2017-04-03T22:10:05.710159: step 176, loss 2.78736, acc 0.203125\n",
      "2017-04-03T22:10:05.912165: step 177, loss 2.38155, acc 0.3125\n",
      "2017-04-03T22:10:06.103728: step 178, loss 3.12065, acc 0.125\n",
      "2017-04-03T22:10:06.299559: step 179, loss 2.13297, acc 0.421875\n",
      "2017-04-03T22:10:06.489287: step 180, loss 2.52717, acc 0.265625\n",
      "2017-04-03T22:10:06.685476: step 181, loss 2.32094, acc 0.21875\n",
      "2017-04-03T22:10:06.875374: step 182, loss 2.61441, acc 0.265625\n",
      "2017-04-03T22:10:07.077776: step 183, loss 2.79401, acc 0.203125\n",
      "2017-04-03T22:10:07.269535: step 184, loss 2.58243, acc 0.328125\n",
      "2017-04-03T22:10:07.468356: step 185, loss 2.39556, acc 0.234375\n",
      "2017-04-03T22:10:07.662304: step 186, loss 2.43942, acc 0.234375\n",
      "2017-04-03T22:10:07.856295: step 187, loss 2.31769, acc 0.375\n",
      "2017-04-03T22:10:08.047736: step 188, loss 2.34534, acc 0.3125\n",
      "2017-04-03T22:10:08.243185: step 189, loss 3.1605, acc 0.203125\n",
      "2017-04-03T22:10:08.432996: step 190, loss 2.56615, acc 0.359375\n",
      "2017-04-03T22:10:08.627921: step 191, loss 2.15829, acc 0.34375\n",
      "2017-04-03T22:10:08.822431: step 192, loss 2.11897, acc 0.34375\n",
      "2017-04-03T22:10:09.018136: step 193, loss 2.54078, acc 0.25\n",
      "2017-04-03T22:10:09.211303: step 194, loss 2.57188, acc 0.265625\n",
      "2017-04-03T22:10:09.432987: step 195, loss 2.24537, acc 0.28125\n",
      "2017-04-03T22:10:09.630578: step 196, loss 2.66203, acc 0.28125\n",
      "2017-04-03T22:10:09.836148: step 197, loss 2.22703, acc 0.3125\n",
      "2017-04-03T22:10:10.034601: step 198, loss 2.78418, acc 0.203125\n",
      "2017-04-03T22:10:10.235125: step 199, loss 2.4805, acc 0.21875\n",
      "2017-04-03T22:10:10.443030: step 200, loss 2.44699, acc 0.296875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:10:11.103143: step 200, loss 1.66228, acc 0.370757\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-200\n",
      "\n",
      "2017-04-03T22:10:11.364724: step 201, loss 2.78216, acc 0.25\n",
      "2017-04-03T22:10:11.574641: step 202, loss 2.71525, acc 0.234375\n",
      "2017-04-03T22:10:11.771008: step 203, loss 2.13086, acc 0.296875\n",
      "2017-04-03T22:10:11.967348: step 204, loss 2.08113, acc 0.390625\n",
      "2017-04-03T22:10:12.179943: step 205, loss 2.48781, acc 0.296875\n",
      "2017-04-03T22:10:12.399663: step 206, loss 2.57562, acc 0.328125\n",
      "2017-04-03T22:10:12.608714: step 207, loss 2.48805, acc 0.3125\n",
      "2017-04-03T22:10:12.832343: step 208, loss 2.56178, acc 0.359375\n",
      "2017-04-03T22:10:13.050706: step 209, loss 2.61503, acc 0.28125\n",
      "2017-04-03T22:10:13.262293: step 210, loss 2.08567, acc 0.359375\n",
      "2017-04-03T22:10:13.470193: step 211, loss 2.21505, acc 0.25\n",
      "2017-04-03T22:10:13.682041: step 212, loss 2.7481, acc 0.265625\n",
      "2017-04-03T22:10:13.905661: step 213, loss 2.29785, acc 0.296875\n",
      "2017-04-03T22:10:14.114798: step 214, loss 2.2165, acc 0.265625\n",
      "2017-04-03T22:10:14.325949: step 215, loss 2.80229, acc 0.203125\n",
      "2017-04-03T22:10:14.510080: step 216, loss 2.20277, acc 0.307692\n",
      "2017-04-03T22:10:14.731151: step 217, loss 2.00383, acc 0.390625\n",
      "2017-04-03T22:10:14.947518: step 218, loss 2.12133, acc 0.390625\n",
      "2017-04-03T22:10:15.158114: step 219, loss 1.9229, acc 0.328125\n",
      "2017-04-03T22:10:15.380652: step 220, loss 1.85291, acc 0.40625\n",
      "2017-04-03T22:10:15.603418: step 221, loss 1.9541, acc 0.265625\n",
      "2017-04-03T22:10:15.822527: step 222, loss 2.17479, acc 0.375\n",
      "2017-04-03T22:10:16.045013: step 223, loss 1.8385, acc 0.359375\n",
      "2017-04-03T22:10:16.255209: step 224, loss 2.14113, acc 0.375\n",
      "2017-04-03T22:10:16.473305: step 225, loss 2.17599, acc 0.328125\n",
      "2017-04-03T22:10:16.691488: step 226, loss 1.73559, acc 0.453125\n",
      "2017-04-03T22:10:16.886182: step 227, loss 2.06413, acc 0.296875\n",
      "2017-04-03T22:10:17.085047: step 228, loss 2.28554, acc 0.390625\n",
      "2017-04-03T22:10:17.284308: step 229, loss 2.35666, acc 0.296875\n",
      "2017-04-03T22:10:17.487027: step 230, loss 2.27368, acc 0.265625\n",
      "2017-04-03T22:10:17.695084: step 231, loss 2.23827, acc 0.296875\n",
      "2017-04-03T22:10:17.896592: step 232, loss 2.25731, acc 0.34375\n",
      "2017-04-03T22:10:18.093509: step 233, loss 1.76842, acc 0.40625\n",
      "2017-04-03T22:10:18.306781: step 234, loss 1.96363, acc 0.390625\n",
      "2017-04-03T22:10:18.504028: step 235, loss 2.27854, acc 0.421875\n",
      "2017-04-03T22:10:18.698393: step 236, loss 1.84122, acc 0.453125\n",
      "2017-04-03T22:10:18.899450: step 237, loss 1.93516, acc 0.453125\n",
      "2017-04-03T22:10:19.093559: step 238, loss 2.23888, acc 0.34375\n",
      "2017-04-03T22:10:19.287271: step 239, loss 2.5099, acc 0.328125\n",
      "2017-04-03T22:10:19.481320: step 240, loss 2.35842, acc 0.265625\n",
      "2017-04-03T22:10:19.673257: step 241, loss 2.15275, acc 0.328125\n",
      "2017-04-03T22:10:19.865504: step 242, loss 2.09728, acc 0.34375\n",
      "2017-04-03T22:10:20.055441: step 243, loss 2.13563, acc 0.34375\n",
      "2017-04-03T22:10:20.246568: step 244, loss 2.18557, acc 0.328125\n",
      "2017-04-03T22:10:20.439896: step 245, loss 2.22695, acc 0.3125\n",
      "2017-04-03T22:10:20.664232: step 246, loss 1.99784, acc 0.34375\n",
      "2017-04-03T22:10:20.869477: step 247, loss 2.07223, acc 0.359375\n",
      "2017-04-03T22:10:21.061109: step 248, loss 2.38755, acc 0.21875\n",
      "2017-04-03T22:10:21.256957: step 249, loss 2.2875, acc 0.390625\n",
      "2017-04-03T22:10:21.471296: step 250, loss 2.56905, acc 0.265625\n",
      "2017-04-03T22:10:21.665368: step 251, loss 2.4121, acc 0.3125\n",
      "2017-04-03T22:10:21.858660: step 252, loss 2.64005, acc 0.203125\n",
      "2017-04-03T22:10:22.051752: step 253, loss 2.35892, acc 0.328125\n",
      "2017-04-03T22:10:22.256399: step 254, loss 2.14325, acc 0.421875\n",
      "2017-04-03T22:10:22.461820: step 255, loss 2.04871, acc 0.25\n",
      "2017-04-03T22:10:22.666386: step 256, loss 2.21147, acc 0.3125\n",
      "2017-04-03T22:10:22.862099: step 257, loss 2.02382, acc 0.421875\n",
      "2017-04-03T22:10:23.058957: step 258, loss 1.75415, acc 0.375\n",
      "2017-04-03T22:10:23.248357: step 259, loss 1.94203, acc 0.375\n",
      "2017-04-03T22:10:23.444698: step 260, loss 2.00822, acc 0.328125\n",
      "2017-04-03T22:10:23.636517: step 261, loss 1.86112, acc 0.4375\n",
      "2017-04-03T22:10:23.827635: step 262, loss 1.86136, acc 0.4375\n",
      "2017-04-03T22:10:24.019894: step 263, loss 2.03542, acc 0.28125\n",
      "2017-04-03T22:10:24.214470: step 264, loss 1.9074, acc 0.359375\n",
      "2017-04-03T22:10:24.415442: step 265, loss 2.16435, acc 0.328125\n",
      "2017-04-03T22:10:24.609861: step 266, loss 1.89505, acc 0.4375\n",
      "2017-04-03T22:10:24.803297: step 267, loss 2.75013, acc 0.234375\n",
      "2017-04-03T22:10:24.998796: step 268, loss 2.31258, acc 0.34375\n",
      "2017-04-03T22:10:25.192388: step 269, loss 2.09307, acc 0.328125\n",
      "2017-04-03T22:10:25.382638: step 270, loss 1.99855, acc 0.328125\n",
      "2017-04-03T22:10:25.574364: step 271, loss 2.26903, acc 0.328125\n",
      "2017-04-03T22:10:25.765090: step 272, loss 2.0815, acc 0.359375\n",
      "2017-04-03T22:10:25.956432: step 273, loss 1.9865, acc 0.359375\n",
      "2017-04-03T22:10:26.151214: step 274, loss 2.05968, acc 0.40625\n",
      "2017-04-03T22:10:26.344331: step 275, loss 1.85078, acc 0.375\n",
      "2017-04-03T22:10:26.542430: step 276, loss 1.99405, acc 0.359375\n",
      "2017-04-03T22:10:26.732210: step 277, loss 2.20119, acc 0.25\n",
      "2017-04-03T22:10:26.924422: step 278, loss 1.93666, acc 0.328125\n",
      "2017-04-03T22:10:27.118597: step 279, loss 1.82751, acc 0.4375\n",
      "2017-04-03T22:10:27.306680: step 280, loss 2.12977, acc 0.296875\n",
      "2017-04-03T22:10:27.500629: step 281, loss 2.18928, acc 0.28125\n",
      "2017-04-03T22:10:27.692602: step 282, loss 2.15759, acc 0.3125\n",
      "2017-04-03T22:10:27.892034: step 283, loss 1.79896, acc 0.4375\n",
      "2017-04-03T22:10:28.083799: step 284, loss 1.93518, acc 0.40625\n",
      "2017-04-03T22:10:28.273189: step 285, loss 1.88893, acc 0.3125\n",
      "2017-04-03T22:10:28.470959: step 286, loss 1.92523, acc 0.375\n",
      "2017-04-03T22:10:28.664000: step 287, loss 1.65136, acc 0.46875\n",
      "2017-04-03T22:10:28.856286: step 288, loss 2.45491, acc 0.265625\n",
      "2017-04-03T22:10:29.047938: step 289, loss 1.70326, acc 0.421875\n",
      "2017-04-03T22:10:29.238550: step 290, loss 1.63604, acc 0.46875\n",
      "2017-04-03T22:10:29.431827: step 291, loss 1.91537, acc 0.390625\n",
      "2017-04-03T22:10:29.624584: step 292, loss 1.66342, acc 0.453125\n",
      "2017-04-03T22:10:29.816639: step 293, loss 1.52054, acc 0.46875\n",
      "2017-04-03T22:10:30.005727: step 294, loss 2.09772, acc 0.3125\n",
      "2017-04-03T22:10:30.197845: step 295, loss 1.78156, acc 0.390625\n",
      "2017-04-03T22:10:30.389287: step 296, loss 2.23156, acc 0.28125\n",
      "2017-04-03T22:10:30.584110: step 297, loss 2.09718, acc 0.375\n",
      "2017-04-03T22:10:30.775136: step 298, loss 2.20287, acc 0.265625\n",
      "2017-04-03T22:10:30.966911: step 299, loss 2.14633, acc 0.390625\n",
      "2017-04-03T22:10:31.159191: step 300, loss 1.89757, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:10:31.812299: step 300, loss 1.5771, acc 0.408616\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-300\n",
      "\n",
      "2017-04-03T22:10:32.060137: step 301, loss 2.07162, acc 0.3125\n",
      "2017-04-03T22:10:32.248911: step 302, loss 1.84593, acc 0.4375\n",
      "2017-04-03T22:10:32.441886: step 303, loss 2.089, acc 0.265625\n",
      "2017-04-03T22:10:32.635038: step 304, loss 1.86407, acc 0.40625\n",
      "2017-04-03T22:10:32.827912: step 305, loss 2.22396, acc 0.28125\n",
      "2017-04-03T22:10:33.018866: step 306, loss 1.86888, acc 0.34375\n",
      "2017-04-03T22:10:33.208638: step 307, loss 2.08434, acc 0.34375\n",
      "2017-04-03T22:10:33.399980: step 308, loss 2.3887, acc 0.140625\n",
      "2017-04-03T22:10:33.594179: step 309, loss 1.87024, acc 0.4375\n",
      "2017-04-03T22:10:33.784047: step 310, loss 2.09125, acc 0.328125\n",
      "2017-04-03T22:10:33.972341: step 311, loss 2.18638, acc 0.3125\n",
      "2017-04-03T22:10:34.165099: step 312, loss 2.21202, acc 0.25\n",
      "2017-04-03T22:10:34.363532: step 313, loss 2.04684, acc 0.375\n",
      "2017-04-03T22:10:34.556145: step 314, loss 1.98013, acc 0.375\n",
      "2017-04-03T22:10:34.749554: step 315, loss 1.85102, acc 0.34375\n",
      "2017-04-03T22:10:34.942294: step 316, loss 2.10554, acc 0.375\n",
      "2017-04-03T22:10:35.134325: step 317, loss 1.86307, acc 0.390625\n",
      "2017-04-03T22:10:35.328178: step 318, loss 2.06953, acc 0.28125\n",
      "2017-04-03T22:10:35.525060: step 319, loss 1.72765, acc 0.40625\n",
      "2017-04-03T22:10:35.722759: step 320, loss 1.87405, acc 0.34375\n",
      "2017-04-03T22:10:35.920665: step 321, loss 2.21039, acc 0.296875\n",
      "2017-04-03T22:10:36.122912: step 322, loss 2.1544, acc 0.25\n",
      "2017-04-03T22:10:36.316624: step 323, loss 2.06378, acc 0.40625\n",
      "2017-04-03T22:10:36.485903: step 324, loss 2.01772, acc 0.307692\n",
      "2017-04-03T22:10:36.678999: step 325, loss 1.7906, acc 0.375\n",
      "2017-04-03T22:10:36.875566: step 326, loss 1.6307, acc 0.421875\n",
      "2017-04-03T22:10:37.082901: step 327, loss 1.6269, acc 0.5\n",
      "2017-04-03T22:10:37.278054: step 328, loss 1.64498, acc 0.46875\n",
      "2017-04-03T22:10:37.471704: step 329, loss 1.64995, acc 0.375\n",
      "2017-04-03T22:10:37.662791: step 330, loss 1.49469, acc 0.453125\n",
      "2017-04-03T22:10:37.852548: step 331, loss 1.84153, acc 0.4375\n",
      "2017-04-03T22:10:38.045714: step 332, loss 1.78485, acc 0.40625\n",
      "2017-04-03T22:10:38.237845: step 333, loss 1.5768, acc 0.4375\n",
      "2017-04-03T22:10:38.429992: step 334, loss 1.70277, acc 0.453125\n",
      "2017-04-03T22:10:38.621030: step 335, loss 1.68441, acc 0.421875\n",
      "2017-04-03T22:10:38.814042: step 336, loss 1.9274, acc 0.390625\n",
      "2017-04-03T22:10:39.010566: step 337, loss 1.452, acc 0.515625\n",
      "2017-04-03T22:10:39.202386: step 338, loss 1.83604, acc 0.34375\n",
      "2017-04-03T22:10:39.391708: step 339, loss 1.62999, acc 0.453125\n",
      "2017-04-03T22:10:39.583762: step 340, loss 1.74323, acc 0.46875\n",
      "2017-04-03T22:10:39.775299: step 341, loss 1.76934, acc 0.421875\n",
      "2017-04-03T22:10:39.963263: step 342, loss 1.73181, acc 0.453125\n",
      "2017-04-03T22:10:40.159372: step 343, loss 1.4504, acc 0.4375\n",
      "2017-04-03T22:10:40.352882: step 344, loss 1.35935, acc 0.515625\n",
      "2017-04-03T22:10:40.541957: step 345, loss 1.72147, acc 0.359375\n",
      "2017-04-03T22:10:40.733285: step 346, loss 1.3563, acc 0.578125\n",
      "2017-04-03T22:10:40.925850: step 347, loss 1.73188, acc 0.46875\n",
      "2017-04-03T22:10:41.117613: step 348, loss 1.5533, acc 0.46875\n",
      "2017-04-03T22:10:41.308383: step 349, loss 1.83387, acc 0.390625\n",
      "2017-04-03T22:10:41.505614: step 350, loss 1.64452, acc 0.484375\n",
      "2017-04-03T22:10:41.698428: step 351, loss 1.58501, acc 0.46875\n",
      "2017-04-03T22:10:41.889398: step 352, loss 1.54953, acc 0.46875\n",
      "2017-04-03T22:10:42.085833: step 353, loss 1.46983, acc 0.515625\n",
      "2017-04-03T22:10:42.280388: step 354, loss 1.68052, acc 0.390625\n",
      "2017-04-03T22:10:42.470693: step 355, loss 1.56458, acc 0.421875\n",
      "2017-04-03T22:10:42.661140: step 356, loss 2.05859, acc 0.28125\n",
      "2017-04-03T22:10:42.853021: step 357, loss 1.61763, acc 0.421875\n",
      "2017-04-03T22:10:43.045463: step 358, loss 1.51828, acc 0.4375\n",
      "2017-04-03T22:10:43.238338: step 359, loss 1.94981, acc 0.265625\n",
      "2017-04-03T22:10:43.433917: step 360, loss 1.59137, acc 0.421875\n",
      "2017-04-03T22:10:43.624604: step 361, loss 1.74915, acc 0.40625\n",
      "2017-04-03T22:10:43.818350: step 362, loss 1.6442, acc 0.453125\n",
      "2017-04-03T22:10:44.012661: step 363, loss 1.59435, acc 0.5\n",
      "2017-04-03T22:10:44.204087: step 364, loss 1.79917, acc 0.421875\n",
      "2017-04-03T22:10:44.399004: step 365, loss 1.59964, acc 0.34375\n",
      "2017-04-03T22:10:44.598280: step 366, loss 1.80717, acc 0.375\n",
      "2017-04-03T22:10:44.786760: step 367, loss 1.69519, acc 0.484375\n",
      "2017-04-03T22:10:44.973886: step 368, loss 1.67508, acc 0.4375\n",
      "2017-04-03T22:10:45.164622: step 369, loss 1.36261, acc 0.5\n",
      "2017-04-03T22:10:45.359516: step 370, loss 1.66054, acc 0.453125\n",
      "2017-04-03T22:10:45.555827: step 371, loss 1.75902, acc 0.421875\n",
      "2017-04-03T22:10:45.747398: step 372, loss 1.79002, acc 0.453125\n",
      "2017-04-03T22:10:45.940167: step 373, loss 1.96869, acc 0.40625\n",
      "2017-04-03T22:10:46.131543: step 374, loss 1.75803, acc 0.375\n",
      "2017-04-03T22:10:46.325759: step 375, loss 1.87234, acc 0.390625\n",
      "2017-04-03T22:10:46.522845: step 376, loss 1.62523, acc 0.34375\n",
      "2017-04-03T22:10:46.715637: step 377, loss 1.34735, acc 0.484375\n",
      "2017-04-03T22:10:46.906749: step 378, loss 1.60395, acc 0.453125\n",
      "2017-04-03T22:10:47.097108: step 379, loss 1.3668, acc 0.484375\n",
      "2017-04-03T22:10:47.287924: step 380, loss 1.83651, acc 0.390625\n",
      "2017-04-03T22:10:47.479610: step 381, loss 1.97059, acc 0.359375\n",
      "2017-04-03T22:10:47.671783: step 382, loss 1.68006, acc 0.46875\n",
      "2017-04-03T22:10:47.864048: step 383, loss 1.56434, acc 0.5\n",
      "2017-04-03T22:10:48.061273: step 384, loss 1.71997, acc 0.4375\n",
      "2017-04-03T22:10:48.256934: step 385, loss 1.63283, acc 0.53125\n",
      "2017-04-03T22:10:48.450859: step 386, loss 1.87636, acc 0.375\n",
      "2017-04-03T22:10:48.648029: step 387, loss 1.51727, acc 0.46875\n",
      "2017-04-03T22:10:48.837695: step 388, loss 1.56479, acc 0.40625\n",
      "2017-04-03T22:10:49.027493: step 389, loss 1.88941, acc 0.328125\n",
      "2017-04-03T22:10:49.219987: step 390, loss 1.56569, acc 0.4375\n",
      "2017-04-03T22:10:49.413768: step 391, loss 1.67142, acc 0.4375\n",
      "2017-04-03T22:10:49.609856: step 392, loss 1.77154, acc 0.375\n",
      "2017-04-03T22:10:49.800525: step 393, loss 1.82012, acc 0.390625\n",
      "2017-04-03T22:10:49.990171: step 394, loss 1.92371, acc 0.390625\n",
      "2017-04-03T22:10:50.187388: step 395, loss 1.48627, acc 0.421875\n",
      "2017-04-03T22:10:50.377556: step 396, loss 1.67533, acc 0.421875\n",
      "2017-04-03T22:10:50.571710: step 397, loss 1.4764, acc 0.609375\n",
      "2017-04-03T22:10:50.767685: step 398, loss 1.45274, acc 0.453125\n",
      "2017-04-03T22:10:50.961658: step 399, loss 1.64178, acc 0.484375\n",
      "2017-04-03T22:10:51.150674: step 400, loss 1.6324, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:10:51.815852: step 400, loss 1.50785, acc 0.438642\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-400\n",
      "\n",
      "2017-04-03T22:10:52.068378: step 401, loss 1.41764, acc 0.53125\n",
      "2017-04-03T22:10:52.261215: step 402, loss 1.54409, acc 0.375\n",
      "2017-04-03T22:10:52.450693: step 403, loss 1.64352, acc 0.484375\n",
      "2017-04-03T22:10:52.645559: step 404, loss 1.70687, acc 0.3125\n",
      "2017-04-03T22:10:52.839190: step 405, loss 1.57857, acc 0.453125\n",
      "2017-04-03T22:10:53.027483: step 406, loss 1.60595, acc 0.421875\n",
      "2017-04-03T22:10:53.222028: step 407, loss 1.89382, acc 0.4375\n",
      "2017-04-03T22:10:53.409714: step 408, loss 1.57408, acc 0.453125\n",
      "2017-04-03T22:10:53.599343: step 409, loss 2.01793, acc 0.3125\n",
      "2017-04-03T22:10:53.791761: step 410, loss 1.70598, acc 0.4375\n",
      "2017-04-03T22:10:53.986609: step 411, loss 1.79356, acc 0.375\n",
      "2017-04-03T22:10:54.182931: step 412, loss 1.72439, acc 0.484375\n",
      "2017-04-03T22:10:54.376871: step 413, loss 1.68093, acc 0.453125\n",
      "2017-04-03T22:10:54.568001: step 414, loss 1.33383, acc 0.515625\n",
      "2017-04-03T22:10:54.758216: step 415, loss 1.71327, acc 0.484375\n",
      "2017-04-03T22:10:54.945792: step 416, loss 1.65723, acc 0.421875\n",
      "2017-04-03T22:10:55.141524: step 417, loss 1.50415, acc 0.453125\n",
      "2017-04-03T22:10:55.333553: step 418, loss 1.89444, acc 0.359375\n",
      "2017-04-03T22:10:55.528626: step 419, loss 1.65748, acc 0.453125\n",
      "2017-04-03T22:10:55.726889: step 420, loss 1.68092, acc 0.390625\n",
      "2017-04-03T22:10:55.926691: step 421, loss 1.63965, acc 0.46875\n",
      "2017-04-03T22:10:56.122038: step 422, loss 1.7478, acc 0.34375\n",
      "2017-04-03T22:10:56.312240: step 423, loss 1.72063, acc 0.453125\n",
      "2017-04-03T22:10:56.500012: step 424, loss 1.32086, acc 0.515625\n",
      "2017-04-03T22:10:56.688181: step 425, loss 1.52489, acc 0.40625\n",
      "2017-04-03T22:10:56.880015: step 426, loss 1.48809, acc 0.484375\n",
      "2017-04-03T22:10:57.074708: step 427, loss 1.36782, acc 0.5625\n",
      "2017-04-03T22:10:57.272698: step 428, loss 1.64948, acc 0.453125\n",
      "2017-04-03T22:10:57.467356: step 429, loss 1.85658, acc 0.34375\n",
      "2017-04-03T22:10:57.660053: step 430, loss 1.80951, acc 0.375\n",
      "2017-04-03T22:10:57.855299: step 431, loss 1.63535, acc 0.40625\n",
      "2017-04-03T22:10:58.012572: step 432, loss 1.80617, acc 0.423077\n",
      "2017-04-03T22:10:58.202996: step 433, loss 1.3607, acc 0.515625\n",
      "2017-04-03T22:10:58.402046: step 434, loss 1.33448, acc 0.515625\n",
      "2017-04-03T22:10:58.595183: step 435, loss 1.32709, acc 0.59375\n",
      "2017-04-03T22:10:58.788537: step 436, loss 1.47094, acc 0.5\n",
      "2017-04-03T22:10:58.984707: step 437, loss 1.31521, acc 0.53125\n",
      "2017-04-03T22:10:59.177108: step 438, loss 1.64016, acc 0.4375\n",
      "2017-04-03T22:10:59.372190: step 439, loss 1.81046, acc 0.34375\n",
      "2017-04-03T22:10:59.564342: step 440, loss 1.43789, acc 0.484375\n",
      "2017-04-03T22:10:59.758125: step 441, loss 1.41249, acc 0.546875\n",
      "2017-04-03T22:10:59.952598: step 442, loss 1.69869, acc 0.4375\n",
      "2017-04-03T22:11:00.149096: step 443, loss 1.45121, acc 0.53125\n",
      "2017-04-03T22:11:00.340929: step 444, loss 1.39359, acc 0.453125\n",
      "2017-04-03T22:11:00.533957: step 445, loss 1.32691, acc 0.546875\n",
      "2017-04-03T22:11:00.724163: step 446, loss 1.50724, acc 0.421875\n",
      "2017-04-03T22:11:00.915362: step 447, loss 1.60801, acc 0.515625\n",
      "2017-04-03T22:11:01.107102: step 448, loss 1.09669, acc 0.53125\n",
      "2017-04-03T22:11:01.296793: step 449, loss 1.5227, acc 0.484375\n",
      "2017-04-03T22:11:01.488811: step 450, loss 1.28923, acc 0.53125\n",
      "2017-04-03T22:11:01.681838: step 451, loss 1.33244, acc 0.484375\n",
      "2017-04-03T22:11:01.874216: step 452, loss 1.30304, acc 0.578125\n",
      "2017-04-03T22:11:02.067600: step 453, loss 1.47325, acc 0.515625\n",
      "2017-04-03T22:11:02.256008: step 454, loss 1.43532, acc 0.59375\n",
      "2017-04-03T22:11:02.446699: step 455, loss 1.31801, acc 0.5625\n",
      "2017-04-03T22:11:02.647736: step 456, loss 1.31267, acc 0.546875\n",
      "2017-04-03T22:11:02.851470: step 457, loss 1.3488, acc 0.453125\n",
      "2017-04-03T22:11:03.043934: step 458, loss 1.44182, acc 0.546875\n",
      "2017-04-03T22:11:03.239796: step 459, loss 1.34693, acc 0.515625\n",
      "2017-04-03T22:11:03.439981: step 460, loss 1.68848, acc 0.453125\n",
      "2017-04-03T22:11:03.631930: step 461, loss 1.06045, acc 0.59375\n",
      "2017-04-03T22:11:03.826931: step 462, loss 1.43316, acc 0.453125\n",
      "2017-04-03T22:11:04.022971: step 463, loss 1.80348, acc 0.390625\n",
      "2017-04-03T22:11:04.215479: step 464, loss 1.40745, acc 0.484375\n",
      "2017-04-03T22:11:04.408756: step 465, loss 1.55614, acc 0.421875\n",
      "2017-04-03T22:11:04.603884: step 466, loss 1.52484, acc 0.484375\n",
      "2017-04-03T22:11:04.795107: step 467, loss 1.74826, acc 0.390625\n",
      "2017-04-03T22:11:04.990325: step 468, loss 1.69172, acc 0.5\n",
      "2017-04-03T22:11:05.184277: step 469, loss 1.30536, acc 0.515625\n",
      "2017-04-03T22:11:05.378982: step 470, loss 1.23914, acc 0.59375\n",
      "2017-04-03T22:11:05.571608: step 471, loss 1.29743, acc 0.578125\n",
      "2017-04-03T22:11:05.766136: step 472, loss 1.28566, acc 0.5625\n",
      "2017-04-03T22:11:05.965652: step 473, loss 1.30599, acc 0.53125\n",
      "2017-04-03T22:11:06.161989: step 474, loss 1.41446, acc 0.515625\n",
      "2017-04-03T22:11:06.360681: step 475, loss 1.58251, acc 0.421875\n",
      "2017-04-03T22:11:06.549503: step 476, loss 1.2506, acc 0.625\n",
      "2017-04-03T22:11:06.741151: step 477, loss 1.34914, acc 0.546875\n",
      "2017-04-03T22:11:06.928248: step 478, loss 1.42579, acc 0.53125\n",
      "2017-04-03T22:11:07.121619: step 479, loss 1.5706, acc 0.46875\n",
      "2017-04-03T22:11:07.311996: step 480, loss 1.52802, acc 0.390625\n",
      "2017-04-03T22:11:07.506601: step 481, loss 1.22424, acc 0.5\n",
      "2017-04-03T22:11:07.704319: step 482, loss 1.61017, acc 0.453125\n",
      "2017-04-03T22:11:07.925884: step 483, loss 1.26291, acc 0.59375\n",
      "2017-04-03T22:11:08.133419: step 484, loss 1.5665, acc 0.421875\n",
      "2017-04-03T22:11:08.329884: step 485, loss 1.17022, acc 0.5625\n",
      "2017-04-03T22:11:08.521698: step 486, loss 1.48448, acc 0.421875\n",
      "2017-04-03T22:11:08.717349: step 487, loss 1.66633, acc 0.421875\n",
      "2017-04-03T22:11:08.915948: step 488, loss 1.21275, acc 0.625\n",
      "2017-04-03T22:11:09.120558: step 489, loss 1.36684, acc 0.53125\n",
      "2017-04-03T22:11:09.321992: step 490, loss 1.36278, acc 0.515625\n",
      "2017-04-03T22:11:09.534080: step 491, loss 1.3493, acc 0.53125\n",
      "2017-04-03T22:11:09.733188: step 492, loss 1.68036, acc 0.40625\n",
      "2017-04-03T22:11:09.928935: step 493, loss 1.4427, acc 0.40625\n",
      "2017-04-03T22:11:10.140680: step 494, loss 1.57055, acc 0.484375\n",
      "2017-04-03T22:11:10.334883: step 495, loss 1.39296, acc 0.53125\n",
      "2017-04-03T22:11:10.527773: step 496, loss 1.18228, acc 0.609375\n",
      "2017-04-03T22:11:10.728070: step 497, loss 1.40295, acc 0.4375\n",
      "2017-04-03T22:11:10.931080: step 498, loss 1.50552, acc 0.4375\n",
      "2017-04-03T22:11:11.125334: step 499, loss 1.40035, acc 0.484375\n",
      "2017-04-03T22:11:11.317129: step 500, loss 1.69888, acc 0.4375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:11:11.968861: step 500, loss 1.48127, acc 0.434726\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-500\n",
      "\n",
      "2017-04-03T22:11:12.219095: step 501, loss 1.28111, acc 0.578125\n",
      "2017-04-03T22:11:12.414900: step 502, loss 1.3886, acc 0.53125\n",
      "2017-04-03T22:11:12.607407: step 503, loss 1.56516, acc 0.46875\n",
      "2017-04-03T22:11:12.799671: step 504, loss 1.38083, acc 0.5\n",
      "2017-04-03T22:11:12.990984: step 505, loss 1.44206, acc 0.546875\n",
      "2017-04-03T22:11:13.183545: step 506, loss 1.42277, acc 0.59375\n",
      "2017-04-03T22:11:13.375969: step 507, loss 1.26809, acc 0.53125\n",
      "2017-04-03T22:11:13.569382: step 508, loss 1.24763, acc 0.53125\n",
      "2017-04-03T22:11:13.766630: step 509, loss 1.38545, acc 0.4375\n",
      "2017-04-03T22:11:13.958884: step 510, loss 1.3718, acc 0.5\n",
      "2017-04-03T22:11:14.153648: step 511, loss 1.37428, acc 0.421875\n",
      "2017-04-03T22:11:14.351667: step 512, loss 1.20407, acc 0.59375\n",
      "2017-04-03T22:11:14.545379: step 513, loss 1.29206, acc 0.59375\n",
      "2017-04-03T22:11:14.737185: step 514, loss 1.20229, acc 0.609375\n",
      "2017-04-03T22:11:14.931329: step 515, loss 1.42493, acc 0.546875\n",
      "2017-04-03T22:11:15.120203: step 516, loss 1.20138, acc 0.640625\n",
      "2017-04-03T22:11:15.312312: step 517, loss 1.13761, acc 0.65625\n",
      "2017-04-03T22:11:15.502504: step 518, loss 1.41898, acc 0.53125\n",
      "2017-04-03T22:11:15.698769: step 519, loss 1.34702, acc 0.546875\n",
      "2017-04-03T22:11:15.891387: step 520, loss 1.52506, acc 0.484375\n",
      "2017-04-03T22:11:16.084465: step 521, loss 1.23546, acc 0.53125\n",
      "2017-04-03T22:11:16.279107: step 522, loss 1.45825, acc 0.4375\n",
      "2017-04-03T22:11:16.473025: step 523, loss 1.26707, acc 0.546875\n",
      "2017-04-03T22:11:16.670391: step 524, loss 1.42265, acc 0.53125\n",
      "2017-04-03T22:11:16.861264: step 525, loss 1.24453, acc 0.5\n",
      "2017-04-03T22:11:17.062053: step 526, loss 1.55961, acc 0.421875\n",
      "2017-04-03T22:11:17.253568: step 527, loss 1.39676, acc 0.484375\n",
      "2017-04-03T22:11:17.449977: step 528, loss 1.52384, acc 0.46875\n",
      "2017-04-03T22:11:17.640274: step 529, loss 1.47463, acc 0.484375\n",
      "2017-04-03T22:11:17.832523: step 530, loss 1.12424, acc 0.609375\n",
      "2017-04-03T22:11:18.022710: step 531, loss 1.37829, acc 0.53125\n",
      "2017-04-03T22:11:18.215170: step 532, loss 1.50252, acc 0.421875\n",
      "2017-04-03T22:11:18.405004: step 533, loss 1.50592, acc 0.390625\n",
      "2017-04-03T22:11:18.593615: step 534, loss 1.23174, acc 0.59375\n",
      "2017-04-03T22:11:18.784514: step 535, loss 1.14443, acc 0.578125\n",
      "2017-04-03T22:11:18.976931: step 536, loss 1.39196, acc 0.53125\n",
      "2017-04-03T22:11:19.166430: step 537, loss 1.19003, acc 0.546875\n",
      "2017-04-03T22:11:19.358536: step 538, loss 1.61402, acc 0.4375\n",
      "2017-04-03T22:11:19.551180: step 539, loss 1.17879, acc 0.59375\n",
      "2017-04-03T22:11:19.713058: step 540, loss 1.83759, acc 0.307692\n",
      "2017-04-03T22:11:19.905862: step 541, loss 1.45316, acc 0.46875\n",
      "2017-04-03T22:11:20.102845: step 542, loss 1.11185, acc 0.625\n",
      "2017-04-03T22:11:20.293382: step 543, loss 1.19033, acc 0.53125\n",
      "2017-04-03T22:11:20.486640: step 544, loss 1.167, acc 0.65625\n",
      "2017-04-03T22:11:20.684416: step 545, loss 1.01243, acc 0.609375\n",
      "2017-04-03T22:11:20.879950: step 546, loss 1.15043, acc 0.609375\n",
      "2017-04-03T22:11:21.075660: step 547, loss 1.31423, acc 0.546875\n",
      "2017-04-03T22:11:21.269474: step 548, loss 1.335, acc 0.484375\n",
      "2017-04-03T22:11:21.462620: step 549, loss 1.21508, acc 0.515625\n",
      "2017-04-03T22:11:21.656357: step 550, loss 1.20921, acc 0.546875\n",
      "2017-04-03T22:11:21.850691: step 551, loss 1.25581, acc 0.546875\n",
      "2017-04-03T22:11:22.040729: step 552, loss 1.3169, acc 0.5625\n",
      "2017-04-03T22:11:22.232682: step 553, loss 1.19983, acc 0.578125\n",
      "2017-04-03T22:11:22.425578: step 554, loss 1.26268, acc 0.515625\n",
      "2017-04-03T22:11:22.617585: step 555, loss 1.14338, acc 0.53125\n",
      "2017-04-03T22:11:22.810343: step 556, loss 1.03575, acc 0.640625\n",
      "2017-04-03T22:11:23.004844: step 557, loss 0.944049, acc 0.6875\n",
      "2017-04-03T22:11:23.196804: step 558, loss 1.48721, acc 0.421875\n",
      "2017-04-03T22:11:23.389607: step 559, loss 1.11906, acc 0.640625\n",
      "2017-04-03T22:11:23.582337: step 560, loss 1.14087, acc 0.546875\n",
      "2017-04-03T22:11:23.772891: step 561, loss 0.941948, acc 0.6875\n",
      "2017-04-03T22:11:23.962653: step 562, loss 1.04494, acc 0.640625\n",
      "2017-04-03T22:11:24.152412: step 563, loss 1.1189, acc 0.609375\n",
      "2017-04-03T22:11:24.345928: step 564, loss 1.29886, acc 0.609375\n",
      "2017-04-03T22:11:24.543668: step 565, loss 1.29021, acc 0.546875\n",
      "2017-04-03T22:11:24.737702: step 566, loss 1.31709, acc 0.484375\n",
      "2017-04-03T22:11:24.931878: step 567, loss 1.18445, acc 0.53125\n",
      "2017-04-03T22:11:25.120004: step 568, loss 1.34072, acc 0.53125\n",
      "2017-04-03T22:11:25.308355: step 569, loss 1.10885, acc 0.625\n",
      "2017-04-03T22:11:25.501295: step 570, loss 1.27741, acc 0.53125\n",
      "2017-04-03T22:11:25.691719: step 571, loss 1.48321, acc 0.515625\n",
      "2017-04-03T22:11:25.889121: step 572, loss 1.30022, acc 0.546875\n",
      "2017-04-03T22:11:26.085694: step 573, loss 1.02963, acc 0.65625\n",
      "2017-04-03T22:11:26.275956: step 574, loss 1.04652, acc 0.59375\n",
      "2017-04-03T22:11:26.469253: step 575, loss 1.53194, acc 0.5\n",
      "2017-04-03T22:11:26.660215: step 576, loss 1.16342, acc 0.5625\n",
      "2017-04-03T22:11:26.852713: step 577, loss 1.20439, acc 0.515625\n",
      "2017-04-03T22:11:27.045190: step 578, loss 1.03364, acc 0.53125\n",
      "2017-04-03T22:11:27.238061: step 579, loss 1.03074, acc 0.578125\n",
      "2017-04-03T22:11:27.432658: step 580, loss 1.19805, acc 0.578125\n",
      "2017-04-03T22:11:27.624520: step 581, loss 1.18401, acc 0.578125\n",
      "2017-04-03T22:11:27.818846: step 582, loss 1.30299, acc 0.546875\n",
      "2017-04-03T22:11:28.010716: step 583, loss 1.17854, acc 0.578125\n",
      "2017-04-03T22:11:28.198974: step 584, loss 0.865734, acc 0.75\n",
      "2017-04-03T22:11:28.391457: step 585, loss 1.2768, acc 0.5\n",
      "2017-04-03T22:11:28.582576: step 586, loss 1.25904, acc 0.59375\n",
      "2017-04-03T22:11:28.775835: step 587, loss 1.27743, acc 0.5625\n",
      "2017-04-03T22:11:28.967129: step 588, loss 1.0291, acc 0.65625\n",
      "2017-04-03T22:11:29.158116: step 589, loss 1.34662, acc 0.5\n",
      "2017-04-03T22:11:29.352713: step 590, loss 1.30955, acc 0.484375\n",
      "2017-04-03T22:11:29.546079: step 591, loss 1.42331, acc 0.453125\n",
      "2017-04-03T22:11:29.742172: step 592, loss 1.07964, acc 0.59375\n",
      "2017-04-03T22:11:29.938764: step 593, loss 1.37197, acc 0.515625\n",
      "2017-04-03T22:11:30.132507: step 594, loss 1.35349, acc 0.46875\n",
      "2017-04-03T22:11:30.328913: step 595, loss 1.10689, acc 0.609375\n",
      "2017-04-03T22:11:30.521201: step 596, loss 1.19171, acc 0.59375\n",
      "2017-04-03T22:11:30.712320: step 597, loss 1.02052, acc 0.609375\n",
      "2017-04-03T22:11:30.909714: step 598, loss 1.01181, acc 0.671875\n",
      "2017-04-03T22:11:31.108541: step 599, loss 1.43413, acc 0.5\n",
      "2017-04-03T22:11:31.302326: step 600, loss 1.11384, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:11:31.974055: step 600, loss 1.4331, acc 0.456919\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-600\n",
      "\n",
      "2017-04-03T22:11:32.233731: step 601, loss 1.31166, acc 0.484375\n",
      "2017-04-03T22:11:32.422546: step 602, loss 1.3654, acc 0.5\n",
      "2017-04-03T22:11:32.616588: step 603, loss 1.29191, acc 0.5625\n",
      "2017-04-03T22:11:32.808974: step 604, loss 1.33937, acc 0.578125\n",
      "2017-04-03T22:11:32.998928: step 605, loss 1.41153, acc 0.4375\n",
      "2017-04-03T22:11:33.191064: step 606, loss 1.22325, acc 0.59375\n",
      "2017-04-03T22:11:33.385255: step 607, loss 1.30405, acc 0.53125\n",
      "2017-04-03T22:11:33.575813: step 608, loss 1.26263, acc 0.546875\n",
      "2017-04-03T22:11:33.774659: step 609, loss 1.31077, acc 0.546875\n",
      "2017-04-03T22:11:33.968127: step 610, loss 1.28573, acc 0.546875\n",
      "2017-04-03T22:11:34.162444: step 611, loss 0.936565, acc 0.734375\n",
      "2017-04-03T22:11:34.349937: step 612, loss 1.15024, acc 0.59375\n",
      "2017-04-03T22:11:34.541581: step 613, loss 1.27754, acc 0.46875\n",
      "2017-04-03T22:11:34.734938: step 614, loss 1.21268, acc 0.546875\n",
      "2017-04-03T22:11:34.925292: step 615, loss 1.1811, acc 0.59375\n",
      "2017-04-03T22:11:35.115693: step 616, loss 1.25586, acc 0.421875\n",
      "2017-04-03T22:11:35.305685: step 617, loss 1.24532, acc 0.515625\n",
      "2017-04-03T22:11:35.493665: step 618, loss 1.02281, acc 0.65625\n",
      "2017-04-03T22:11:35.684651: step 619, loss 1.30219, acc 0.5\n",
      "2017-04-03T22:11:35.876688: step 620, loss 1.38775, acc 0.53125\n",
      "2017-04-03T22:11:36.072371: step 621, loss 0.926412, acc 0.71875\n",
      "2017-04-03T22:11:36.267448: step 622, loss 1.31522, acc 0.5625\n",
      "2017-04-03T22:11:36.458892: step 623, loss 1.37851, acc 0.5\n",
      "2017-04-03T22:11:36.653901: step 624, loss 1.33559, acc 0.46875\n",
      "2017-04-03T22:11:36.844783: step 625, loss 1.36832, acc 0.5625\n",
      "2017-04-03T22:11:37.039092: step 626, loss 1.37788, acc 0.46875\n",
      "2017-04-03T22:11:37.228194: step 627, loss 1.19277, acc 0.5625\n",
      "2017-04-03T22:11:37.421041: step 628, loss 1.34997, acc 0.4375\n",
      "2017-04-03T22:11:37.615452: step 629, loss 1.48995, acc 0.453125\n",
      "2017-04-03T22:11:37.806596: step 630, loss 1.24419, acc 0.53125\n",
      "2017-04-03T22:11:37.999856: step 631, loss 1.17923, acc 0.578125\n",
      "2017-04-03T22:11:38.190812: step 632, loss 1.36605, acc 0.53125\n",
      "2017-04-03T22:11:38.385504: step 633, loss 1.21694, acc 0.5625\n",
      "2017-04-03T22:11:38.574085: step 634, loss 1.06659, acc 0.640625\n",
      "2017-04-03T22:11:38.765997: step 635, loss 1.34653, acc 0.515625\n",
      "2017-04-03T22:11:38.961277: step 636, loss 1.18585, acc 0.59375\n",
      "2017-04-03T22:11:39.156057: step 637, loss 1.3867, acc 0.53125\n",
      "2017-04-03T22:11:39.347833: step 638, loss 1.3033, acc 0.53125\n",
      "2017-04-03T22:11:39.540789: step 639, loss 1.18838, acc 0.5625\n",
      "2017-04-03T22:11:39.730042: step 640, loss 1.56758, acc 0.484375\n",
      "2017-04-03T22:11:39.922099: step 641, loss 1.18322, acc 0.5625\n",
      "2017-04-03T22:11:40.116433: step 642, loss 1.11425, acc 0.640625\n",
      "2017-04-03T22:11:40.306282: step 643, loss 1.33639, acc 0.53125\n",
      "2017-04-03T22:11:40.502021: step 644, loss 1.16702, acc 0.5625\n",
      "2017-04-03T22:11:40.697077: step 645, loss 1.18011, acc 0.515625\n",
      "2017-04-03T22:11:40.886647: step 646, loss 1.10415, acc 0.625\n",
      "2017-04-03T22:11:41.081123: step 647, loss 1.22059, acc 0.5\n",
      "2017-04-03T22:11:41.242364: step 648, loss 1.40508, acc 0.5\n",
      "2017-04-03T22:11:41.432412: step 649, loss 0.87202, acc 0.765625\n",
      "2017-04-03T22:11:41.624041: step 650, loss 1.03918, acc 0.65625\n",
      "2017-04-03T22:11:41.816566: step 651, loss 1.11341, acc 0.640625\n",
      "2017-04-03T22:11:42.010716: step 652, loss 1.08234, acc 0.59375\n",
      "2017-04-03T22:11:42.200997: step 653, loss 1.44102, acc 0.4375\n",
      "2017-04-03T22:11:42.393661: step 654, loss 1.05217, acc 0.609375\n",
      "2017-04-03T22:11:42.585218: step 655, loss 0.846983, acc 0.703125\n",
      "2017-04-03T22:11:42.777908: step 656, loss 1.17232, acc 0.59375\n",
      "2017-04-03T22:11:42.972158: step 657, loss 0.98119, acc 0.640625\n",
      "2017-04-03T22:11:43.164279: step 658, loss 1.18061, acc 0.59375\n",
      "2017-04-03T22:11:43.356075: step 659, loss 1.05351, acc 0.625\n",
      "2017-04-03T22:11:43.548847: step 660, loss 1.18781, acc 0.5625\n",
      "2017-04-03T22:11:43.739362: step 661, loss 1.13256, acc 0.640625\n",
      "2017-04-03T22:11:43.930220: step 662, loss 1.10825, acc 0.59375\n",
      "2017-04-03T22:11:44.120683: step 663, loss 1.22927, acc 0.546875\n",
      "2017-04-03T22:11:44.313357: step 664, loss 1.04848, acc 0.640625\n",
      "2017-04-03T22:11:44.503739: step 665, loss 1.31967, acc 0.578125\n",
      "2017-04-03T22:11:44.696674: step 666, loss 1.05765, acc 0.59375\n",
      "2017-04-03T22:11:44.893996: step 667, loss 1.10593, acc 0.578125\n",
      "2017-04-03T22:11:45.116179: step 668, loss 1.01303, acc 0.65625\n",
      "2017-04-03T22:11:45.333194: step 669, loss 1.09627, acc 0.640625\n",
      "2017-04-03T22:11:45.527104: step 670, loss 0.931037, acc 0.65625\n",
      "2017-04-03T22:11:45.720783: step 671, loss 1.21775, acc 0.578125\n",
      "2017-04-03T22:11:45.910894: step 672, loss 1.00409, acc 0.578125\n",
      "2017-04-03T22:11:46.099392: step 673, loss 1.03425, acc 0.609375\n",
      "2017-04-03T22:11:46.292298: step 674, loss 0.95381, acc 0.71875\n",
      "2017-04-03T22:11:46.489711: step 675, loss 1.09004, acc 0.5625\n",
      "2017-04-03T22:11:46.683211: step 676, loss 1.17781, acc 0.515625\n",
      "2017-04-03T22:11:46.880407: step 677, loss 1.23117, acc 0.5625\n",
      "2017-04-03T22:11:47.082045: step 678, loss 0.965369, acc 0.625\n",
      "2017-04-03T22:11:47.291805: step 679, loss 1.37647, acc 0.53125\n",
      "2017-04-03T22:11:47.499095: step 680, loss 0.900414, acc 0.703125\n",
      "2017-04-03T22:11:47.693100: step 681, loss 1.23003, acc 0.546875\n",
      "2017-04-03T22:11:47.885202: step 682, loss 1.05519, acc 0.71875\n",
      "2017-04-03T22:11:48.083835: step 683, loss 1.32089, acc 0.546875\n",
      "2017-04-03T22:11:48.278914: step 684, loss 0.995535, acc 0.65625\n",
      "2017-04-03T22:11:48.470647: step 685, loss 1.32544, acc 0.546875\n",
      "2017-04-03T22:11:48.660519: step 686, loss 0.97943, acc 0.65625\n",
      "2017-04-03T22:11:48.852905: step 687, loss 1.18068, acc 0.5\n",
      "2017-04-03T22:11:49.041460: step 688, loss 1.05923, acc 0.6875\n",
      "2017-04-03T22:11:49.239168: step 689, loss 1.16601, acc 0.609375\n",
      "2017-04-03T22:11:49.432546: step 690, loss 1.02637, acc 0.71875\n",
      "2017-04-03T22:11:49.626800: step 691, loss 1.1448, acc 0.609375\n",
      "2017-04-03T22:11:49.821996: step 692, loss 1.11674, acc 0.609375\n",
      "2017-04-03T22:11:50.016179: step 693, loss 0.98127, acc 0.671875\n",
      "2017-04-03T22:11:50.210324: step 694, loss 1.16496, acc 0.609375\n",
      "2017-04-03T22:11:50.403846: step 695, loss 1.1559, acc 0.59375\n",
      "2017-04-03T22:11:50.594992: step 696, loss 1.05957, acc 0.59375\n",
      "2017-04-03T22:11:50.785530: step 697, loss 0.960499, acc 0.703125\n",
      "2017-04-03T22:11:50.977298: step 698, loss 0.972971, acc 0.671875\n",
      "2017-04-03T22:11:51.171775: step 699, loss 0.956023, acc 0.71875\n",
      "2017-04-03T22:11:51.369089: step 700, loss 1.33118, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:11:52.026605: step 700, loss 1.39476, acc 0.507833\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-700\n",
      "\n",
      "2017-04-03T22:11:52.278436: step 701, loss 1.18647, acc 0.53125\n",
      "2017-04-03T22:11:52.470305: step 702, loss 1.23299, acc 0.578125\n",
      "2017-04-03T22:11:52.663417: step 703, loss 0.933245, acc 0.6875\n",
      "2017-04-03T22:11:52.856218: step 704, loss 1.03004, acc 0.53125\n",
      "2017-04-03T22:11:53.049307: step 705, loss 1.10806, acc 0.5625\n",
      "2017-04-03T22:11:53.240020: step 706, loss 0.914558, acc 0.734375\n",
      "2017-04-03T22:11:53.430885: step 707, loss 1.21745, acc 0.578125\n",
      "2017-04-03T22:11:53.622827: step 708, loss 1.14756, acc 0.640625\n",
      "2017-04-03T22:11:53.811800: step 709, loss 0.939963, acc 0.65625\n",
      "2017-04-03T22:11:54.006113: step 710, loss 1.16338, acc 0.609375\n",
      "2017-04-03T22:11:54.199471: step 711, loss 1.07653, acc 0.609375\n",
      "2017-04-03T22:11:54.390578: step 712, loss 1.06939, acc 0.59375\n",
      "2017-04-03T22:11:54.586216: step 713, loss 1.01784, acc 0.671875\n",
      "2017-04-03T22:11:54.781835: step 714, loss 1.24542, acc 0.546875\n",
      "2017-04-03T22:11:54.972937: step 715, loss 1.22419, acc 0.5625\n",
      "2017-04-03T22:11:55.163494: step 716, loss 1.09916, acc 0.59375\n",
      "2017-04-03T22:11:55.352894: step 717, loss 1.10633, acc 0.609375\n",
      "2017-04-03T22:11:55.542006: step 718, loss 1.19322, acc 0.546875\n",
      "2017-04-03T22:11:55.734082: step 719, loss 1.05469, acc 0.59375\n",
      "2017-04-03T22:11:55.922967: step 720, loss 1.13904, acc 0.5625\n",
      "2017-04-03T22:11:56.112837: step 721, loss 1.1279, acc 0.546875\n",
      "2017-04-03T22:11:56.302118: step 722, loss 1.08387, acc 0.640625\n",
      "2017-04-03T22:11:56.493148: step 723, loss 1.02857, acc 0.625\n",
      "2017-04-03T22:11:56.683319: step 724, loss 1.4171, acc 0.546875\n",
      "2017-04-03T22:11:56.877558: step 725, loss 1.37268, acc 0.53125\n",
      "2017-04-03T22:11:57.069326: step 726, loss 1.07616, acc 0.625\n",
      "2017-04-03T22:11:57.259993: step 727, loss 1.12175, acc 0.5625\n",
      "2017-04-03T22:11:57.451496: step 728, loss 0.899672, acc 0.6875\n",
      "2017-04-03T22:11:57.643187: step 729, loss 1.13839, acc 0.609375\n",
      "2017-04-03T22:11:57.836095: step 730, loss 1.14496, acc 0.53125\n",
      "2017-04-03T22:11:58.028793: step 731, loss 1.02892, acc 0.609375\n",
      "2017-04-03T22:11:58.222912: step 732, loss 1.08807, acc 0.5625\n",
      "2017-04-03T22:11:58.421118: step 733, loss 1.04928, acc 0.65625\n",
      "2017-04-03T22:11:58.616183: step 734, loss 0.924951, acc 0.640625\n",
      "2017-04-03T22:11:58.809032: step 735, loss 1.20893, acc 0.546875\n",
      "2017-04-03T22:11:59.007685: step 736, loss 1.05657, acc 0.640625\n",
      "2017-04-03T22:11:59.199474: step 737, loss 1.0878, acc 0.640625\n",
      "2017-04-03T22:11:59.392988: step 738, loss 1.19534, acc 0.515625\n",
      "2017-04-03T22:11:59.584122: step 739, loss 1.33871, acc 0.5625\n",
      "2017-04-03T22:11:59.777187: step 740, loss 1.00641, acc 0.6875\n",
      "2017-04-03T22:11:59.974006: step 741, loss 0.981834, acc 0.578125\n",
      "2017-04-03T22:12:00.165299: step 742, loss 1.16371, acc 0.578125\n",
      "2017-04-03T22:12:00.362512: step 743, loss 0.98431, acc 0.6875\n",
      "2017-04-03T22:12:00.553747: step 744, loss 1.04542, acc 0.6875\n",
      "2017-04-03T22:12:00.750548: step 745, loss 1.1492, acc 0.578125\n",
      "2017-04-03T22:12:00.942376: step 746, loss 1.03242, acc 0.671875\n",
      "2017-04-03T22:12:01.133124: step 747, loss 0.841491, acc 0.71875\n",
      "2017-04-03T22:12:01.325896: step 748, loss 1.17402, acc 0.5625\n",
      "2017-04-03T22:12:01.518617: step 749, loss 0.82928, acc 0.765625\n",
      "2017-04-03T22:12:01.709091: step 750, loss 1.49294, acc 0.515625\n",
      "2017-04-03T22:12:01.901777: step 751, loss 1.04108, acc 0.625\n",
      "2017-04-03T22:12:02.094068: step 752, loss 1.1196, acc 0.59375\n",
      "2017-04-03T22:12:02.285728: step 753, loss 1.03592, acc 0.640625\n",
      "2017-04-03T22:12:02.479080: step 754, loss 1.17571, acc 0.53125\n",
      "2017-04-03T22:12:02.671388: step 755, loss 1.10341, acc 0.578125\n",
      "2017-04-03T22:12:02.831971: step 756, loss 1.03146, acc 0.576923\n",
      "2017-04-03T22:12:03.020983: step 757, loss 0.924604, acc 0.625\n",
      "2017-04-03T22:12:03.215140: step 758, loss 0.896642, acc 0.75\n",
      "2017-04-03T22:12:03.411688: step 759, loss 0.811037, acc 0.734375\n",
      "2017-04-03T22:12:03.604142: step 760, loss 0.934578, acc 0.65625\n",
      "2017-04-03T22:12:03.798004: step 761, loss 1.13653, acc 0.578125\n",
      "2017-04-03T22:12:03.988066: step 762, loss 0.908336, acc 0.765625\n",
      "2017-04-03T22:12:04.178655: step 763, loss 1.00044, acc 0.625\n",
      "2017-04-03T22:12:04.369757: step 764, loss 0.845383, acc 0.734375\n",
      "2017-04-03T22:12:04.559956: step 765, loss 0.773269, acc 0.75\n",
      "2017-04-03T22:12:04.754317: step 766, loss 0.906436, acc 0.6875\n",
      "2017-04-03T22:12:04.945420: step 767, loss 0.948678, acc 0.6875\n",
      "2017-04-03T22:12:05.135885: step 768, loss 1.0047, acc 0.640625\n",
      "2017-04-03T22:12:05.327373: step 769, loss 0.962175, acc 0.59375\n",
      "2017-04-03T22:12:05.520956: step 770, loss 1.2402, acc 0.578125\n",
      "2017-04-03T22:12:05.716789: step 771, loss 1.04473, acc 0.609375\n",
      "2017-04-03T22:12:05.911774: step 772, loss 0.837759, acc 0.734375\n",
      "2017-04-03T22:12:06.105406: step 773, loss 1.04034, acc 0.640625\n",
      "2017-04-03T22:12:06.296766: step 774, loss 1.05279, acc 0.640625\n",
      "2017-04-03T22:12:06.487865: step 775, loss 0.93527, acc 0.703125\n",
      "2017-04-03T22:12:06.681227: step 776, loss 1.01181, acc 0.65625\n",
      "2017-04-03T22:12:06.871489: step 777, loss 0.864937, acc 0.71875\n",
      "2017-04-03T22:12:07.062297: step 778, loss 1.13803, acc 0.640625\n",
      "2017-04-03T22:12:07.254713: step 779, loss 0.929965, acc 0.59375\n",
      "2017-04-03T22:12:07.445996: step 780, loss 0.812533, acc 0.703125\n",
      "2017-04-03T22:12:07.645637: step 781, loss 1.02498, acc 0.625\n",
      "2017-04-03T22:12:07.837615: step 782, loss 0.952819, acc 0.671875\n",
      "2017-04-03T22:12:08.037233: step 783, loss 0.9255, acc 0.625\n",
      "2017-04-03T22:12:08.236533: step 784, loss 1.09831, acc 0.65625\n",
      "2017-04-03T22:12:08.426599: step 785, loss 0.996894, acc 0.625\n",
      "2017-04-03T22:12:08.615505: step 786, loss 0.806491, acc 0.65625\n",
      "2017-04-03T22:12:08.809636: step 787, loss 1.13091, acc 0.5625\n",
      "2017-04-03T22:12:09.001952: step 788, loss 1.19694, acc 0.625\n",
      "2017-04-03T22:12:09.196983: step 789, loss 1.00057, acc 0.671875\n",
      "2017-04-03T22:12:09.389237: step 790, loss 0.740717, acc 0.796875\n",
      "2017-04-03T22:12:09.583082: step 791, loss 0.990024, acc 0.640625\n",
      "2017-04-03T22:12:09.773554: step 792, loss 1.1797, acc 0.578125\n",
      "2017-04-03T22:12:09.965416: step 793, loss 1.01406, acc 0.625\n",
      "2017-04-03T22:12:10.157521: step 794, loss 0.861738, acc 0.75\n",
      "2017-04-03T22:12:10.347879: step 795, loss 0.871387, acc 0.734375\n",
      "2017-04-03T22:12:10.543163: step 796, loss 0.843569, acc 0.71875\n",
      "2017-04-03T22:12:10.733317: step 797, loss 0.903171, acc 0.734375\n",
      "2017-04-03T22:12:10.921963: step 798, loss 1.22272, acc 0.578125\n",
      "2017-04-03T22:12:11.119304: step 799, loss 1.04381, acc 0.578125\n",
      "2017-04-03T22:12:11.312275: step 800, loss 1.03232, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:12:11.959485: step 800, loss 1.38025, acc 0.506527\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-800\n",
      "\n",
      "2017-04-03T22:12:12.211363: step 801, loss 0.936908, acc 0.765625\n",
      "2017-04-03T22:12:12.404250: step 802, loss 0.910493, acc 0.6875\n",
      "2017-04-03T22:12:12.597007: step 803, loss 1.05553, acc 0.65625\n",
      "2017-04-03T22:12:12.788405: step 804, loss 0.745135, acc 0.734375\n",
      "2017-04-03T22:12:12.979735: step 805, loss 1.05015, acc 0.65625\n",
      "2017-04-03T22:12:13.174881: step 806, loss 0.784571, acc 0.796875\n",
      "2017-04-03T22:12:13.368747: step 807, loss 0.88618, acc 0.6875\n",
      "2017-04-03T22:12:13.565003: step 808, loss 0.972841, acc 0.71875\n",
      "2017-04-03T22:12:13.760311: step 809, loss 1.30854, acc 0.53125\n",
      "2017-04-03T22:12:13.949283: step 810, loss 0.916576, acc 0.6875\n",
      "2017-04-03T22:12:14.139535: step 811, loss 1.01921, acc 0.640625\n",
      "2017-04-03T22:12:14.333467: step 812, loss 0.872326, acc 0.765625\n",
      "2017-04-03T22:12:14.526058: step 813, loss 0.995982, acc 0.703125\n",
      "2017-04-03T22:12:14.719525: step 814, loss 1.14608, acc 0.59375\n",
      "2017-04-03T22:12:14.918330: step 815, loss 1.00116, acc 0.640625\n",
      "2017-04-03T22:12:15.106534: step 816, loss 0.903599, acc 0.6875\n",
      "2017-04-03T22:12:15.298674: step 817, loss 0.924496, acc 0.75\n",
      "2017-04-03T22:12:15.489947: step 818, loss 0.922572, acc 0.6875\n",
      "2017-04-03T22:12:15.679501: step 819, loss 1.01808, acc 0.625\n",
      "2017-04-03T22:12:15.869386: step 820, loss 1.05325, acc 0.59375\n",
      "2017-04-03T22:12:16.060363: step 821, loss 0.875027, acc 0.6875\n",
      "2017-04-03T22:12:16.253218: step 822, loss 1.21521, acc 0.578125\n",
      "2017-04-03T22:12:16.448231: step 823, loss 1.02442, acc 0.609375\n",
      "2017-04-03T22:12:16.642739: step 824, loss 0.932519, acc 0.71875\n",
      "2017-04-03T22:12:16.836304: step 825, loss 0.926936, acc 0.625\n",
      "2017-04-03T22:12:17.027710: step 826, loss 1.0405, acc 0.5625\n",
      "2017-04-03T22:12:17.218936: step 827, loss 1.12515, acc 0.609375\n",
      "2017-04-03T22:12:17.410801: step 828, loss 0.870975, acc 0.703125\n",
      "2017-04-03T22:12:17.610466: step 829, loss 0.765305, acc 0.78125\n",
      "2017-04-03T22:12:17.804882: step 830, loss 0.806882, acc 0.703125\n",
      "2017-04-03T22:12:17.996572: step 831, loss 1.10343, acc 0.59375\n",
      "2017-04-03T22:12:18.187320: step 832, loss 0.891664, acc 0.71875\n",
      "2017-04-03T22:12:18.375074: step 833, loss 1.11215, acc 0.609375\n",
      "2017-04-03T22:12:18.566225: step 834, loss 1.03445, acc 0.609375\n",
      "2017-04-03T22:12:18.760682: step 835, loss 0.874695, acc 0.703125\n",
      "2017-04-03T22:12:18.953299: step 836, loss 0.931742, acc 0.625\n",
      "2017-04-03T22:12:19.141028: step 837, loss 0.825555, acc 0.65625\n",
      "2017-04-03T22:12:19.332418: step 838, loss 0.850798, acc 0.78125\n",
      "2017-04-03T22:12:19.527749: step 839, loss 1.01898, acc 0.640625\n",
      "2017-04-03T22:12:19.719795: step 840, loss 0.841649, acc 0.6875\n",
      "2017-04-03T22:12:19.910106: step 841, loss 1.07157, acc 0.578125\n",
      "2017-04-03T22:12:20.106543: step 842, loss 1.00378, acc 0.671875\n",
      "2017-04-03T22:12:20.298312: step 843, loss 1.05385, acc 0.546875\n",
      "2017-04-03T22:12:20.490747: step 844, loss 1.02548, acc 0.625\n",
      "2017-04-03T22:12:20.688322: step 845, loss 0.821979, acc 0.75\n",
      "2017-04-03T22:12:20.880641: step 846, loss 0.909743, acc 0.671875\n",
      "2017-04-03T22:12:21.070406: step 847, loss 1.28297, acc 0.484375\n",
      "2017-04-03T22:12:21.261864: step 848, loss 0.828367, acc 0.734375\n",
      "2017-04-03T22:12:21.452306: step 849, loss 0.866016, acc 0.703125\n",
      "2017-04-03T22:12:21.649609: step 850, loss 0.891057, acc 0.71875\n",
      "2017-04-03T22:12:21.845048: step 851, loss 1.14061, acc 0.53125\n",
      "2017-04-03T22:12:22.032944: step 852, loss 0.816213, acc 0.703125\n",
      "2017-04-03T22:12:22.229994: step 853, loss 1.00943, acc 0.671875\n",
      "2017-04-03T22:12:22.429052: step 854, loss 0.92054, acc 0.65625\n",
      "2017-04-03T22:12:22.622192: step 855, loss 1.18336, acc 0.546875\n",
      "2017-04-03T22:12:22.810948: step 856, loss 0.944566, acc 0.703125\n",
      "2017-04-03T22:12:23.003336: step 857, loss 0.896932, acc 0.703125\n",
      "2017-04-03T22:12:23.196916: step 858, loss 0.91666, acc 0.671875\n",
      "2017-04-03T22:12:23.391063: step 859, loss 0.950912, acc 0.6875\n",
      "2017-04-03T22:12:23.584259: step 860, loss 0.997404, acc 0.671875\n",
      "2017-04-03T22:12:23.778053: step 861, loss 0.79072, acc 0.75\n",
      "2017-04-03T22:12:23.974019: step 862, loss 0.954722, acc 0.671875\n",
      "2017-04-03T22:12:24.176190: step 863, loss 1.07629, acc 0.65625\n",
      "2017-04-03T22:12:24.336191: step 864, loss 1.22405, acc 0.576923\n",
      "2017-04-03T22:12:24.532768: step 865, loss 0.860491, acc 0.75\n",
      "2017-04-03T22:12:24.744594: step 866, loss 0.903762, acc 0.71875\n",
      "2017-04-03T22:12:24.951935: step 867, loss 0.83385, acc 0.6875\n",
      "2017-04-03T22:12:25.164231: step 868, loss 0.718745, acc 0.828125\n",
      "2017-04-03T22:12:25.375953: step 869, loss 0.832914, acc 0.703125\n",
      "2017-04-03T22:12:25.574917: step 870, loss 0.930166, acc 0.703125\n",
      "2017-04-03T22:12:25.785552: step 871, loss 0.777153, acc 0.78125\n",
      "2017-04-03T22:12:25.978751: step 872, loss 0.832398, acc 0.71875\n",
      "2017-04-03T22:12:26.172622: step 873, loss 0.866216, acc 0.703125\n",
      "2017-04-03T22:12:26.363012: step 874, loss 0.822365, acc 0.6875\n",
      "2017-04-03T22:12:26.554256: step 875, loss 0.800369, acc 0.671875\n",
      "2017-04-03T22:12:26.748735: step 876, loss 1.04559, acc 0.59375\n",
      "2017-04-03T22:12:26.940936: step 877, loss 0.913193, acc 0.71875\n",
      "2017-04-03T22:12:27.139226: step 878, loss 0.74993, acc 0.78125\n",
      "2017-04-03T22:12:27.327707: step 879, loss 0.723592, acc 0.734375\n",
      "2017-04-03T22:12:27.519863: step 880, loss 0.930853, acc 0.640625\n",
      "2017-04-03T22:12:27.715851: step 881, loss 0.986828, acc 0.6875\n",
      "2017-04-03T22:12:27.907927: step 882, loss 0.66356, acc 0.796875\n",
      "2017-04-03T22:12:28.100239: step 883, loss 0.89927, acc 0.71875\n",
      "2017-04-03T22:12:28.295306: step 884, loss 0.823933, acc 0.65625\n",
      "2017-04-03T22:12:28.490903: step 885, loss 0.961876, acc 0.703125\n",
      "2017-04-03T22:12:28.684058: step 886, loss 0.697942, acc 0.734375\n",
      "2017-04-03T22:12:28.880515: step 887, loss 0.911794, acc 0.703125\n",
      "2017-04-03T22:12:29.073505: step 888, loss 0.712389, acc 0.75\n",
      "2017-04-03T22:12:29.265603: step 889, loss 0.889072, acc 0.703125\n",
      "2017-04-03T22:12:29.457162: step 890, loss 0.97503, acc 0.640625\n",
      "2017-04-03T22:12:29.650014: step 891, loss 0.795526, acc 0.734375\n",
      "2017-04-03T22:12:29.841258: step 892, loss 0.911943, acc 0.671875\n",
      "2017-04-03T22:12:30.034077: step 893, loss 1.0329, acc 0.65625\n",
      "2017-04-03T22:12:30.223418: step 894, loss 0.748633, acc 0.78125\n",
      "2017-04-03T22:12:30.418278: step 895, loss 0.964845, acc 0.65625\n",
      "2017-04-03T22:12:30.610736: step 896, loss 0.869746, acc 0.734375\n",
      "2017-04-03T22:12:30.799786: step 897, loss 0.916925, acc 0.6875\n",
      "2017-04-03T22:12:30.991087: step 898, loss 0.890382, acc 0.734375\n",
      "2017-04-03T22:12:31.183650: step 899, loss 0.901273, acc 0.71875\n",
      "2017-04-03T22:12:31.376826: step 900, loss 0.84924, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:12:32.043105: step 900, loss 1.34629, acc 0.519582\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-900\n",
      "\n",
      "2017-04-03T22:12:32.301637: step 901, loss 0.737098, acc 0.765625\n",
      "2017-04-03T22:12:32.496313: step 902, loss 0.827236, acc 0.71875\n",
      "2017-04-03T22:12:32.689161: step 903, loss 0.656545, acc 0.78125\n",
      "2017-04-03T22:12:32.879249: step 904, loss 0.934177, acc 0.640625\n",
      "2017-04-03T22:12:33.072961: step 905, loss 0.620835, acc 0.796875\n",
      "2017-04-03T22:12:33.264408: step 906, loss 0.798485, acc 0.703125\n",
      "2017-04-03T22:12:33.459311: step 907, loss 0.765684, acc 0.734375\n",
      "2017-04-03T22:12:33.657484: step 908, loss 0.93819, acc 0.65625\n",
      "2017-04-03T22:12:33.850711: step 909, loss 0.753944, acc 0.75\n",
      "2017-04-03T22:12:34.042221: step 910, loss 0.852433, acc 0.65625\n",
      "2017-04-03T22:12:34.234523: step 911, loss 0.70526, acc 0.734375\n",
      "2017-04-03T22:12:34.423158: step 912, loss 0.71426, acc 0.765625\n",
      "2017-04-03T22:12:34.616875: step 913, loss 0.946519, acc 0.65625\n",
      "2017-04-03T22:12:34.812396: step 914, loss 0.799048, acc 0.71875\n",
      "2017-04-03T22:12:35.003504: step 915, loss 0.720208, acc 0.78125\n",
      "2017-04-03T22:12:35.195718: step 916, loss 0.867981, acc 0.71875\n",
      "2017-04-03T22:12:35.388934: step 917, loss 0.784243, acc 0.71875\n",
      "2017-04-03T22:12:35.582698: step 918, loss 0.686507, acc 0.8125\n",
      "2017-04-03T22:12:35.776240: step 919, loss 1.01885, acc 0.703125\n",
      "2017-04-03T22:12:35.967621: step 920, loss 0.71373, acc 0.796875\n",
      "2017-04-03T22:12:36.161338: step 921, loss 1.1536, acc 0.625\n",
      "2017-04-03T22:12:36.353244: step 922, loss 0.848719, acc 0.671875\n",
      "2017-04-03T22:12:36.547604: step 923, loss 0.825249, acc 0.65625\n",
      "2017-04-03T22:12:36.741514: step 924, loss 0.729852, acc 0.75\n",
      "2017-04-03T22:12:36.936000: step 925, loss 0.783527, acc 0.703125\n",
      "2017-04-03T22:12:37.126407: step 926, loss 0.971789, acc 0.609375\n",
      "2017-04-03T22:12:37.317574: step 927, loss 0.859499, acc 0.75\n",
      "2017-04-03T22:12:37.508727: step 928, loss 0.881771, acc 0.640625\n",
      "2017-04-03T22:12:37.698448: step 929, loss 0.910572, acc 0.609375\n",
      "2017-04-03T22:12:37.888702: step 930, loss 0.817176, acc 0.71875\n",
      "2017-04-03T22:12:38.081017: step 931, loss 0.896044, acc 0.765625\n",
      "2017-04-03T22:12:38.275646: step 932, loss 0.85366, acc 0.625\n",
      "2017-04-03T22:12:38.469386: step 933, loss 0.619554, acc 0.796875\n",
      "2017-04-03T22:12:38.670257: step 934, loss 0.752794, acc 0.71875\n",
      "2017-04-03T22:12:38.867236: step 935, loss 0.93054, acc 0.625\n",
      "2017-04-03T22:12:39.065106: step 936, loss 0.86984, acc 0.734375\n",
      "2017-04-03T22:12:39.258592: step 937, loss 0.809658, acc 0.671875\n",
      "2017-04-03T22:12:39.450800: step 938, loss 1.01056, acc 0.65625\n",
      "2017-04-03T22:12:39.647823: step 939, loss 0.691827, acc 0.796875\n",
      "2017-04-03T22:12:39.845680: step 940, loss 0.665868, acc 0.734375\n",
      "2017-04-03T22:12:40.039208: step 941, loss 0.994953, acc 0.625\n",
      "2017-04-03T22:12:40.230510: step 942, loss 0.776199, acc 0.765625\n",
      "2017-04-03T22:12:40.423867: step 943, loss 0.879951, acc 0.703125\n",
      "2017-04-03T22:12:40.617922: step 944, loss 0.70631, acc 0.765625\n",
      "2017-04-03T22:12:40.805276: step 945, loss 1.04818, acc 0.609375\n",
      "2017-04-03T22:12:40.999003: step 946, loss 0.809233, acc 0.671875\n",
      "2017-04-03T22:12:41.190014: step 947, loss 0.950141, acc 0.671875\n",
      "2017-04-03T22:12:41.381105: step 948, loss 0.689902, acc 0.78125\n",
      "2017-04-03T22:12:41.572349: step 949, loss 0.669745, acc 0.8125\n",
      "2017-04-03T22:12:41.767318: step 950, loss 0.841982, acc 0.703125\n",
      "2017-04-03T22:12:41.959899: step 951, loss 0.832604, acc 0.765625\n",
      "2017-04-03T22:12:42.151619: step 952, loss 0.896362, acc 0.71875\n",
      "2017-04-03T22:12:42.341870: step 953, loss 0.917882, acc 0.640625\n",
      "2017-04-03T22:12:42.536379: step 954, loss 0.941088, acc 0.71875\n",
      "2017-04-03T22:12:42.727142: step 955, loss 0.805914, acc 0.6875\n",
      "2017-04-03T22:12:42.919228: step 956, loss 0.95092, acc 0.65625\n",
      "2017-04-03T22:12:43.115683: step 957, loss 0.951497, acc 0.671875\n",
      "2017-04-03T22:12:43.306877: step 958, loss 0.770398, acc 0.71875\n",
      "2017-04-03T22:12:43.499893: step 959, loss 0.969626, acc 0.625\n",
      "2017-04-03T22:12:43.690928: step 960, loss 0.875554, acc 0.671875\n",
      "2017-04-03T22:12:43.885696: step 961, loss 0.791352, acc 0.765625\n",
      "2017-04-03T22:12:44.079723: step 962, loss 0.994855, acc 0.640625\n",
      "2017-04-03T22:12:44.272604: step 963, loss 0.802891, acc 0.75\n",
      "2017-04-03T22:12:44.467783: step 964, loss 0.839036, acc 0.75\n",
      "2017-04-03T22:12:44.667441: step 965, loss 0.71415, acc 0.75\n",
      "2017-04-03T22:12:44.864804: step 966, loss 1.07765, acc 0.5625\n",
      "2017-04-03T22:12:45.057255: step 967, loss 0.902957, acc 0.71875\n",
      "2017-04-03T22:12:45.249541: step 968, loss 0.971985, acc 0.65625\n",
      "2017-04-03T22:12:45.443317: step 969, loss 0.852444, acc 0.75\n",
      "2017-04-03T22:12:45.633963: step 970, loss 0.974132, acc 0.65625\n",
      "2017-04-03T22:12:45.828675: step 971, loss 0.836091, acc 0.71875\n",
      "2017-04-03T22:12:45.992133: step 972, loss 0.820014, acc 0.711538\n",
      "2017-04-03T22:12:46.184829: step 973, loss 1.00701, acc 0.65625\n",
      "2017-04-03T22:12:46.375091: step 974, loss 0.900809, acc 0.671875\n",
      "2017-04-03T22:12:46.567495: step 975, loss 0.63566, acc 0.796875\n",
      "2017-04-03T22:12:46.759933: step 976, loss 0.830321, acc 0.671875\n",
      "2017-04-03T22:12:46.951274: step 977, loss 0.869932, acc 0.65625\n",
      "2017-04-03T22:12:47.144415: step 978, loss 0.706282, acc 0.828125\n",
      "2017-04-03T22:12:47.339076: step 979, loss 0.656075, acc 0.78125\n",
      "2017-04-03T22:12:47.531726: step 980, loss 0.732823, acc 0.734375\n",
      "2017-04-03T22:12:47.723939: step 981, loss 0.523265, acc 0.84375\n",
      "2017-04-03T22:12:47.915605: step 982, loss 0.556246, acc 0.828125\n",
      "2017-04-03T22:12:48.107192: step 983, loss 0.705852, acc 0.75\n",
      "2017-04-03T22:12:48.302487: step 984, loss 0.67033, acc 0.734375\n",
      "2017-04-03T22:12:48.492445: step 985, loss 0.888132, acc 0.671875\n",
      "2017-04-03T22:12:48.687603: step 986, loss 0.779277, acc 0.765625\n",
      "2017-04-03T22:12:48.876343: step 987, loss 0.906186, acc 0.65625\n",
      "2017-04-03T22:12:49.067226: step 988, loss 0.904066, acc 0.703125\n",
      "2017-04-03T22:12:49.258461: step 989, loss 0.847419, acc 0.71875\n",
      "2017-04-03T22:12:49.450747: step 990, loss 0.789945, acc 0.734375\n",
      "2017-04-03T22:12:49.654087: step 991, loss 0.52478, acc 0.890625\n",
      "2017-04-03T22:12:49.845379: step 992, loss 0.827741, acc 0.65625\n",
      "2017-04-03T22:12:50.036985: step 993, loss 0.681067, acc 0.78125\n",
      "2017-04-03T22:12:50.229234: step 994, loss 0.718428, acc 0.75\n",
      "2017-04-03T22:12:50.425728: step 995, loss 0.571012, acc 0.84375\n",
      "2017-04-03T22:12:50.615456: step 996, loss 0.95099, acc 0.625\n",
      "2017-04-03T22:12:50.808473: step 997, loss 0.875945, acc 0.65625\n",
      "2017-04-03T22:12:50.996810: step 998, loss 0.690066, acc 0.828125\n",
      "2017-04-03T22:12:51.188169: step 999, loss 0.735448, acc 0.71875\n",
      "2017-04-03T22:12:51.382035: step 1000, loss 0.755029, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:12:52.038026: step 1000, loss 1.33693, acc 0.515666\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1000\n",
      "\n",
      "2017-04-03T22:12:52.291760: step 1001, loss 0.744671, acc 0.71875\n",
      "2017-04-03T22:12:52.484697: step 1002, loss 0.680844, acc 0.765625\n",
      "2017-04-03T22:12:52.678543: step 1003, loss 0.732935, acc 0.75\n",
      "2017-04-03T22:12:52.870940: step 1004, loss 0.778112, acc 0.78125\n",
      "2017-04-03T22:12:53.065513: step 1005, loss 0.914079, acc 0.671875\n",
      "2017-04-03T22:12:53.255266: step 1006, loss 0.673059, acc 0.75\n",
      "2017-04-03T22:12:53.450707: step 1007, loss 0.739372, acc 0.765625\n",
      "2017-04-03T22:12:53.642982: step 1008, loss 0.735165, acc 0.765625\n",
      "2017-04-03T22:12:53.835605: step 1009, loss 0.700663, acc 0.765625\n",
      "2017-04-03T22:12:54.029153: step 1010, loss 0.706583, acc 0.8125\n",
      "2017-04-03T22:12:54.223046: step 1011, loss 0.858604, acc 0.671875\n",
      "2017-04-03T22:12:54.416041: step 1012, loss 0.681956, acc 0.765625\n",
      "2017-04-03T22:12:54.608240: step 1013, loss 0.722172, acc 0.734375\n",
      "2017-04-03T22:12:54.802262: step 1014, loss 0.925672, acc 0.625\n",
      "2017-04-03T22:12:54.998758: step 1015, loss 0.721849, acc 0.734375\n",
      "2017-04-03T22:12:55.190731: step 1016, loss 0.686435, acc 0.765625\n",
      "2017-04-03T22:12:55.391053: step 1017, loss 0.604303, acc 0.765625\n",
      "2017-04-03T22:12:55.592247: step 1018, loss 0.684675, acc 0.78125\n",
      "2017-04-03T22:12:55.787692: step 1019, loss 0.802656, acc 0.6875\n",
      "2017-04-03T22:12:55.980445: step 1020, loss 0.693175, acc 0.84375\n",
      "2017-04-03T22:12:56.179087: step 1021, loss 0.614086, acc 0.78125\n",
      "2017-04-03T22:12:56.381446: step 1022, loss 0.675134, acc 0.828125\n",
      "2017-04-03T22:12:56.579423: step 1023, loss 0.784684, acc 0.671875\n",
      "2017-04-03T22:12:56.786173: step 1024, loss 0.631469, acc 0.796875\n",
      "2017-04-03T22:12:56.994621: step 1025, loss 0.870208, acc 0.703125\n",
      "2017-04-03T22:12:57.192588: step 1026, loss 0.582565, acc 0.75\n",
      "2017-04-03T22:12:57.412142: step 1027, loss 0.619324, acc 0.796875\n",
      "2017-04-03T22:12:57.613288: step 1028, loss 0.766185, acc 0.796875\n",
      "2017-04-03T22:12:57.812089: step 1029, loss 0.658237, acc 0.78125\n",
      "2017-04-03T22:12:58.023395: step 1030, loss 0.746494, acc 0.78125\n",
      "2017-04-03T22:12:58.222894: step 1031, loss 0.83352, acc 0.71875\n",
      "2017-04-03T22:12:58.451201: step 1032, loss 0.634802, acc 0.84375\n",
      "2017-04-03T22:12:58.676998: step 1033, loss 0.566169, acc 0.84375\n",
      "2017-04-03T22:12:58.890105: step 1034, loss 0.655897, acc 0.828125\n",
      "2017-04-03T22:12:59.101407: step 1035, loss 0.775932, acc 0.765625\n",
      "2017-04-03T22:12:59.295469: step 1036, loss 0.966682, acc 0.703125\n",
      "2017-04-03T22:12:59.488676: step 1037, loss 0.651507, acc 0.703125\n",
      "2017-04-03T22:12:59.678469: step 1038, loss 0.85171, acc 0.6875\n",
      "2017-04-03T22:12:59.873647: step 1039, loss 0.967857, acc 0.65625\n",
      "2017-04-03T22:13:00.068259: step 1040, loss 0.626047, acc 0.796875\n",
      "2017-04-03T22:13:00.260018: step 1041, loss 0.714121, acc 0.8125\n",
      "2017-04-03T22:13:00.456919: step 1042, loss 0.71232, acc 0.78125\n",
      "2017-04-03T22:13:00.656161: step 1043, loss 0.629367, acc 0.796875\n",
      "2017-04-03T22:13:00.851814: step 1044, loss 0.97435, acc 0.625\n",
      "2017-04-03T22:13:01.047309: step 1045, loss 0.878005, acc 0.703125\n",
      "2017-04-03T22:13:01.236754: step 1046, loss 0.761304, acc 0.765625\n",
      "2017-04-03T22:13:01.431094: step 1047, loss 0.903103, acc 0.6875\n",
      "2017-04-03T22:13:01.632144: step 1048, loss 0.716567, acc 0.734375\n",
      "2017-04-03T22:13:01.823669: step 1049, loss 0.736128, acc 0.78125\n",
      "2017-04-03T22:13:02.023757: step 1050, loss 0.724452, acc 0.71875\n",
      "2017-04-03T22:13:02.214979: step 1051, loss 0.823912, acc 0.671875\n",
      "2017-04-03T22:13:02.408653: step 1052, loss 0.978446, acc 0.609375\n",
      "2017-04-03T22:13:02.604994: step 1053, loss 0.880019, acc 0.703125\n",
      "2017-04-03T22:13:02.801792: step 1054, loss 0.847508, acc 0.6875\n",
      "2017-04-03T22:13:02.998602: step 1055, loss 0.79154, acc 0.734375\n",
      "2017-04-03T22:13:03.190964: step 1056, loss 0.822295, acc 0.71875\n",
      "2017-04-03T22:13:03.382475: step 1057, loss 0.754391, acc 0.734375\n",
      "2017-04-03T22:13:03.577208: step 1058, loss 0.770076, acc 0.78125\n",
      "2017-04-03T22:13:03.767663: step 1059, loss 0.799583, acc 0.734375\n",
      "2017-04-03T22:13:03.963869: step 1060, loss 0.820703, acc 0.6875\n",
      "2017-04-03T22:13:04.157500: step 1061, loss 0.593481, acc 0.796875\n",
      "2017-04-03T22:13:04.346739: step 1062, loss 0.666656, acc 0.75\n",
      "2017-04-03T22:13:04.545487: step 1063, loss 0.766428, acc 0.78125\n",
      "2017-04-03T22:13:04.736711: step 1064, loss 0.914019, acc 0.71875\n",
      "2017-04-03T22:13:04.930666: step 1065, loss 0.825409, acc 0.671875\n",
      "2017-04-03T22:13:05.127221: step 1066, loss 0.877298, acc 0.71875\n",
      "2017-04-03T22:13:05.322117: step 1067, loss 0.814298, acc 0.671875\n",
      "2017-04-03T22:13:05.520493: step 1068, loss 0.852241, acc 0.75\n",
      "2017-04-03T22:13:05.710433: step 1069, loss 0.809038, acc 0.765625\n",
      "2017-04-03T22:13:05.897680: step 1070, loss 0.739036, acc 0.734375\n",
      "2017-04-03T22:13:06.097753: step 1071, loss 0.769206, acc 0.765625\n",
      "2017-04-03T22:13:06.289701: step 1072, loss 0.683869, acc 0.796875\n",
      "2017-04-03T22:13:06.488424: step 1073, loss 0.774886, acc 0.734375\n",
      "2017-04-03T22:13:06.676491: step 1074, loss 0.742021, acc 0.765625\n",
      "2017-04-03T22:13:06.871548: step 1075, loss 0.727264, acc 0.75\n",
      "2017-04-03T22:13:07.073329: step 1076, loss 0.60499, acc 0.796875\n",
      "2017-04-03T22:13:07.270687: step 1077, loss 0.856535, acc 0.765625\n",
      "2017-04-03T22:13:07.463571: step 1078, loss 0.736275, acc 0.75\n",
      "2017-04-03T22:13:07.663554: step 1079, loss 0.770286, acc 0.78125\n",
      "2017-04-03T22:13:07.825237: step 1080, loss 0.567416, acc 0.846154\n",
      "2017-04-03T22:13:08.021699: step 1081, loss 0.74588, acc 0.734375\n",
      "2017-04-03T22:13:08.218947: step 1082, loss 0.80093, acc 0.6875\n",
      "2017-04-03T22:13:08.409380: step 1083, loss 0.671579, acc 0.75\n",
      "2017-04-03T22:13:08.602543: step 1084, loss 0.665855, acc 0.75\n",
      "2017-04-03T22:13:08.815311: step 1085, loss 0.713375, acc 0.75\n",
      "2017-04-03T22:13:09.015591: step 1086, loss 0.729061, acc 0.78125\n",
      "2017-04-03T22:13:09.222163: step 1087, loss 0.489433, acc 0.921875\n",
      "2017-04-03T22:13:09.445731: step 1088, loss 0.741915, acc 0.734375\n",
      "2017-04-03T22:13:09.643242: step 1089, loss 0.643882, acc 0.828125\n",
      "2017-04-03T22:13:09.850337: step 1090, loss 0.694488, acc 0.78125\n",
      "2017-04-03T22:13:10.058698: step 1091, loss 0.575624, acc 0.828125\n",
      "2017-04-03T22:13:10.262338: step 1092, loss 0.585914, acc 0.875\n",
      "2017-04-03T22:13:10.453771: step 1093, loss 0.6006, acc 0.8125\n",
      "2017-04-03T22:13:10.651007: step 1094, loss 0.662145, acc 0.796875\n",
      "2017-04-03T22:13:10.852340: step 1095, loss 0.57213, acc 0.796875\n",
      "2017-04-03T22:13:11.047085: step 1096, loss 0.672292, acc 0.765625\n",
      "2017-04-03T22:13:11.236208: step 1097, loss 0.550303, acc 0.828125\n",
      "2017-04-03T22:13:11.429057: step 1098, loss 0.581587, acc 0.8125\n",
      "2017-04-03T22:13:11.619673: step 1099, loss 0.564607, acc 0.828125\n",
      "2017-04-03T22:13:11.823720: step 1100, loss 0.649561, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:13:12.502467: step 1100, loss 1.35918, acc 0.52611\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1100\n",
      "\n",
      "2017-04-03T22:13:12.781057: step 1101, loss 0.684443, acc 0.78125\n",
      "2017-04-03T22:13:12.983880: step 1102, loss 0.465461, acc 0.84375\n",
      "2017-04-03T22:13:13.184826: step 1103, loss 0.632982, acc 0.8125\n",
      "2017-04-03T22:13:13.383014: step 1104, loss 0.695446, acc 0.75\n",
      "2017-04-03T22:13:13.583222: step 1105, loss 0.689835, acc 0.78125\n",
      "2017-04-03T22:13:13.805142: step 1106, loss 0.741687, acc 0.765625\n",
      "2017-04-03T22:13:14.019500: step 1107, loss 0.544365, acc 0.859375\n",
      "2017-04-03T22:13:14.231962: step 1108, loss 0.587276, acc 0.828125\n",
      "2017-04-03T22:13:14.444007: step 1109, loss 0.691681, acc 0.78125\n",
      "2017-04-03T22:13:14.658480: step 1110, loss 0.431024, acc 0.890625\n",
      "2017-04-03T22:13:14.867943: step 1111, loss 0.687961, acc 0.796875\n",
      "2017-04-03T22:13:15.074538: step 1112, loss 0.821916, acc 0.71875\n",
      "2017-04-03T22:13:15.284253: step 1113, loss 0.67588, acc 0.78125\n",
      "2017-04-03T22:13:15.494976: step 1114, loss 0.641607, acc 0.765625\n",
      "2017-04-03T22:13:15.710866: step 1115, loss 0.662007, acc 0.75\n",
      "2017-04-03T22:13:15.923039: step 1116, loss 0.65696, acc 0.796875\n",
      "2017-04-03T22:13:16.133159: step 1117, loss 0.653272, acc 0.828125\n",
      "2017-04-03T22:13:16.338928: step 1118, loss 0.565631, acc 0.859375\n",
      "2017-04-03T22:13:16.559013: step 1119, loss 0.623723, acc 0.765625\n",
      "2017-04-03T22:13:16.770144: step 1120, loss 0.376828, acc 0.921875\n",
      "2017-04-03T22:13:16.986557: step 1121, loss 0.60188, acc 0.8125\n",
      "2017-04-03T22:13:17.200320: step 1122, loss 0.498194, acc 0.8125\n",
      "2017-04-03T22:13:17.420123: step 1123, loss 0.57142, acc 0.828125\n",
      "2017-04-03T22:13:17.637717: step 1124, loss 0.623356, acc 0.765625\n",
      "2017-04-03T22:13:17.862735: step 1125, loss 0.653932, acc 0.796875\n",
      "2017-04-03T22:13:18.073540: step 1126, loss 0.625798, acc 0.796875\n",
      "2017-04-03T22:13:18.285152: step 1127, loss 0.644778, acc 0.765625\n",
      "2017-04-03T22:13:18.497794: step 1128, loss 0.678888, acc 0.78125\n",
      "2017-04-03T22:13:18.716650: step 1129, loss 0.748352, acc 0.78125\n",
      "2017-04-03T22:13:18.928837: step 1130, loss 0.81343, acc 0.78125\n",
      "2017-04-03T22:13:19.151004: step 1131, loss 0.710809, acc 0.75\n",
      "2017-04-03T22:13:19.355416: step 1132, loss 0.416854, acc 0.859375\n",
      "2017-04-03T22:13:19.553833: step 1133, loss 0.725139, acc 0.75\n",
      "2017-04-03T22:13:19.751937: step 1134, loss 0.796714, acc 0.75\n",
      "2017-04-03T22:13:19.952446: step 1135, loss 0.704352, acc 0.78125\n",
      "2017-04-03T22:13:20.144860: step 1136, loss 0.65965, acc 0.78125\n",
      "2017-04-03T22:13:20.338969: step 1137, loss 0.694718, acc 0.734375\n",
      "2017-04-03T22:13:20.533111: step 1138, loss 0.724862, acc 0.734375\n",
      "2017-04-03T22:13:20.723821: step 1139, loss 0.630801, acc 0.78125\n",
      "2017-04-03T22:13:20.918348: step 1140, loss 0.463555, acc 0.890625\n",
      "2017-04-03T22:13:21.108803: step 1141, loss 0.534309, acc 0.859375\n",
      "2017-04-03T22:13:21.301132: step 1142, loss 0.557189, acc 0.84375\n",
      "2017-04-03T22:13:21.494205: step 1143, loss 0.792829, acc 0.75\n",
      "2017-04-03T22:13:21.689465: step 1144, loss 0.702217, acc 0.8125\n",
      "2017-04-03T22:13:21.883055: step 1145, loss 0.590698, acc 0.8125\n",
      "2017-04-03T22:13:22.073972: step 1146, loss 0.773045, acc 0.6875\n",
      "2017-04-03T22:13:22.266299: step 1147, loss 0.499704, acc 0.828125\n",
      "2017-04-03T22:13:22.455614: step 1148, loss 0.774311, acc 0.703125\n",
      "2017-04-03T22:13:22.643969: step 1149, loss 0.521795, acc 0.84375\n",
      "2017-04-03T22:13:22.837827: step 1150, loss 0.681286, acc 0.78125\n",
      "2017-04-03T22:13:23.029467: step 1151, loss 0.578711, acc 0.75\n",
      "2017-04-03T22:13:23.223202: step 1152, loss 0.754657, acc 0.734375\n",
      "2017-04-03T22:13:23.415609: step 1153, loss 0.796672, acc 0.75\n",
      "2017-04-03T22:13:23.607887: step 1154, loss 0.851268, acc 0.609375\n",
      "2017-04-03T22:13:23.799273: step 1155, loss 0.523828, acc 0.8125\n",
      "2017-04-03T22:13:23.991666: step 1156, loss 0.534466, acc 0.796875\n",
      "2017-04-03T22:13:24.184496: step 1157, loss 0.521041, acc 0.890625\n",
      "2017-04-03T22:13:24.377466: step 1158, loss 0.715924, acc 0.75\n",
      "2017-04-03T22:13:24.565590: step 1159, loss 0.704146, acc 0.78125\n",
      "2017-04-03T22:13:24.753260: step 1160, loss 0.579001, acc 0.78125\n",
      "2017-04-03T22:13:24.945271: step 1161, loss 0.658651, acc 0.765625\n",
      "2017-04-03T22:13:25.142065: step 1162, loss 0.751196, acc 0.765625\n",
      "2017-04-03T22:13:25.329946: step 1163, loss 0.866729, acc 0.6875\n",
      "2017-04-03T22:13:25.526426: step 1164, loss 0.78082, acc 0.734375\n",
      "2017-04-03T22:13:25.715485: step 1165, loss 0.746659, acc 0.734375\n",
      "2017-04-03T22:13:25.908771: step 1166, loss 0.72017, acc 0.796875\n",
      "2017-04-03T22:13:26.100504: step 1167, loss 0.590629, acc 0.859375\n",
      "2017-04-03T22:13:26.293527: step 1168, loss 0.689913, acc 0.75\n",
      "2017-04-03T22:13:26.484215: step 1169, loss 0.640382, acc 0.828125\n",
      "2017-04-03T22:13:26.676272: step 1170, loss 0.693749, acc 0.78125\n",
      "2017-04-03T22:13:26.869673: step 1171, loss 0.716699, acc 0.78125\n",
      "2017-04-03T22:13:27.062280: step 1172, loss 0.579728, acc 0.828125\n",
      "2017-04-03T22:13:27.253021: step 1173, loss 0.651864, acc 0.796875\n",
      "2017-04-03T22:13:27.444852: step 1174, loss 0.593337, acc 0.75\n",
      "2017-04-03T22:13:27.638231: step 1175, loss 0.57937, acc 0.828125\n",
      "2017-04-03T22:13:27.832375: step 1176, loss 0.949101, acc 0.640625\n",
      "2017-04-03T22:13:28.025933: step 1177, loss 0.668305, acc 0.765625\n",
      "2017-04-03T22:13:28.217940: step 1178, loss 0.714663, acc 0.75\n",
      "2017-04-03T22:13:28.410419: step 1179, loss 0.815968, acc 0.765625\n",
      "2017-04-03T22:13:28.602166: step 1180, loss 0.623309, acc 0.84375\n",
      "2017-04-03T22:13:28.796979: step 1181, loss 0.621502, acc 0.796875\n",
      "2017-04-03T22:13:28.989576: step 1182, loss 0.696411, acc 0.78125\n",
      "2017-04-03T22:13:29.181648: step 1183, loss 0.759421, acc 0.71875\n",
      "2017-04-03T22:13:29.373420: step 1184, loss 0.624744, acc 0.84375\n",
      "2017-04-03T22:13:29.565355: step 1185, loss 0.553681, acc 0.890625\n",
      "2017-04-03T22:13:29.753894: step 1186, loss 0.633523, acc 0.8125\n",
      "2017-04-03T22:13:29.946311: step 1187, loss 0.845517, acc 0.71875\n",
      "2017-04-03T22:13:30.107771: step 1188, loss 0.664201, acc 0.807692\n",
      "2017-04-03T22:13:30.303840: step 1189, loss 0.545554, acc 0.84375\n",
      "2017-04-03T22:13:30.497946: step 1190, loss 0.581467, acc 0.859375\n",
      "2017-04-03T22:13:30.685931: step 1191, loss 0.479711, acc 0.875\n",
      "2017-04-03T22:13:30.881073: step 1192, loss 0.486586, acc 0.90625\n",
      "2017-04-03T22:13:31.074525: step 1193, loss 0.5917, acc 0.84375\n",
      "2017-04-03T22:13:31.268174: step 1194, loss 0.534432, acc 0.84375\n",
      "2017-04-03T22:13:31.459945: step 1195, loss 0.601413, acc 0.796875\n",
      "2017-04-03T22:13:31.650490: step 1196, loss 0.350563, acc 0.953125\n",
      "2017-04-03T22:13:31.850435: step 1197, loss 0.561974, acc 0.8125\n",
      "2017-04-03T22:13:32.044309: step 1198, loss 0.600013, acc 0.84375\n",
      "2017-04-03T22:13:32.237775: step 1199, loss 0.420787, acc 0.9375\n",
      "2017-04-03T22:13:32.433447: step 1200, loss 0.436445, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:13:33.091166: step 1200, loss 1.34032, acc 0.549608\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1200\n",
      "\n",
      "2017-04-03T22:13:33.341895: step 1201, loss 0.588632, acc 0.8125\n",
      "2017-04-03T22:13:33.543723: step 1202, loss 0.409089, acc 0.890625\n",
      "2017-04-03T22:13:33.734924: step 1203, loss 0.763098, acc 0.78125\n",
      "2017-04-03T22:13:33.932803: step 1204, loss 0.455986, acc 0.875\n",
      "2017-04-03T22:13:34.124748: step 1205, loss 0.500821, acc 0.875\n",
      "2017-04-03T22:13:34.319706: step 1206, loss 0.425998, acc 0.859375\n",
      "2017-04-03T22:13:34.511599: step 1207, loss 0.452203, acc 0.875\n",
      "2017-04-03T22:13:34.705660: step 1208, loss 0.461912, acc 0.875\n",
      "2017-04-03T22:13:34.901271: step 1209, loss 0.657755, acc 0.78125\n",
      "2017-04-03T22:13:35.095908: step 1210, loss 0.58615, acc 0.8125\n",
      "2017-04-03T22:13:35.286087: step 1211, loss 0.524807, acc 0.875\n",
      "2017-04-03T22:13:35.482456: step 1212, loss 0.429886, acc 0.84375\n",
      "2017-04-03T22:13:35.672126: step 1213, loss 0.679932, acc 0.734375\n",
      "2017-04-03T22:13:35.866145: step 1214, loss 0.659985, acc 0.75\n",
      "2017-04-03T22:13:36.055636: step 1215, loss 0.760148, acc 0.703125\n",
      "2017-04-03T22:13:36.246274: step 1216, loss 0.53342, acc 0.890625\n",
      "2017-04-03T22:13:36.439515: step 1217, loss 0.430076, acc 0.90625\n",
      "2017-04-03T22:13:36.634259: step 1218, loss 0.542042, acc 0.84375\n",
      "2017-04-03T22:13:36.822786: step 1219, loss 0.576185, acc 0.828125\n",
      "2017-04-03T22:13:37.012412: step 1220, loss 0.792679, acc 0.703125\n",
      "2017-04-03T22:13:37.205149: step 1221, loss 0.647945, acc 0.765625\n",
      "2017-04-03T22:13:37.397982: step 1222, loss 0.546807, acc 0.796875\n",
      "2017-04-03T22:13:37.589877: step 1223, loss 0.609133, acc 0.78125\n",
      "2017-04-03T22:13:37.781340: step 1224, loss 0.631282, acc 0.796875\n",
      "2017-04-03T22:13:37.975194: step 1225, loss 0.573539, acc 0.8125\n",
      "2017-04-03T22:13:38.168747: step 1226, loss 0.574675, acc 0.78125\n",
      "2017-04-03T22:13:38.357410: step 1227, loss 0.547852, acc 0.84375\n",
      "2017-04-03T22:13:38.553427: step 1228, loss 0.471961, acc 0.859375\n",
      "2017-04-03T22:13:38.745704: step 1229, loss 0.761154, acc 0.75\n",
      "2017-04-03T22:13:38.938365: step 1230, loss 0.420775, acc 0.890625\n",
      "2017-04-03T22:13:39.130791: step 1231, loss 0.521707, acc 0.828125\n",
      "2017-04-03T22:13:39.321822: step 1232, loss 0.557162, acc 0.84375\n",
      "2017-04-03T22:13:39.512198: step 1233, loss 0.47451, acc 0.828125\n",
      "2017-04-03T22:13:39.700992: step 1234, loss 0.536542, acc 0.828125\n",
      "2017-04-03T22:13:39.896770: step 1235, loss 0.54503, acc 0.875\n",
      "2017-04-03T22:13:40.089833: step 1236, loss 0.535271, acc 0.828125\n",
      "2017-04-03T22:13:40.283599: step 1237, loss 0.684969, acc 0.796875\n",
      "2017-04-03T22:13:40.478629: step 1238, loss 0.573285, acc 0.78125\n",
      "2017-04-03T22:13:40.669491: step 1239, loss 0.571105, acc 0.75\n",
      "2017-04-03T22:13:40.863027: step 1240, loss 0.475633, acc 0.859375\n",
      "2017-04-03T22:13:41.052674: step 1241, loss 0.526688, acc 0.828125\n",
      "2017-04-03T22:13:41.245915: step 1242, loss 0.745089, acc 0.71875\n",
      "2017-04-03T22:13:41.444286: step 1243, loss 0.49411, acc 0.84375\n",
      "2017-04-03T22:13:41.637501: step 1244, loss 0.610246, acc 0.828125\n",
      "2017-04-03T22:13:41.829197: step 1245, loss 0.598947, acc 0.859375\n",
      "2017-04-03T22:13:42.022577: step 1246, loss 0.557301, acc 0.859375\n",
      "2017-04-03T22:13:42.217743: step 1247, loss 0.591306, acc 0.78125\n",
      "2017-04-03T22:13:42.410782: step 1248, loss 0.531347, acc 0.875\n",
      "2017-04-03T22:13:42.598834: step 1249, loss 0.630784, acc 0.796875\n",
      "2017-04-03T22:13:42.788905: step 1250, loss 0.502753, acc 0.859375\n",
      "2017-04-03T22:13:42.984578: step 1251, loss 0.630908, acc 0.734375\n",
      "2017-04-03T22:13:43.177243: step 1252, loss 0.477427, acc 0.828125\n",
      "2017-04-03T22:13:43.374878: step 1253, loss 0.573433, acc 0.828125\n",
      "2017-04-03T22:13:43.564170: step 1254, loss 0.751088, acc 0.71875\n",
      "2017-04-03T22:13:43.754648: step 1255, loss 0.570394, acc 0.765625\n",
      "2017-04-03T22:13:43.948120: step 1256, loss 0.60632, acc 0.8125\n",
      "2017-04-03T22:13:44.145837: step 1257, loss 0.633522, acc 0.8125\n",
      "2017-04-03T22:13:44.339180: step 1258, loss 0.407387, acc 0.875\n",
      "2017-04-03T22:13:44.534086: step 1259, loss 0.522512, acc 0.84375\n",
      "2017-04-03T22:13:44.725358: step 1260, loss 0.57122, acc 0.78125\n",
      "2017-04-03T22:13:44.918740: step 1261, loss 0.765799, acc 0.671875\n",
      "2017-04-03T22:13:45.111610: step 1262, loss 0.423967, acc 0.90625\n",
      "2017-04-03T22:13:45.302105: step 1263, loss 0.485192, acc 0.875\n",
      "2017-04-03T22:13:45.496611: step 1264, loss 0.601358, acc 0.828125\n",
      "2017-04-03T22:13:45.687050: step 1265, loss 0.599165, acc 0.78125\n",
      "2017-04-03T22:13:45.881460: step 1266, loss 0.57341, acc 0.796875\n",
      "2017-04-03T22:13:46.076628: step 1267, loss 0.697303, acc 0.75\n",
      "2017-04-03T22:13:46.267684: step 1268, loss 0.568572, acc 0.828125\n",
      "2017-04-03T22:13:46.461367: step 1269, loss 0.5168, acc 0.875\n",
      "2017-04-03T22:13:46.660606: step 1270, loss 0.574154, acc 0.78125\n",
      "2017-04-03T22:13:46.853217: step 1271, loss 0.504315, acc 0.859375\n",
      "2017-04-03T22:13:47.044666: step 1272, loss 0.595793, acc 0.765625\n",
      "2017-04-03T22:13:47.241567: step 1273, loss 0.674123, acc 0.8125\n",
      "2017-04-03T22:13:47.429708: step 1274, loss 0.697587, acc 0.828125\n",
      "2017-04-03T22:13:47.619196: step 1275, loss 0.545519, acc 0.84375\n",
      "2017-04-03T22:13:47.810468: step 1276, loss 0.845883, acc 0.71875\n",
      "2017-04-03T22:13:48.001617: step 1277, loss 0.759125, acc 0.734375\n",
      "2017-04-03T22:13:48.194651: step 1278, loss 0.362642, acc 0.921875\n",
      "2017-04-03T22:13:48.387549: step 1279, loss 0.695199, acc 0.78125\n",
      "2017-04-03T22:13:48.583028: step 1280, loss 0.497828, acc 0.828125\n",
      "2017-04-03T22:13:48.778429: step 1281, loss 0.647989, acc 0.8125\n",
      "2017-04-03T22:13:48.973809: step 1282, loss 0.692571, acc 0.765625\n",
      "2017-04-03T22:13:49.167602: step 1283, loss 0.781744, acc 0.734375\n",
      "2017-04-03T22:13:49.357060: step 1284, loss 0.753659, acc 0.8125\n",
      "2017-04-03T22:13:49.548186: step 1285, loss 0.507348, acc 0.78125\n",
      "2017-04-03T22:13:49.740384: step 1286, loss 0.465222, acc 0.90625\n",
      "2017-04-03T22:13:49.934808: step 1287, loss 0.812685, acc 0.703125\n",
      "2017-04-03T22:13:50.126887: step 1288, loss 0.4287, acc 0.90625\n",
      "2017-04-03T22:13:50.320015: step 1289, loss 0.747179, acc 0.734375\n",
      "2017-04-03T22:13:50.514920: step 1290, loss 0.604175, acc 0.875\n",
      "2017-04-03T22:13:50.708482: step 1291, loss 0.709502, acc 0.75\n",
      "2017-04-03T22:13:50.902319: step 1292, loss 0.631824, acc 0.78125\n",
      "2017-04-03T22:13:51.091683: step 1293, loss 0.488815, acc 0.859375\n",
      "2017-04-03T22:13:51.285889: step 1294, loss 0.632535, acc 0.84375\n",
      "2017-04-03T22:13:51.481953: step 1295, loss 0.74649, acc 0.671875\n",
      "2017-04-03T22:13:51.642845: step 1296, loss 0.653869, acc 0.769231\n",
      "2017-04-03T22:13:51.835210: step 1297, loss 0.530043, acc 0.875\n",
      "2017-04-03T22:13:52.025316: step 1298, loss 0.625142, acc 0.796875\n",
      "2017-04-03T22:13:52.214265: step 1299, loss 0.389092, acc 0.9375\n",
      "2017-04-03T22:13:52.416490: step 1300, loss 0.372988, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:13:53.064895: step 1300, loss 1.33336, acc 0.544386\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1300\n",
      "\n",
      "2017-04-03T22:13:53.321213: step 1301, loss 0.494048, acc 0.859375\n",
      "2017-04-03T22:13:53.512919: step 1302, loss 0.328182, acc 0.921875\n",
      "2017-04-03T22:13:53.702561: step 1303, loss 0.615135, acc 0.71875\n",
      "2017-04-03T22:13:53.898648: step 1304, loss 0.503429, acc 0.859375\n",
      "2017-04-03T22:13:54.089418: step 1305, loss 0.596787, acc 0.84375\n",
      "2017-04-03T22:13:54.285543: step 1306, loss 0.454468, acc 0.875\n",
      "2017-04-03T22:13:54.479366: step 1307, loss 0.491106, acc 0.859375\n",
      "2017-04-03T22:13:54.668811: step 1308, loss 0.386189, acc 0.875\n",
      "2017-04-03T22:13:54.858711: step 1309, loss 0.403491, acc 0.890625\n",
      "2017-04-03T22:13:55.053356: step 1310, loss 0.443565, acc 0.875\n",
      "2017-04-03T22:13:55.246322: step 1311, loss 0.476678, acc 0.875\n",
      "2017-04-03T22:13:55.437498: step 1312, loss 0.5442, acc 0.875\n",
      "2017-04-03T22:13:55.628553: step 1313, loss 0.417837, acc 0.875\n",
      "2017-04-03T22:13:55.818879: step 1314, loss 0.567615, acc 0.8125\n",
      "2017-04-03T22:13:56.009858: step 1315, loss 0.483017, acc 0.875\n",
      "2017-04-03T22:13:56.203303: step 1316, loss 0.420342, acc 0.875\n",
      "2017-04-03T22:13:56.395767: step 1317, loss 0.618474, acc 0.796875\n",
      "2017-04-03T22:13:56.592414: step 1318, loss 0.475731, acc 0.875\n",
      "2017-04-03T22:13:56.785248: step 1319, loss 0.635789, acc 0.859375\n",
      "2017-04-03T22:13:56.973961: step 1320, loss 0.422856, acc 0.859375\n",
      "2017-04-03T22:13:57.169143: step 1321, loss 0.465602, acc 0.90625\n",
      "2017-04-03T22:13:57.358175: step 1322, loss 0.405102, acc 0.90625\n",
      "2017-04-03T22:13:57.551033: step 1323, loss 0.366648, acc 0.890625\n",
      "2017-04-03T22:13:57.739641: step 1324, loss 0.463305, acc 0.859375\n",
      "2017-04-03T22:13:57.929414: step 1325, loss 0.447588, acc 0.921875\n",
      "2017-04-03T22:13:58.118626: step 1326, loss 0.498174, acc 0.859375\n",
      "2017-04-03T22:13:58.315220: step 1327, loss 0.414016, acc 0.90625\n",
      "2017-04-03T22:13:58.505870: step 1328, loss 0.499785, acc 0.859375\n",
      "2017-04-03T22:13:58.699785: step 1329, loss 0.514791, acc 0.875\n",
      "2017-04-03T22:13:58.890804: step 1330, loss 0.642335, acc 0.84375\n",
      "2017-04-03T22:13:59.080892: step 1331, loss 0.71542, acc 0.78125\n",
      "2017-04-03T22:13:59.272447: step 1332, loss 0.682961, acc 0.78125\n",
      "2017-04-03T22:13:59.461180: step 1333, loss 0.561583, acc 0.859375\n",
      "2017-04-03T22:13:59.655686: step 1334, loss 0.532001, acc 0.875\n",
      "2017-04-03T22:13:59.848082: step 1335, loss 0.512797, acc 0.84375\n",
      "2017-04-03T22:14:00.040031: step 1336, loss 0.426658, acc 0.890625\n",
      "2017-04-03T22:14:00.235177: step 1337, loss 0.477359, acc 0.859375\n",
      "2017-04-03T22:14:00.424792: step 1338, loss 0.694297, acc 0.765625\n",
      "2017-04-03T22:14:00.616512: step 1339, loss 0.630619, acc 0.828125\n",
      "2017-04-03T22:14:00.814227: step 1340, loss 0.612002, acc 0.8125\n",
      "2017-04-03T22:14:01.005250: step 1341, loss 0.605716, acc 0.796875\n",
      "2017-04-03T22:14:01.201218: step 1342, loss 0.569508, acc 0.84375\n",
      "2017-04-03T22:14:01.390805: step 1343, loss 0.597984, acc 0.84375\n",
      "2017-04-03T22:14:01.579933: step 1344, loss 0.577479, acc 0.796875\n",
      "2017-04-03T22:14:01.775789: step 1345, loss 0.429031, acc 0.859375\n",
      "2017-04-03T22:14:01.967639: step 1346, loss 0.558778, acc 0.765625\n",
      "2017-04-03T22:14:02.159794: step 1347, loss 0.522416, acc 0.796875\n",
      "2017-04-03T22:14:02.349390: step 1348, loss 0.408548, acc 0.921875\n",
      "2017-04-03T22:14:02.543477: step 1349, loss 0.401687, acc 0.859375\n",
      "2017-04-03T22:14:02.739700: step 1350, loss 0.448939, acc 0.890625\n",
      "2017-04-03T22:14:02.931464: step 1351, loss 0.385916, acc 0.90625\n",
      "2017-04-03T22:14:03.125002: step 1352, loss 0.411448, acc 0.859375\n",
      "2017-04-03T22:14:03.316368: step 1353, loss 0.669447, acc 0.78125\n",
      "2017-04-03T22:14:03.509414: step 1354, loss 0.50662, acc 0.859375\n",
      "2017-04-03T22:14:03.706182: step 1355, loss 0.533792, acc 0.84375\n",
      "2017-04-03T22:14:03.900087: step 1356, loss 0.335596, acc 0.90625\n",
      "2017-04-03T22:14:04.091441: step 1357, loss 0.485132, acc 0.828125\n",
      "2017-04-03T22:14:04.283790: step 1358, loss 0.436223, acc 0.875\n",
      "2017-04-03T22:14:04.476413: step 1359, loss 0.645954, acc 0.765625\n",
      "2017-04-03T22:14:04.672137: step 1360, loss 0.466247, acc 0.875\n",
      "2017-04-03T22:14:04.864133: step 1361, loss 0.599826, acc 0.84375\n",
      "2017-04-03T22:14:05.057520: step 1362, loss 0.575531, acc 0.78125\n",
      "2017-04-03T22:14:05.251631: step 1363, loss 0.502732, acc 0.859375\n",
      "2017-04-03T22:14:05.444679: step 1364, loss 0.640461, acc 0.796875\n",
      "2017-04-03T22:14:05.637276: step 1365, loss 0.538613, acc 0.765625\n",
      "2017-04-03T22:14:05.829030: step 1366, loss 0.509484, acc 0.828125\n",
      "2017-04-03T22:14:06.019716: step 1367, loss 0.494739, acc 0.828125\n",
      "2017-04-03T22:14:06.212167: step 1368, loss 0.508459, acc 0.828125\n",
      "2017-04-03T22:14:06.406311: step 1369, loss 0.55541, acc 0.828125\n",
      "2017-04-03T22:14:06.599894: step 1370, loss 0.483468, acc 0.859375\n",
      "2017-04-03T22:14:06.788603: step 1371, loss 0.481301, acc 0.890625\n",
      "2017-04-03T22:14:06.983562: step 1372, loss 0.389219, acc 0.875\n",
      "2017-04-03T22:14:07.175829: step 1373, loss 0.566336, acc 0.828125\n",
      "2017-04-03T22:14:07.367007: step 1374, loss 0.504796, acc 0.84375\n",
      "2017-04-03T22:14:07.560816: step 1375, loss 0.424051, acc 0.875\n",
      "2017-04-03T22:14:07.753082: step 1376, loss 0.370613, acc 0.9375\n",
      "2017-04-03T22:14:07.946640: step 1377, loss 0.435324, acc 0.84375\n",
      "2017-04-03T22:14:08.139186: step 1378, loss 0.488635, acc 0.890625\n",
      "2017-04-03T22:14:08.329908: step 1379, loss 0.383857, acc 0.875\n",
      "2017-04-03T22:14:08.521812: step 1380, loss 0.569776, acc 0.828125\n",
      "2017-04-03T22:14:08.711070: step 1381, loss 0.347033, acc 0.921875\n",
      "2017-04-03T22:14:08.904240: step 1382, loss 0.481221, acc 0.875\n",
      "2017-04-03T22:14:09.096112: step 1383, loss 0.365966, acc 0.875\n",
      "2017-04-03T22:14:09.284415: step 1384, loss 0.532307, acc 0.8125\n",
      "2017-04-03T22:14:09.472913: step 1385, loss 0.498886, acc 0.828125\n",
      "2017-04-03T22:14:09.665243: step 1386, loss 0.462523, acc 0.84375\n",
      "2017-04-03T22:14:09.861054: step 1387, loss 0.470658, acc 0.875\n",
      "2017-04-03T22:14:10.058656: step 1388, loss 0.521237, acc 0.875\n",
      "2017-04-03T22:14:10.252487: step 1389, loss 0.54556, acc 0.828125\n",
      "2017-04-03T22:14:10.441254: step 1390, loss 0.477449, acc 0.875\n",
      "2017-04-03T22:14:10.631556: step 1391, loss 0.538382, acc 0.8125\n",
      "2017-04-03T22:14:10.819833: step 1392, loss 0.512893, acc 0.875\n",
      "2017-04-03T22:14:11.013888: step 1393, loss 0.510518, acc 0.8125\n",
      "2017-04-03T22:14:11.206287: step 1394, loss 0.553002, acc 0.828125\n",
      "2017-04-03T22:14:11.397006: step 1395, loss 0.653661, acc 0.78125\n",
      "2017-04-03T22:14:11.592121: step 1396, loss 0.647951, acc 0.75\n",
      "2017-04-03T22:14:11.784921: step 1397, loss 0.510457, acc 0.8125\n",
      "2017-04-03T22:14:11.976333: step 1398, loss 0.60389, acc 0.796875\n",
      "2017-04-03T22:14:12.172579: step 1399, loss 0.595456, acc 0.828125\n",
      "2017-04-03T22:14:12.366463: step 1400, loss 0.605774, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:14:13.017908: step 1400, loss 1.34136, acc 0.546997\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1400\n",
      "\n",
      "2017-04-03T22:14:13.270865: step 1401, loss 0.651541, acc 0.765625\n",
      "2017-04-03T22:14:13.463932: step 1402, loss 0.665668, acc 0.765625\n",
      "2017-04-03T22:14:13.657449: step 1403, loss 0.577094, acc 0.78125\n",
      "2017-04-03T22:14:13.818193: step 1404, loss 0.368351, acc 0.923077\n",
      "2017-04-03T22:14:14.011211: step 1405, loss 0.496412, acc 0.84375\n",
      "2017-04-03T22:14:14.201704: step 1406, loss 0.255004, acc 0.953125\n",
      "2017-04-03T22:14:14.394480: step 1407, loss 0.493668, acc 0.890625\n",
      "2017-04-03T22:14:14.592917: step 1408, loss 0.423486, acc 0.90625\n",
      "2017-04-03T22:14:14.782074: step 1409, loss 0.429611, acc 0.890625\n",
      "2017-04-03T22:14:14.972621: step 1410, loss 0.422849, acc 0.921875\n",
      "2017-04-03T22:14:15.163065: step 1411, loss 0.416997, acc 0.90625\n",
      "2017-04-03T22:14:15.356553: step 1412, loss 0.294995, acc 0.921875\n",
      "2017-04-03T22:14:15.551200: step 1413, loss 0.440394, acc 0.84375\n",
      "2017-04-03T22:14:15.747256: step 1414, loss 0.4455, acc 0.921875\n",
      "2017-04-03T22:14:15.944302: step 1415, loss 0.489655, acc 0.90625\n",
      "2017-04-03T22:14:16.134932: step 1416, loss 0.524603, acc 0.828125\n",
      "2017-04-03T22:14:16.332972: step 1417, loss 0.495164, acc 0.84375\n",
      "2017-04-03T22:14:16.523719: step 1418, loss 0.469497, acc 0.859375\n",
      "2017-04-03T22:14:16.717516: step 1419, loss 0.307391, acc 0.921875\n",
      "2017-04-03T22:14:16.907679: step 1420, loss 0.577985, acc 0.796875\n",
      "2017-04-03T22:14:17.098024: step 1421, loss 0.585782, acc 0.796875\n",
      "2017-04-03T22:14:17.291212: step 1422, loss 0.292242, acc 0.921875\n",
      "2017-04-03T22:14:17.486562: step 1423, loss 0.368557, acc 0.9375\n",
      "2017-04-03T22:14:17.679901: step 1424, loss 0.42517, acc 0.875\n",
      "2017-04-03T22:14:17.868183: step 1425, loss 0.396483, acc 0.875\n",
      "2017-04-03T22:14:18.059616: step 1426, loss 0.428939, acc 0.890625\n",
      "2017-04-03T22:14:18.252202: step 1427, loss 0.457608, acc 0.84375\n",
      "2017-04-03T22:14:18.443106: step 1428, loss 0.480885, acc 0.84375\n",
      "2017-04-03T22:14:18.633251: step 1429, loss 0.428186, acc 0.90625\n",
      "2017-04-03T22:14:18.826474: step 1430, loss 0.577664, acc 0.84375\n",
      "2017-04-03T22:14:19.017528: step 1431, loss 0.344294, acc 0.90625\n",
      "2017-04-03T22:14:19.209059: step 1432, loss 0.301682, acc 0.90625\n",
      "2017-04-03T22:14:19.404496: step 1433, loss 0.380653, acc 0.890625\n",
      "2017-04-03T22:14:19.596410: step 1434, loss 0.510311, acc 0.875\n",
      "2017-04-03T22:14:19.791569: step 1435, loss 0.495815, acc 0.875\n",
      "2017-04-03T22:14:19.985503: step 1436, loss 0.384401, acc 0.921875\n",
      "2017-04-03T22:14:20.177975: step 1437, loss 0.259399, acc 0.953125\n",
      "2017-04-03T22:14:20.370662: step 1438, loss 0.469707, acc 0.875\n",
      "2017-04-03T22:14:20.564074: step 1439, loss 0.428155, acc 0.890625\n",
      "2017-04-03T22:14:20.760539: step 1440, loss 0.369627, acc 0.875\n",
      "2017-04-03T22:14:20.950123: step 1441, loss 0.387219, acc 0.890625\n",
      "2017-04-03T22:14:21.138891: step 1442, loss 0.465735, acc 0.890625\n",
      "2017-04-03T22:14:21.331976: step 1443, loss 0.424747, acc 0.921875\n",
      "2017-04-03T22:14:21.524616: step 1444, loss 0.459762, acc 0.890625\n",
      "2017-04-03T22:14:21.721529: step 1445, loss 0.360483, acc 0.890625\n",
      "2017-04-03T22:14:21.912657: step 1446, loss 0.383417, acc 0.875\n",
      "2017-04-03T22:14:22.103955: step 1447, loss 0.415927, acc 0.875\n",
      "2017-04-03T22:14:22.297608: step 1448, loss 0.428134, acc 0.84375\n",
      "2017-04-03T22:14:22.490194: step 1449, loss 0.47384, acc 0.828125\n",
      "2017-04-03T22:14:22.682962: step 1450, loss 0.452067, acc 0.890625\n",
      "2017-04-03T22:14:22.875722: step 1451, loss 0.549584, acc 0.84375\n",
      "2017-04-03T22:14:23.069709: step 1452, loss 0.397784, acc 0.890625\n",
      "2017-04-03T22:14:23.264215: step 1453, loss 0.295485, acc 0.9375\n",
      "2017-04-03T22:14:23.457948: step 1454, loss 0.326502, acc 0.9375\n",
      "2017-04-03T22:14:23.652305: step 1455, loss 0.434677, acc 0.890625\n",
      "2017-04-03T22:14:23.845560: step 1456, loss 0.441811, acc 0.890625\n",
      "2017-04-03T22:14:24.043162: step 1457, loss 0.526664, acc 0.8125\n",
      "2017-04-03T22:14:24.239915: step 1458, loss 0.450269, acc 0.890625\n",
      "2017-04-03T22:14:24.435884: step 1459, loss 0.478848, acc 0.890625\n",
      "2017-04-03T22:14:24.628043: step 1460, loss 0.480048, acc 0.8125\n",
      "2017-04-03T22:14:24.824146: step 1461, loss 0.528081, acc 0.875\n",
      "2017-04-03T22:14:25.021501: step 1462, loss 0.520137, acc 0.84375\n",
      "2017-04-03T22:14:25.211450: step 1463, loss 0.618889, acc 0.796875\n",
      "2017-04-03T22:14:25.404326: step 1464, loss 0.488013, acc 0.796875\n",
      "2017-04-03T22:14:25.599828: step 1465, loss 0.521433, acc 0.828125\n",
      "2017-04-03T22:14:25.794079: step 1466, loss 0.420712, acc 0.875\n",
      "2017-04-03T22:14:25.984628: step 1467, loss 0.578451, acc 0.828125\n",
      "2017-04-03T22:14:26.177520: step 1468, loss 0.468937, acc 0.828125\n",
      "2017-04-03T22:14:26.368876: step 1469, loss 0.447365, acc 0.859375\n",
      "2017-04-03T22:14:26.560150: step 1470, loss 0.511667, acc 0.875\n",
      "2017-04-03T22:14:26.755791: step 1471, loss 0.620518, acc 0.796875\n",
      "2017-04-03T22:14:26.948445: step 1472, loss 0.424393, acc 0.890625\n",
      "2017-04-03T22:14:27.138875: step 1473, loss 0.356572, acc 0.890625\n",
      "2017-04-03T22:14:27.331338: step 1474, loss 0.462946, acc 0.890625\n",
      "2017-04-03T22:14:27.524779: step 1475, loss 0.38418, acc 0.921875\n",
      "2017-04-03T22:14:27.714000: step 1476, loss 0.454706, acc 0.890625\n",
      "2017-04-03T22:14:27.908823: step 1477, loss 0.604558, acc 0.796875\n",
      "2017-04-03T22:14:28.101082: step 1478, loss 0.446959, acc 0.859375\n",
      "2017-04-03T22:14:28.294484: step 1479, loss 0.507627, acc 0.84375\n",
      "2017-04-03T22:14:28.488140: step 1480, loss 0.476662, acc 0.875\n",
      "2017-04-03T22:14:28.683219: step 1481, loss 0.494088, acc 0.828125\n",
      "2017-04-03T22:14:28.871696: step 1482, loss 0.456845, acc 0.84375\n",
      "2017-04-03T22:14:29.060776: step 1483, loss 0.661934, acc 0.78125\n",
      "2017-04-03T22:14:29.252977: step 1484, loss 0.389237, acc 0.875\n",
      "2017-04-03T22:14:29.446518: step 1485, loss 0.526446, acc 0.8125\n",
      "2017-04-03T22:14:29.639895: step 1486, loss 0.44934, acc 0.890625\n",
      "2017-04-03T22:14:29.836328: step 1487, loss 0.484289, acc 0.796875\n",
      "2017-04-03T22:14:30.029301: step 1488, loss 0.407409, acc 0.859375\n",
      "2017-04-03T22:14:30.224095: step 1489, loss 0.520829, acc 0.828125\n",
      "2017-04-03T22:14:30.415262: step 1490, loss 0.44077, acc 0.8125\n",
      "2017-04-03T22:14:30.609400: step 1491, loss 0.59761, acc 0.796875\n",
      "2017-04-03T22:14:30.800387: step 1492, loss 0.449495, acc 0.859375\n",
      "2017-04-03T22:14:30.992770: step 1493, loss 0.467277, acc 0.859375\n",
      "2017-04-03T22:14:31.182157: step 1494, loss 0.420295, acc 0.828125\n",
      "2017-04-03T22:14:31.371708: step 1495, loss 0.545846, acc 0.828125\n",
      "2017-04-03T22:14:31.560288: step 1496, loss 0.428141, acc 0.859375\n",
      "2017-04-03T22:14:31.750768: step 1497, loss 0.720013, acc 0.765625\n",
      "2017-04-03T22:14:31.939847: step 1498, loss 0.384212, acc 0.859375\n",
      "2017-04-03T22:14:32.131604: step 1499, loss 0.454042, acc 0.859375\n",
      "2017-04-03T22:14:32.322954: step 1500, loss 0.454556, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:14:32.998955: step 1500, loss 1.34365, acc 0.543081\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1500\n",
      "\n",
      "2017-04-03T22:14:33.295041: step 1501, loss 0.427936, acc 0.890625\n",
      "2017-04-03T22:14:33.488653: step 1502, loss 0.456234, acc 0.859375\n",
      "2017-04-03T22:14:33.676394: step 1503, loss 0.414578, acc 0.890625\n",
      "2017-04-03T22:14:33.869675: step 1504, loss 0.535644, acc 0.8125\n",
      "2017-04-03T22:14:34.058497: step 1505, loss 0.558968, acc 0.828125\n",
      "2017-04-03T22:14:34.254129: step 1506, loss 0.374438, acc 0.90625\n",
      "2017-04-03T22:14:34.445510: step 1507, loss 0.503798, acc 0.84375\n",
      "2017-04-03T22:14:34.637190: step 1508, loss 0.300709, acc 0.953125\n",
      "2017-04-03T22:14:34.828314: step 1509, loss 0.473938, acc 0.859375\n",
      "2017-04-03T22:14:35.017733: step 1510, loss 0.444244, acc 0.875\n",
      "2017-04-03T22:14:35.210177: step 1511, loss 0.587788, acc 0.875\n",
      "2017-04-03T22:14:35.369570: step 1512, loss 0.422366, acc 0.923077\n",
      "2017-04-03T22:14:35.563910: step 1513, loss 0.368021, acc 0.875\n",
      "2017-04-03T22:14:35.753160: step 1514, loss 0.373035, acc 0.859375\n",
      "2017-04-03T22:14:35.946416: step 1515, loss 0.442765, acc 0.890625\n",
      "2017-04-03T22:14:36.138503: step 1516, loss 0.547059, acc 0.828125\n",
      "2017-04-03T22:14:36.329252: step 1517, loss 0.362459, acc 0.90625\n",
      "2017-04-03T22:14:36.520307: step 1518, loss 0.326544, acc 0.890625\n",
      "2017-04-03T22:14:36.714740: step 1519, loss 0.327349, acc 0.921875\n",
      "2017-04-03T22:14:36.911068: step 1520, loss 0.614461, acc 0.796875\n",
      "2017-04-03T22:14:37.102123: step 1521, loss 0.483516, acc 0.796875\n",
      "2017-04-03T22:14:37.289391: step 1522, loss 0.32578, acc 0.953125\n",
      "2017-04-03T22:14:37.479464: step 1523, loss 0.31459, acc 0.921875\n",
      "2017-04-03T22:14:37.671098: step 1524, loss 0.406442, acc 0.875\n",
      "2017-04-03T22:14:37.862781: step 1525, loss 0.265298, acc 0.9375\n",
      "2017-04-03T22:14:38.058663: step 1526, loss 0.340987, acc 0.875\n",
      "2017-04-03T22:14:38.253011: step 1527, loss 0.322752, acc 0.953125\n",
      "2017-04-03T22:14:38.445867: step 1528, loss 0.319925, acc 0.921875\n",
      "2017-04-03T22:14:38.637251: step 1529, loss 0.168019, acc 0.984375\n",
      "2017-04-03T22:14:38.830728: step 1530, loss 0.385949, acc 0.84375\n",
      "2017-04-03T22:14:39.026199: step 1531, loss 0.299401, acc 0.875\n",
      "2017-04-03T22:14:39.218830: step 1532, loss 0.318371, acc 0.921875\n",
      "2017-04-03T22:14:39.413910: step 1533, loss 0.47981, acc 0.875\n",
      "2017-04-03T22:14:39.608084: step 1534, loss 0.30459, acc 0.890625\n",
      "2017-04-03T22:14:39.799689: step 1535, loss 0.386416, acc 0.859375\n",
      "2017-04-03T22:14:39.991444: step 1536, loss 0.370478, acc 0.90625\n",
      "2017-04-03T22:14:40.189214: step 1537, loss 0.406632, acc 0.890625\n",
      "2017-04-03T22:14:40.383789: step 1538, loss 0.31673, acc 0.890625\n",
      "2017-04-03T22:14:40.581362: step 1539, loss 0.363036, acc 0.890625\n",
      "2017-04-03T22:14:40.771626: step 1540, loss 0.52302, acc 0.796875\n",
      "2017-04-03T22:14:40.961802: step 1541, loss 0.247188, acc 0.953125\n",
      "2017-04-03T22:14:41.153000: step 1542, loss 0.386933, acc 0.90625\n",
      "2017-04-03T22:14:41.347738: step 1543, loss 0.425267, acc 0.859375\n",
      "2017-04-03T22:14:41.543447: step 1544, loss 0.361913, acc 0.921875\n",
      "2017-04-03T22:14:41.739124: step 1545, loss 0.424947, acc 0.875\n",
      "2017-04-03T22:14:41.932485: step 1546, loss 0.402113, acc 0.90625\n",
      "2017-04-03T22:14:42.125730: step 1547, loss 0.466416, acc 0.890625\n",
      "2017-04-03T22:14:42.317443: step 1548, loss 0.320015, acc 0.921875\n",
      "2017-04-03T22:14:42.515364: step 1549, loss 0.448683, acc 0.859375\n",
      "2017-04-03T22:14:42.710751: step 1550, loss 0.302657, acc 0.890625\n",
      "2017-04-03T22:14:42.901390: step 1551, loss 0.399418, acc 0.90625\n",
      "2017-04-03T22:14:43.094226: step 1552, loss 0.431656, acc 0.890625\n",
      "2017-04-03T22:14:43.288342: step 1553, loss 0.373827, acc 0.90625\n",
      "2017-04-03T22:14:43.482929: step 1554, loss 0.361039, acc 0.90625\n",
      "2017-04-03T22:14:43.676270: step 1555, loss 0.616959, acc 0.796875\n",
      "2017-04-03T22:14:43.868063: step 1556, loss 0.381713, acc 0.90625\n",
      "2017-04-03T22:14:44.058401: step 1557, loss 0.260908, acc 0.9375\n",
      "2017-04-03T22:14:44.251019: step 1558, loss 0.316222, acc 0.921875\n",
      "2017-04-03T22:14:44.444317: step 1559, loss 0.296712, acc 0.953125\n",
      "2017-04-03T22:14:44.636816: step 1560, loss 0.39641, acc 0.875\n",
      "2017-04-03T22:14:44.827013: step 1561, loss 0.373449, acc 0.921875\n",
      "2017-04-03T22:14:45.021025: step 1562, loss 0.280145, acc 0.921875\n",
      "2017-04-03T22:14:45.217302: step 1563, loss 0.328317, acc 0.921875\n",
      "2017-04-03T22:14:45.406420: step 1564, loss 0.365629, acc 0.9375\n",
      "2017-04-03T22:14:45.596770: step 1565, loss 0.411362, acc 0.84375\n",
      "2017-04-03T22:14:45.791806: step 1566, loss 0.42003, acc 0.859375\n",
      "2017-04-03T22:14:45.984534: step 1567, loss 0.27418, acc 0.9375\n",
      "2017-04-03T22:14:46.180721: step 1568, loss 0.374507, acc 0.890625\n",
      "2017-04-03T22:14:46.370575: step 1569, loss 0.414794, acc 0.890625\n",
      "2017-04-03T22:14:46.559153: step 1570, loss 0.377582, acc 0.890625\n",
      "2017-04-03T22:14:46.751674: step 1571, loss 0.380585, acc 0.875\n",
      "2017-04-03T22:14:46.946834: step 1572, loss 0.341508, acc 0.875\n",
      "2017-04-03T22:14:47.138477: step 1573, loss 0.441826, acc 0.859375\n",
      "2017-04-03T22:14:47.327479: step 1574, loss 0.298465, acc 0.9375\n",
      "2017-04-03T22:14:47.518490: step 1575, loss 0.438975, acc 0.828125\n",
      "2017-04-03T22:14:47.708933: step 1576, loss 0.549096, acc 0.859375\n",
      "2017-04-03T22:14:47.902084: step 1577, loss 0.417299, acc 0.859375\n",
      "2017-04-03T22:14:48.104768: step 1578, loss 0.430874, acc 0.890625\n",
      "2017-04-03T22:14:48.298834: step 1579, loss 0.239238, acc 0.96875\n",
      "2017-04-03T22:14:48.496164: step 1580, loss 0.404071, acc 0.890625\n",
      "2017-04-03T22:14:48.683962: step 1581, loss 0.40151, acc 0.921875\n",
      "2017-04-03T22:14:48.877925: step 1582, loss 0.377911, acc 0.875\n",
      "2017-04-03T22:14:49.071239: step 1583, loss 0.519347, acc 0.859375\n",
      "2017-04-03T22:14:49.261010: step 1584, loss 0.340661, acc 0.9375\n",
      "2017-04-03T22:14:49.456105: step 1585, loss 0.268687, acc 0.921875\n",
      "2017-04-03T22:14:49.648302: step 1586, loss 0.376521, acc 0.859375\n",
      "2017-04-03T22:14:49.838512: step 1587, loss 0.429726, acc 0.859375\n",
      "2017-04-03T22:14:50.031805: step 1588, loss 0.36224, acc 0.875\n",
      "2017-04-03T22:14:50.225188: step 1589, loss 0.322329, acc 0.9375\n",
      "2017-04-03T22:14:50.417126: step 1590, loss 0.456511, acc 0.859375\n",
      "2017-04-03T22:14:50.608337: step 1591, loss 0.494237, acc 0.859375\n",
      "2017-04-03T22:14:50.804506: step 1592, loss 0.311531, acc 0.9375\n",
      "2017-04-03T22:14:50.997754: step 1593, loss 0.560359, acc 0.859375\n",
      "2017-04-03T22:14:51.191941: step 1594, loss 0.482835, acc 0.828125\n",
      "2017-04-03T22:14:51.382724: step 1595, loss 0.526605, acc 0.78125\n",
      "2017-04-03T22:14:51.577300: step 1596, loss 0.274984, acc 0.921875\n",
      "2017-04-03T22:14:51.766354: step 1597, loss 0.448451, acc 0.890625\n",
      "2017-04-03T22:14:51.962558: step 1598, loss 0.332607, acc 0.921875\n",
      "2017-04-03T22:14:52.157246: step 1599, loss 0.391237, acc 0.890625\n",
      "2017-04-03T22:14:52.350020: step 1600, loss 0.273298, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:14:53.015774: step 1600, loss 1.36473, acc 0.546997\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1600\n",
      "\n",
      "2017-04-03T22:14:53.274889: step 1601, loss 0.384999, acc 0.90625\n",
      "2017-04-03T22:14:53.469303: step 1602, loss 0.322847, acc 0.9375\n",
      "2017-04-03T22:14:53.679277: step 1603, loss 0.423033, acc 0.890625\n",
      "2017-04-03T22:14:53.876668: step 1604, loss 0.487538, acc 0.84375\n",
      "2017-04-03T22:14:54.073563: step 1605, loss 0.43445, acc 0.859375\n",
      "2017-04-03T22:14:54.264089: step 1606, loss 0.500484, acc 0.828125\n",
      "2017-04-03T22:14:54.458971: step 1607, loss 0.414397, acc 0.90625\n",
      "2017-04-03T22:14:54.653309: step 1608, loss 0.533065, acc 0.859375\n",
      "2017-04-03T22:14:54.847181: step 1609, loss 0.432909, acc 0.890625\n",
      "2017-04-03T22:14:55.038872: step 1610, loss 0.388997, acc 0.859375\n",
      "2017-04-03T22:14:55.228926: step 1611, loss 0.316657, acc 0.921875\n",
      "2017-04-03T22:14:55.419527: step 1612, loss 0.300359, acc 0.9375\n",
      "2017-04-03T22:14:55.611459: step 1613, loss 0.356672, acc 0.875\n",
      "2017-04-03T22:14:55.800805: step 1614, loss 0.312603, acc 0.921875\n",
      "2017-04-03T22:14:55.993661: step 1615, loss 0.446499, acc 0.84375\n",
      "2017-04-03T22:14:56.186512: step 1616, loss 0.410168, acc 0.859375\n",
      "2017-04-03T22:14:56.381963: step 1617, loss 0.262016, acc 0.9375\n",
      "2017-04-03T22:14:56.573678: step 1618, loss 0.421489, acc 0.890625\n",
      "2017-04-03T22:14:56.764411: step 1619, loss 0.48358, acc 0.875\n",
      "2017-04-03T22:14:56.927288: step 1620, loss 0.372598, acc 0.884615\n",
      "2017-04-03T22:14:57.120297: step 1621, loss 0.297129, acc 0.9375\n",
      "2017-04-03T22:14:57.311166: step 1622, loss 0.309677, acc 0.90625\n",
      "2017-04-03T22:14:57.503531: step 1623, loss 0.234255, acc 0.9375\n",
      "2017-04-03T22:14:57.709684: step 1624, loss 0.247649, acc 0.953125\n",
      "2017-04-03T22:14:57.899265: step 1625, loss 0.318213, acc 0.921875\n",
      "2017-04-03T22:14:58.095045: step 1626, loss 0.333099, acc 0.90625\n",
      "2017-04-03T22:14:58.285951: step 1627, loss 0.285127, acc 0.921875\n",
      "2017-04-03T22:14:58.475115: step 1628, loss 0.406948, acc 0.859375\n",
      "2017-04-03T22:14:58.664277: step 1629, loss 0.259298, acc 0.9375\n",
      "2017-04-03T22:14:58.855720: step 1630, loss 0.462579, acc 0.875\n",
      "2017-04-03T22:14:59.053948: step 1631, loss 0.329957, acc 0.90625\n",
      "2017-04-03T22:14:59.242715: step 1632, loss 0.304489, acc 0.90625\n",
      "2017-04-03T22:14:59.434863: step 1633, loss 0.309018, acc 0.890625\n",
      "2017-04-03T22:14:59.629295: step 1634, loss 0.32285, acc 0.921875\n",
      "2017-04-03T22:14:59.819967: step 1635, loss 0.349384, acc 0.921875\n",
      "2017-04-03T22:15:00.020043: step 1636, loss 0.26558, acc 0.96875\n",
      "2017-04-03T22:15:00.209110: step 1637, loss 0.160033, acc 0.984375\n",
      "2017-04-03T22:15:00.400739: step 1638, loss 0.424439, acc 0.84375\n",
      "2017-04-03T22:15:00.590146: step 1639, loss 0.273431, acc 0.90625\n",
      "2017-04-03T22:15:00.782241: step 1640, loss 0.300151, acc 0.9375\n",
      "2017-04-03T22:15:00.973095: step 1641, loss 0.397755, acc 0.890625\n",
      "2017-04-03T22:15:01.164224: step 1642, loss 0.314352, acc 0.921875\n",
      "2017-04-03T22:15:01.356381: step 1643, loss 0.416356, acc 0.859375\n",
      "2017-04-03T22:15:01.552806: step 1644, loss 0.279502, acc 0.9375\n",
      "2017-04-03T22:15:01.753016: step 1645, loss 0.366326, acc 0.90625\n",
      "2017-04-03T22:15:01.947999: step 1646, loss 0.330226, acc 0.921875\n",
      "2017-04-03T22:15:02.134967: step 1647, loss 0.336705, acc 0.859375\n",
      "2017-04-03T22:15:02.328307: step 1648, loss 0.319618, acc 0.921875\n",
      "2017-04-03T22:15:02.520721: step 1649, loss 0.44787, acc 0.875\n",
      "2017-04-03T22:15:02.717947: step 1650, loss 0.480117, acc 0.921875\n",
      "2017-04-03T22:15:02.913638: step 1651, loss 0.449728, acc 0.84375\n",
      "2017-04-03T22:15:03.106001: step 1652, loss 0.513067, acc 0.859375\n",
      "2017-04-03T22:15:03.300361: step 1653, loss 0.394439, acc 0.84375\n",
      "2017-04-03T22:15:03.495790: step 1654, loss 0.31705, acc 0.9375\n",
      "2017-04-03T22:15:03.687981: step 1655, loss 0.373439, acc 0.890625\n",
      "2017-04-03T22:15:03.883865: step 1656, loss 0.321273, acc 0.921875\n",
      "2017-04-03T22:15:04.076595: step 1657, loss 0.318466, acc 0.921875\n",
      "2017-04-03T22:15:04.268345: step 1658, loss 0.437342, acc 0.859375\n",
      "2017-04-03T22:15:04.458567: step 1659, loss 0.404465, acc 0.953125\n",
      "2017-04-03T22:15:04.648839: step 1660, loss 0.2854, acc 0.90625\n",
      "2017-04-03T22:15:04.840376: step 1661, loss 0.232152, acc 1\n",
      "2017-04-03T22:15:05.037060: step 1662, loss 0.481023, acc 0.875\n",
      "2017-04-03T22:15:05.225943: step 1663, loss 0.249621, acc 0.921875\n",
      "2017-04-03T22:15:05.422416: step 1664, loss 0.406214, acc 0.875\n",
      "2017-04-03T22:15:05.626447: step 1665, loss 0.275678, acc 0.921875\n",
      "2017-04-03T22:15:05.820925: step 1666, loss 0.284832, acc 0.90625\n",
      "2017-04-03T22:15:06.012557: step 1667, loss 0.303663, acc 0.953125\n",
      "2017-04-03T22:15:06.209233: step 1668, loss 0.248248, acc 0.9375\n",
      "2017-04-03T22:15:06.400365: step 1669, loss 0.221124, acc 0.953125\n",
      "2017-04-03T22:15:06.593438: step 1670, loss 0.428166, acc 0.90625\n",
      "2017-04-03T22:15:06.782559: step 1671, loss 0.250431, acc 0.90625\n",
      "2017-04-03T22:15:06.975686: step 1672, loss 0.389067, acc 0.921875\n",
      "2017-04-03T22:15:07.167534: step 1673, loss 0.304706, acc 0.921875\n",
      "2017-04-03T22:15:07.362163: step 1674, loss 0.244383, acc 0.9375\n",
      "2017-04-03T22:15:07.554949: step 1675, loss 0.397588, acc 0.890625\n",
      "2017-04-03T22:15:07.745126: step 1676, loss 0.392171, acc 0.890625\n",
      "2017-04-03T22:15:07.940043: step 1677, loss 0.3126, acc 0.921875\n",
      "2017-04-03T22:15:08.129258: step 1678, loss 0.310375, acc 0.953125\n",
      "2017-04-03T22:15:08.327495: step 1679, loss 0.303131, acc 0.90625\n",
      "2017-04-03T22:15:08.515689: step 1680, loss 0.241971, acc 0.96875\n",
      "2017-04-03T22:15:08.713212: step 1681, loss 0.376637, acc 0.921875\n",
      "2017-04-03T22:15:08.905431: step 1682, loss 0.259178, acc 0.953125\n",
      "2017-04-03T22:15:09.100369: step 1683, loss 0.264951, acc 0.921875\n",
      "2017-04-03T22:15:09.291975: step 1684, loss 0.219842, acc 0.953125\n",
      "2017-04-03T22:15:09.485312: step 1685, loss 0.427994, acc 0.875\n",
      "2017-04-03T22:15:09.672826: step 1686, loss 0.222046, acc 0.96875\n",
      "2017-04-03T22:15:09.868620: step 1687, loss 0.307741, acc 0.890625\n",
      "2017-04-03T22:15:10.059884: step 1688, loss 0.312545, acc 0.921875\n",
      "2017-04-03T22:15:10.249355: step 1689, loss 0.363804, acc 0.859375\n",
      "2017-04-03T22:15:10.442003: step 1690, loss 0.330607, acc 0.90625\n",
      "2017-04-03T22:15:10.634659: step 1691, loss 0.258153, acc 0.90625\n",
      "2017-04-03T22:15:10.825183: step 1692, loss 0.397764, acc 0.84375\n",
      "2017-04-03T22:15:11.016697: step 1693, loss 0.388473, acc 0.875\n",
      "2017-04-03T22:15:11.205047: step 1694, loss 0.424658, acc 0.890625\n",
      "2017-04-03T22:15:11.397191: step 1695, loss 0.314355, acc 0.90625\n",
      "2017-04-03T22:15:11.586779: step 1696, loss 0.397553, acc 0.84375\n",
      "2017-04-03T22:15:11.780784: step 1697, loss 0.501046, acc 0.8125\n",
      "2017-04-03T22:15:11.970859: step 1698, loss 0.424298, acc 0.875\n",
      "2017-04-03T22:15:12.165740: step 1699, loss 0.379816, acc 0.9375\n",
      "2017-04-03T22:15:12.358750: step 1700, loss 0.414898, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:15:13.019405: step 1700, loss 1.38324, acc 0.556136\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1700\n",
      "\n",
      "2017-04-03T22:15:13.276095: step 1701, loss 0.241786, acc 0.921875\n",
      "2017-04-03T22:15:13.469519: step 1702, loss 0.286981, acc 0.921875\n",
      "2017-04-03T22:15:13.665980: step 1703, loss 0.151426, acc 0.984375\n",
      "2017-04-03T22:15:13.861006: step 1704, loss 0.475864, acc 0.890625\n",
      "2017-04-03T22:15:14.051459: step 1705, loss 0.437252, acc 0.875\n",
      "2017-04-03T22:15:14.241297: step 1706, loss 0.44943, acc 0.78125\n",
      "2017-04-03T22:15:14.433132: step 1707, loss 0.395532, acc 0.875\n",
      "2017-04-03T22:15:14.621870: step 1708, loss 0.368229, acc 0.875\n",
      "2017-04-03T22:15:14.819399: step 1709, loss 0.277257, acc 0.90625\n",
      "2017-04-03T22:15:15.012278: step 1710, loss 0.433996, acc 0.875\n",
      "2017-04-03T22:15:15.207878: step 1711, loss 0.402598, acc 0.890625\n",
      "2017-04-03T22:15:15.399223: step 1712, loss 0.351655, acc 0.875\n",
      "2017-04-03T22:15:15.588770: step 1713, loss 0.402625, acc 0.84375\n",
      "2017-04-03T22:15:15.782968: step 1714, loss 0.345839, acc 0.953125\n",
      "2017-04-03T22:15:15.977541: step 1715, loss 0.371292, acc 0.90625\n",
      "2017-04-03T22:15:16.171795: step 1716, loss 0.36257, acc 0.875\n",
      "2017-04-03T22:15:16.364753: step 1717, loss 0.360354, acc 0.890625\n",
      "2017-04-03T22:15:16.558949: step 1718, loss 0.318279, acc 0.921875\n",
      "2017-04-03T22:15:16.751972: step 1719, loss 0.271033, acc 0.953125\n",
      "2017-04-03T22:15:16.942667: step 1720, loss 0.310169, acc 0.9375\n",
      "2017-04-03T22:15:17.134066: step 1721, loss 0.406218, acc 0.875\n",
      "2017-04-03T22:15:17.327586: step 1722, loss 0.486744, acc 0.8125\n",
      "2017-04-03T22:15:17.520937: step 1723, loss 0.336543, acc 0.90625\n",
      "2017-04-03T22:15:17.712333: step 1724, loss 0.308855, acc 0.90625\n",
      "2017-04-03T22:15:17.900993: step 1725, loss 0.335228, acc 0.890625\n",
      "2017-04-03T22:15:18.096394: step 1726, loss 0.38196, acc 0.90625\n",
      "2017-04-03T22:15:18.289418: step 1727, loss 0.445877, acc 0.84375\n",
      "2017-04-03T22:15:18.452008: step 1728, loss 0.371129, acc 0.865385\n",
      "2017-04-03T22:15:18.647797: step 1729, loss 0.197991, acc 0.96875\n",
      "2017-04-03T22:15:18.843360: step 1730, loss 0.32627, acc 0.921875\n",
      "2017-04-03T22:15:19.036537: step 1731, loss 0.358645, acc 0.921875\n",
      "2017-04-03T22:15:19.231771: step 1732, loss 0.194832, acc 1\n",
      "2017-04-03T22:15:19.424862: step 1733, loss 0.379642, acc 0.890625\n",
      "2017-04-03T22:15:19.616637: step 1734, loss 0.168583, acc 0.96875\n",
      "2017-04-03T22:15:19.809264: step 1735, loss 0.250208, acc 0.921875\n",
      "2017-04-03T22:15:20.000762: step 1736, loss 0.287123, acc 0.921875\n",
      "2017-04-03T22:15:20.189620: step 1737, loss 0.194472, acc 0.96875\n",
      "2017-04-03T22:15:20.379958: step 1738, loss 0.143079, acc 0.984375\n",
      "2017-04-03T22:15:20.575612: step 1739, loss 0.288417, acc 0.90625\n",
      "2017-04-03T22:15:20.765853: step 1740, loss 0.249156, acc 0.953125\n",
      "2017-04-03T22:15:20.960005: step 1741, loss 0.252396, acc 0.921875\n",
      "2017-04-03T22:15:21.148829: step 1742, loss 0.382823, acc 0.890625\n",
      "2017-04-03T22:15:21.344590: step 1743, loss 0.301378, acc 0.890625\n",
      "2017-04-03T22:15:21.538498: step 1744, loss 0.226456, acc 0.96875\n",
      "2017-04-03T22:15:21.730888: step 1745, loss 0.299866, acc 0.90625\n",
      "2017-04-03T22:15:21.921638: step 1746, loss 0.328202, acc 0.921875\n",
      "2017-04-03T22:15:22.112224: step 1747, loss 0.315316, acc 0.90625\n",
      "2017-04-03T22:15:22.301576: step 1748, loss 0.261337, acc 0.9375\n",
      "2017-04-03T22:15:22.493126: step 1749, loss 0.289471, acc 0.90625\n",
      "2017-04-03T22:15:22.685879: step 1750, loss 0.219522, acc 0.953125\n",
      "2017-04-03T22:15:22.878334: step 1751, loss 0.338157, acc 0.921875\n",
      "2017-04-03T22:15:23.070095: step 1752, loss 0.242172, acc 0.921875\n",
      "2017-04-03T22:15:23.262474: step 1753, loss 0.218105, acc 0.9375\n",
      "2017-04-03T22:15:23.452046: step 1754, loss 0.27238, acc 0.921875\n",
      "2017-04-03T22:15:23.644011: step 1755, loss 0.246777, acc 0.9375\n",
      "2017-04-03T22:15:23.833970: step 1756, loss 0.403187, acc 0.875\n",
      "2017-04-03T22:15:24.026383: step 1757, loss 0.292583, acc 0.9375\n",
      "2017-04-03T22:15:24.220851: step 1758, loss 0.186065, acc 0.96875\n",
      "2017-04-03T22:15:24.416946: step 1759, loss 0.332182, acc 0.890625\n",
      "2017-04-03T22:15:24.611674: step 1760, loss 0.256256, acc 0.953125\n",
      "2017-04-03T22:15:24.808794: step 1761, loss 0.266379, acc 0.953125\n",
      "2017-04-03T22:15:24.999596: step 1762, loss 0.27152, acc 0.921875\n",
      "2017-04-03T22:15:25.191894: step 1763, loss 0.324303, acc 0.9375\n",
      "2017-04-03T22:15:25.380162: step 1764, loss 0.391636, acc 0.921875\n",
      "2017-04-03T22:15:25.571817: step 1765, loss 0.331758, acc 0.921875\n",
      "2017-04-03T22:15:25.762722: step 1766, loss 0.398471, acc 0.859375\n",
      "2017-04-03T22:15:25.953287: step 1767, loss 0.252858, acc 0.921875\n",
      "2017-04-03T22:15:26.144831: step 1768, loss 0.531224, acc 0.8125\n",
      "2017-04-03T22:15:26.342807: step 1769, loss 0.267342, acc 0.9375\n",
      "2017-04-03T22:15:26.534282: step 1770, loss 0.265799, acc 0.90625\n",
      "2017-04-03T22:15:26.728611: step 1771, loss 0.43781, acc 0.859375\n",
      "2017-04-03T22:15:26.923695: step 1772, loss 0.301322, acc 0.90625\n",
      "2017-04-03T22:15:27.115047: step 1773, loss 0.329983, acc 0.90625\n",
      "2017-04-03T22:15:27.307557: step 1774, loss 0.330921, acc 0.921875\n",
      "2017-04-03T22:15:27.499987: step 1775, loss 0.230038, acc 0.9375\n",
      "2017-04-03T22:15:27.694576: step 1776, loss 0.236065, acc 0.9375\n",
      "2017-04-03T22:15:27.888117: step 1777, loss 0.279756, acc 0.921875\n",
      "2017-04-03T22:15:28.085603: step 1778, loss 0.19758, acc 0.96875\n",
      "2017-04-03T22:15:28.278086: step 1779, loss 0.305625, acc 0.921875\n",
      "2017-04-03T22:15:28.470026: step 1780, loss 0.398504, acc 0.90625\n",
      "2017-04-03T22:15:28.663364: step 1781, loss 0.363015, acc 0.890625\n",
      "2017-04-03T22:15:28.858165: step 1782, loss 0.216534, acc 0.953125\n",
      "2017-04-03T22:15:29.048950: step 1783, loss 0.283917, acc 0.953125\n",
      "2017-04-03T22:15:29.242386: step 1784, loss 0.319226, acc 0.859375\n",
      "2017-04-03T22:15:29.437946: step 1785, loss 0.319926, acc 0.921875\n",
      "2017-04-03T22:15:29.630357: step 1786, loss 0.266854, acc 0.9375\n",
      "2017-04-03T22:15:29.820036: step 1787, loss 0.287393, acc 0.953125\n",
      "2017-04-03T22:15:30.010004: step 1788, loss 0.255916, acc 0.9375\n",
      "2017-04-03T22:15:30.205049: step 1789, loss 0.447149, acc 0.875\n",
      "2017-04-03T22:15:30.399531: step 1790, loss 0.389872, acc 0.90625\n",
      "2017-04-03T22:15:30.589278: step 1791, loss 0.303224, acc 0.875\n",
      "2017-04-03T22:15:30.779290: step 1792, loss 0.390577, acc 0.890625\n",
      "2017-04-03T22:15:30.969770: step 1793, loss 0.467402, acc 0.8125\n",
      "2017-04-03T22:15:31.161818: step 1794, loss 0.237715, acc 0.921875\n",
      "2017-04-03T22:15:31.353144: step 1795, loss 0.156384, acc 0.984375\n",
      "2017-04-03T22:15:31.547657: step 1796, loss 0.221363, acc 0.953125\n",
      "2017-04-03T22:15:31.744487: step 1797, loss 0.303379, acc 0.859375\n",
      "2017-04-03T22:15:31.934237: step 1798, loss 0.254757, acc 0.9375\n",
      "2017-04-03T22:15:32.130059: step 1799, loss 0.209897, acc 0.96875\n",
      "2017-04-03T22:15:32.324372: step 1800, loss 0.306318, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:15:32.975296: step 1800, loss 1.41699, acc 0.539164\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1800\n",
      "\n",
      "2017-04-03T22:15:33.221876: step 1801, loss 0.361502, acc 0.84375\n",
      "2017-04-03T22:15:33.413236: step 1802, loss 0.331303, acc 0.90625\n",
      "2017-04-03T22:15:33.605511: step 1803, loss 0.27431, acc 0.90625\n",
      "2017-04-03T22:15:33.795064: step 1804, loss 0.238383, acc 0.953125\n",
      "2017-04-03T22:15:33.986095: step 1805, loss 0.181756, acc 1\n",
      "2017-04-03T22:15:34.178522: step 1806, loss 0.38293, acc 0.875\n",
      "2017-04-03T22:15:34.368919: step 1807, loss 0.238916, acc 0.984375\n",
      "2017-04-03T22:15:34.562404: step 1808, loss 0.382367, acc 0.84375\n",
      "2017-04-03T22:15:34.759292: step 1809, loss 0.270076, acc 0.921875\n",
      "2017-04-03T22:15:34.948872: step 1810, loss 0.29862, acc 0.921875\n",
      "2017-04-03T22:15:35.142567: step 1811, loss 0.254667, acc 0.90625\n",
      "2017-04-03T22:15:35.333577: step 1812, loss 0.309689, acc 0.9375\n",
      "2017-04-03T22:15:35.523119: step 1813, loss 0.331049, acc 0.9375\n",
      "2017-04-03T22:15:35.714292: step 1814, loss 0.294666, acc 0.890625\n",
      "2017-04-03T22:15:35.906652: step 1815, loss 0.224774, acc 0.96875\n",
      "2017-04-03T22:15:36.095664: step 1816, loss 0.243633, acc 0.9375\n",
      "2017-04-03T22:15:36.291821: step 1817, loss 0.218631, acc 0.96875\n",
      "2017-04-03T22:15:36.486446: step 1818, loss 0.262617, acc 0.96875\n",
      "2017-04-03T22:15:36.681682: step 1819, loss 0.266889, acc 0.90625\n",
      "2017-04-03T22:15:36.871428: step 1820, loss 0.344898, acc 0.90625\n",
      "2017-04-03T22:15:37.067090: step 1821, loss 0.19025, acc 0.953125\n",
      "2017-04-03T22:15:37.256919: step 1822, loss 0.251617, acc 0.921875\n",
      "2017-04-03T22:15:37.450670: step 1823, loss 0.275216, acc 0.96875\n",
      "2017-04-03T22:15:37.639412: step 1824, loss 0.418046, acc 0.90625\n",
      "2017-04-03T22:15:37.830082: step 1825, loss 0.288017, acc 0.9375\n",
      "2017-04-03T22:15:38.022439: step 1826, loss 0.459986, acc 0.84375\n",
      "2017-04-03T22:15:38.214590: step 1827, loss 0.325437, acc 0.890625\n",
      "2017-04-03T22:15:38.403939: step 1828, loss 0.373864, acc 0.859375\n",
      "2017-04-03T22:15:38.593146: step 1829, loss 0.302733, acc 0.921875\n",
      "2017-04-03T22:15:38.787033: step 1830, loss 0.370843, acc 0.890625\n",
      "2017-04-03T22:15:38.985845: step 1831, loss 0.283736, acc 0.953125\n",
      "2017-04-03T22:15:39.179346: step 1832, loss 0.405671, acc 0.890625\n",
      "2017-04-03T22:15:39.372695: step 1833, loss 0.30432, acc 0.90625\n",
      "2017-04-03T22:15:39.565920: step 1834, loss 0.314316, acc 0.90625\n",
      "2017-04-03T22:15:39.756953: step 1835, loss 0.297289, acc 0.890625\n",
      "2017-04-03T22:15:39.922578: step 1836, loss 0.155355, acc 0.961538\n",
      "2017-04-03T22:15:40.117206: step 1837, loss 0.252714, acc 0.9375\n",
      "2017-04-03T22:15:40.309074: step 1838, loss 0.165834, acc 0.984375\n",
      "2017-04-03T22:15:40.504802: step 1839, loss 0.299763, acc 0.921875\n",
      "2017-04-03T22:15:40.697223: step 1840, loss 0.340995, acc 0.890625\n",
      "2017-04-03T22:15:40.891356: step 1841, loss 0.23955, acc 0.9375\n",
      "2017-04-03T22:15:41.087541: step 1842, loss 0.262798, acc 0.921875\n",
      "2017-04-03T22:15:41.283342: step 1843, loss 0.258825, acc 0.9375\n",
      "2017-04-03T22:15:41.476775: step 1844, loss 0.329447, acc 0.90625\n",
      "2017-04-03T22:15:41.669609: step 1845, loss 0.248987, acc 0.953125\n",
      "2017-04-03T22:15:41.860449: step 1846, loss 0.181601, acc 0.953125\n",
      "2017-04-03T22:15:42.051710: step 1847, loss 0.231007, acc 0.90625\n",
      "2017-04-03T22:15:42.239674: step 1848, loss 0.228712, acc 0.953125\n",
      "2017-04-03T22:15:42.433466: step 1849, loss 0.258529, acc 0.953125\n",
      "2017-04-03T22:15:42.629735: step 1850, loss 0.173755, acc 0.96875\n",
      "2017-04-03T22:15:42.823821: step 1851, loss 0.26331, acc 0.9375\n",
      "2017-04-03T22:15:43.017090: step 1852, loss 0.309908, acc 0.875\n",
      "2017-04-03T22:15:43.209473: step 1853, loss 0.182419, acc 0.953125\n",
      "2017-04-03T22:15:43.403227: step 1854, loss 0.213104, acc 0.96875\n",
      "2017-04-03T22:15:43.601194: step 1855, loss 0.209811, acc 0.96875\n",
      "2017-04-03T22:15:43.797077: step 1856, loss 0.201578, acc 0.953125\n",
      "2017-04-03T22:15:43.988680: step 1857, loss 0.184737, acc 0.984375\n",
      "2017-04-03T22:15:44.183486: step 1858, loss 0.174222, acc 0.96875\n",
      "2017-04-03T22:15:44.376059: step 1859, loss 0.288433, acc 0.9375\n",
      "2017-04-03T22:15:44.566699: step 1860, loss 0.194175, acc 0.984375\n",
      "2017-04-03T22:15:44.762705: step 1861, loss 0.24391, acc 0.90625\n",
      "2017-04-03T22:15:44.961680: step 1862, loss 0.293809, acc 0.90625\n",
      "2017-04-03T22:15:45.155932: step 1863, loss 0.197078, acc 0.953125\n",
      "2017-04-03T22:15:45.348075: step 1864, loss 0.184461, acc 1\n",
      "2017-04-03T22:15:45.540968: step 1865, loss 0.297646, acc 0.90625\n",
      "2017-04-03T22:15:45.735675: step 1866, loss 0.248051, acc 0.9375\n",
      "2017-04-03T22:15:45.928745: step 1867, loss 0.203918, acc 0.921875\n",
      "2017-04-03T22:15:46.117276: step 1868, loss 0.287463, acc 0.890625\n",
      "2017-04-03T22:15:46.309954: step 1869, loss 0.358059, acc 0.921875\n",
      "2017-04-03T22:15:46.504626: step 1870, loss 0.237318, acc 0.953125\n",
      "2017-04-03T22:15:46.697457: step 1871, loss 0.325029, acc 0.90625\n",
      "2017-04-03T22:15:46.887731: step 1872, loss 0.365148, acc 0.921875\n",
      "2017-04-03T22:15:47.078126: step 1873, loss 0.213588, acc 0.9375\n",
      "2017-04-03T22:15:47.269512: step 1874, loss 0.230194, acc 0.9375\n",
      "2017-04-03T22:15:47.463125: step 1875, loss 0.246305, acc 0.953125\n",
      "2017-04-03T22:15:47.661214: step 1876, loss 0.232931, acc 0.953125\n",
      "2017-04-03T22:15:47.852046: step 1877, loss 0.197324, acc 0.9375\n",
      "2017-04-03T22:15:48.050982: step 1878, loss 0.338517, acc 0.90625\n",
      "2017-04-03T22:15:48.244502: step 1879, loss 0.240215, acc 0.9375\n",
      "2017-04-03T22:15:48.436688: step 1880, loss 0.247891, acc 0.9375\n",
      "2017-04-03T22:15:48.626578: step 1881, loss 0.16093, acc 0.96875\n",
      "2017-04-03T22:15:48.820441: step 1882, loss 0.201429, acc 0.953125\n",
      "2017-04-03T22:15:49.014215: step 1883, loss 0.230528, acc 0.953125\n",
      "2017-04-03T22:15:49.203850: step 1884, loss 0.2353, acc 0.953125\n",
      "2017-04-03T22:15:49.396614: step 1885, loss 0.292782, acc 0.875\n",
      "2017-04-03T22:15:49.590007: step 1886, loss 0.206751, acc 0.953125\n",
      "2017-04-03T22:15:49.789880: step 1887, loss 0.265886, acc 0.96875\n",
      "2017-04-03T22:15:49.984669: step 1888, loss 0.288388, acc 0.890625\n",
      "2017-04-03T22:15:50.174760: step 1889, loss 0.312977, acc 0.90625\n",
      "2017-04-03T22:15:50.370233: step 1890, loss 0.244819, acc 0.953125\n",
      "2017-04-03T22:15:50.564721: step 1891, loss 0.259921, acc 0.90625\n",
      "2017-04-03T22:15:50.761115: step 1892, loss 0.227655, acc 0.953125\n",
      "2017-04-03T22:15:50.954705: step 1893, loss 0.254686, acc 0.921875\n",
      "2017-04-03T22:15:51.145365: step 1894, loss 0.205472, acc 0.96875\n",
      "2017-04-03T22:15:51.336024: step 1895, loss 0.23984, acc 0.9375\n",
      "2017-04-03T22:15:51.523928: step 1896, loss 0.27179, acc 0.9375\n",
      "2017-04-03T22:15:51.716215: step 1897, loss 0.264528, acc 0.890625\n",
      "2017-04-03T22:15:51.909694: step 1898, loss 0.245364, acc 0.9375\n",
      "2017-04-03T22:15:52.104315: step 1899, loss 0.280198, acc 0.90625\n",
      "2017-04-03T22:15:52.298475: step 1900, loss 0.282755, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:15:52.944984: step 1900, loss 1.41389, acc 0.552219\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-1900\n",
      "\n",
      "2017-04-03T22:15:53.196300: step 1901, loss 0.237485, acc 0.953125\n",
      "2017-04-03T22:15:53.394702: step 1902, loss 0.22319, acc 0.953125\n",
      "2017-04-03T22:15:53.584740: step 1903, loss 0.259999, acc 0.953125\n",
      "2017-04-03T22:15:53.773239: step 1904, loss 0.231125, acc 0.9375\n",
      "2017-04-03T22:15:53.969279: step 1905, loss 0.321835, acc 0.90625\n",
      "2017-04-03T22:15:54.161495: step 1906, loss 0.265585, acc 0.90625\n",
      "2017-04-03T22:15:54.355136: step 1907, loss 0.308787, acc 0.890625\n",
      "2017-04-03T22:15:54.547942: step 1908, loss 0.295826, acc 0.921875\n",
      "2017-04-03T22:15:54.738013: step 1909, loss 0.411783, acc 0.859375\n",
      "2017-04-03T22:15:54.928163: step 1910, loss 0.314756, acc 0.9375\n",
      "2017-04-03T22:15:55.119619: step 1911, loss 0.209239, acc 0.9375\n",
      "2017-04-03T22:15:55.314023: step 1912, loss 0.302719, acc 0.90625\n",
      "2017-04-03T22:15:55.507885: step 1913, loss 0.370124, acc 0.90625\n",
      "2017-04-03T22:15:55.704722: step 1914, loss 0.343415, acc 0.890625\n",
      "2017-04-03T22:15:55.897016: step 1915, loss 0.14647, acc 1\n",
      "2017-04-03T22:15:56.092937: step 1916, loss 0.246406, acc 0.9375\n",
      "2017-04-03T22:15:56.286772: step 1917, loss 0.200335, acc 0.96875\n",
      "2017-04-03T22:15:56.477105: step 1918, loss 0.337592, acc 0.890625\n",
      "2017-04-03T22:15:56.670189: step 1919, loss 0.311549, acc 0.875\n",
      "2017-04-03T22:15:56.860906: step 1920, loss 0.14212, acc 0.984375\n",
      "2017-04-03T22:15:57.053092: step 1921, loss 0.235994, acc 0.9375\n",
      "2017-04-03T22:15:57.250099: step 1922, loss 0.19445, acc 0.9375\n",
      "2017-04-03T22:15:57.441249: step 1923, loss 0.186642, acc 0.953125\n",
      "2017-04-03T22:15:57.634665: step 1924, loss 0.205197, acc 0.9375\n",
      "2017-04-03T22:15:57.827299: step 1925, loss 0.29055, acc 0.90625\n",
      "2017-04-03T22:15:58.016223: step 1926, loss 0.228519, acc 0.953125\n",
      "2017-04-03T22:15:58.204169: step 1927, loss 0.393518, acc 0.859375\n",
      "2017-04-03T22:15:58.396407: step 1928, loss 0.345524, acc 0.90625\n",
      "2017-04-03T22:15:58.592664: step 1929, loss 0.24659, acc 0.953125\n",
      "2017-04-03T22:15:58.787561: step 1930, loss 0.364749, acc 0.890625\n",
      "2017-04-03T22:15:58.979488: step 1931, loss 0.23468, acc 0.96875\n",
      "2017-04-03T22:15:59.170028: step 1932, loss 0.299286, acc 0.90625\n",
      "2017-04-03T22:15:59.362261: step 1933, loss 0.229226, acc 0.90625\n",
      "2017-04-03T22:15:59.557868: step 1934, loss 0.337569, acc 0.90625\n",
      "2017-04-03T22:15:59.755465: step 1935, loss 0.329354, acc 0.875\n",
      "2017-04-03T22:15:59.945290: step 1936, loss 0.263721, acc 0.921875\n",
      "2017-04-03T22:16:00.141126: step 1937, loss 0.285925, acc 0.890625\n",
      "2017-04-03T22:16:00.334040: step 1938, loss 0.263375, acc 0.921875\n",
      "2017-04-03T22:16:00.526187: step 1939, loss 0.312991, acc 0.90625\n",
      "2017-04-03T22:16:00.718066: step 1940, loss 0.328351, acc 0.921875\n",
      "2017-04-03T22:16:00.917201: step 1941, loss 0.201726, acc 0.953125\n",
      "2017-04-03T22:16:01.106650: step 1942, loss 0.234465, acc 0.9375\n",
      "2017-04-03T22:16:01.302465: step 1943, loss 0.289439, acc 0.90625\n",
      "2017-04-03T22:16:01.463658: step 1944, loss 0.234106, acc 0.942308\n",
      "2017-04-03T22:16:01.659058: step 1945, loss 0.23059, acc 0.9375\n",
      "2017-04-03T22:16:01.849161: step 1946, loss 0.142535, acc 0.984375\n",
      "2017-04-03T22:16:02.044856: step 1947, loss 0.281132, acc 0.90625\n",
      "2017-04-03T22:16:02.240172: step 1948, loss 0.219086, acc 0.953125\n",
      "2017-04-03T22:16:02.436736: step 1949, loss 0.134108, acc 0.96875\n",
      "2017-04-03T22:16:02.627957: step 1950, loss 0.14942, acc 1\n",
      "2017-04-03T22:16:02.821074: step 1951, loss 0.245542, acc 0.953125\n",
      "2017-04-03T22:16:03.011522: step 1952, loss 0.280745, acc 0.90625\n",
      "2017-04-03T22:16:03.200787: step 1953, loss 0.180782, acc 1\n",
      "2017-04-03T22:16:03.392495: step 1954, loss 0.238571, acc 0.9375\n",
      "2017-04-03T22:16:03.586973: step 1955, loss 0.180679, acc 0.953125\n",
      "2017-04-03T22:16:03.775368: step 1956, loss 0.217871, acc 0.96875\n",
      "2017-04-03T22:16:03.969956: step 1957, loss 0.236419, acc 0.921875\n",
      "2017-04-03T22:16:04.158995: step 1958, loss 0.244264, acc 0.9375\n",
      "2017-04-03T22:16:04.350997: step 1959, loss 0.255977, acc 0.9375\n",
      "2017-04-03T22:16:04.546944: step 1960, loss 0.300414, acc 0.90625\n",
      "2017-04-03T22:16:04.739913: step 1961, loss 0.176344, acc 0.9375\n",
      "2017-04-03T22:16:04.931267: step 1962, loss 0.260326, acc 0.921875\n",
      "2017-04-03T22:16:05.123864: step 1963, loss 0.252999, acc 0.9375\n",
      "2017-04-03T22:16:05.321976: step 1964, loss 0.196166, acc 0.96875\n",
      "2017-04-03T22:16:05.520600: step 1965, loss 0.249168, acc 0.9375\n",
      "2017-04-03T22:16:05.710276: step 1966, loss 0.27689, acc 0.9375\n",
      "2017-04-03T22:16:05.903974: step 1967, loss 0.209415, acc 0.9375\n",
      "2017-04-03T22:16:06.096354: step 1968, loss 0.272104, acc 0.90625\n",
      "2017-04-03T22:16:06.290744: step 1969, loss 0.296666, acc 0.9375\n",
      "2017-04-03T22:16:06.484375: step 1970, loss 0.38627, acc 0.90625\n",
      "2017-04-03T22:16:06.676873: step 1971, loss 0.253652, acc 0.921875\n",
      "2017-04-03T22:16:06.869863: step 1972, loss 0.21682, acc 0.9375\n",
      "2017-04-03T22:16:07.067370: step 1973, loss 0.221528, acc 0.921875\n",
      "2017-04-03T22:16:07.256471: step 1974, loss 0.390936, acc 0.875\n",
      "2017-04-03T22:16:07.445535: step 1975, loss 0.143418, acc 0.953125\n",
      "2017-04-03T22:16:07.637312: step 1976, loss 0.292811, acc 0.890625\n",
      "2017-04-03T22:16:07.830223: step 1977, loss 0.226018, acc 0.921875\n",
      "2017-04-03T22:16:08.029282: step 1978, loss 0.316788, acc 0.921875\n",
      "2017-04-03T22:16:08.224106: step 1979, loss 0.214799, acc 0.953125\n",
      "2017-04-03T22:16:08.412680: step 1980, loss 0.321026, acc 0.890625\n",
      "2017-04-03T22:16:08.604102: step 1981, loss 0.252629, acc 0.921875\n",
      "2017-04-03T22:16:08.791721: step 1982, loss 0.224898, acc 0.9375\n",
      "2017-04-03T22:16:08.982950: step 1983, loss 0.220405, acc 0.9375\n",
      "2017-04-03T22:16:09.177668: step 1984, loss 0.245314, acc 0.953125\n",
      "2017-04-03T22:16:09.370611: step 1985, loss 0.19804, acc 0.953125\n",
      "2017-04-03T22:16:09.558259: step 1986, loss 0.204664, acc 0.96875\n",
      "2017-04-03T22:16:09.755029: step 1987, loss 0.152932, acc 0.953125\n",
      "2017-04-03T22:16:09.950438: step 1988, loss 0.191825, acc 0.953125\n",
      "2017-04-03T22:16:10.147551: step 1989, loss 0.198105, acc 0.96875\n",
      "2017-04-03T22:16:10.340785: step 1990, loss 0.168153, acc 0.953125\n",
      "2017-04-03T22:16:10.531046: step 1991, loss 0.19018, acc 0.921875\n",
      "2017-04-03T22:16:10.728362: step 1992, loss 0.149501, acc 0.984375\n",
      "2017-04-03T22:16:10.921253: step 1993, loss 0.206105, acc 0.953125\n",
      "2017-04-03T22:16:11.110679: step 1994, loss 0.26826, acc 0.921875\n",
      "2017-04-03T22:16:11.300530: step 1995, loss 0.254227, acc 0.953125\n",
      "2017-04-03T22:16:11.492251: step 1996, loss 0.231673, acc 0.953125\n",
      "2017-04-03T22:16:11.687331: step 1997, loss 0.239216, acc 0.921875\n",
      "2017-04-03T22:16:11.877913: step 1998, loss 0.298036, acc 0.90625\n",
      "2017-04-03T22:16:12.069147: step 1999, loss 0.211859, acc 0.90625\n",
      "2017-04-03T22:16:12.261630: step 2000, loss 0.168451, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:16:12.909481: step 2000, loss 1.43185, acc 0.556136\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2000\n",
      "\n",
      "2017-04-03T22:16:13.165267: step 2001, loss 0.306735, acc 0.90625\n",
      "2017-04-03T22:16:13.361506: step 2002, loss 0.190317, acc 0.953125\n",
      "2017-04-03T22:16:13.557941: step 2003, loss 0.155654, acc 1\n",
      "2017-04-03T22:16:13.751783: step 2004, loss 0.198682, acc 0.953125\n",
      "2017-04-03T22:16:13.945066: step 2005, loss 0.305462, acc 0.9375\n",
      "2017-04-03T22:16:14.136262: step 2006, loss 0.170145, acc 0.984375\n",
      "2017-04-03T22:16:14.329238: step 2007, loss 0.18292, acc 0.96875\n",
      "2017-04-03T22:16:14.522332: step 2008, loss 0.186486, acc 0.984375\n",
      "2017-04-03T22:16:14.713321: step 2009, loss 0.24549, acc 0.921875\n",
      "2017-04-03T22:16:14.909270: step 2010, loss 0.307414, acc 0.875\n",
      "2017-04-03T22:16:15.100071: step 2011, loss 0.199209, acc 0.96875\n",
      "2017-04-03T22:16:15.296629: step 2012, loss 0.16654, acc 0.984375\n",
      "2017-04-03T22:16:15.492346: step 2013, loss 0.442497, acc 0.859375\n",
      "2017-04-03T22:16:15.687182: step 2014, loss 0.236221, acc 0.9375\n",
      "2017-04-03T22:16:15.876766: step 2015, loss 0.360273, acc 0.9375\n",
      "2017-04-03T22:16:16.069597: step 2016, loss 0.31249, acc 0.9375\n",
      "2017-04-03T22:16:16.262171: step 2017, loss 0.311526, acc 0.90625\n",
      "2017-04-03T22:16:16.453029: step 2018, loss 0.229124, acc 0.953125\n",
      "2017-04-03T22:16:16.642878: step 2019, loss 0.266984, acc 0.9375\n",
      "2017-04-03T22:16:16.834206: step 2020, loss 0.269511, acc 0.90625\n",
      "2017-04-03T22:16:17.024990: step 2021, loss 0.272746, acc 0.921875\n",
      "2017-04-03T22:16:17.218297: step 2022, loss 0.217779, acc 0.953125\n",
      "2017-04-03T22:16:17.406105: step 2023, loss 0.269382, acc 0.921875\n",
      "2017-04-03T22:16:17.599202: step 2024, loss 0.222533, acc 0.921875\n",
      "2017-04-03T22:16:17.792540: step 2025, loss 0.148875, acc 0.984375\n",
      "2017-04-03T22:16:17.982112: step 2026, loss 0.251652, acc 0.984375\n",
      "2017-04-03T22:16:18.175097: step 2027, loss 0.27168, acc 0.90625\n",
      "2017-04-03T22:16:18.367069: step 2028, loss 0.214796, acc 0.9375\n",
      "2017-04-03T22:16:18.561282: step 2029, loss 0.12949, acc 0.96875\n",
      "2017-04-03T22:16:18.752618: step 2030, loss 0.220177, acc 0.953125\n",
      "2017-04-03T22:16:18.948608: step 2031, loss 0.280201, acc 0.90625\n",
      "2017-04-03T22:16:19.139384: step 2032, loss 0.179788, acc 0.921875\n",
      "2017-04-03T22:16:19.333028: step 2033, loss 0.251731, acc 0.9375\n",
      "2017-04-03T22:16:19.526591: step 2034, loss 0.136869, acc 0.953125\n",
      "2017-04-03T22:16:19.719653: step 2035, loss 0.333675, acc 0.84375\n",
      "2017-04-03T22:16:19.911336: step 2036, loss 0.200938, acc 0.953125\n",
      "2017-04-03T22:16:20.105606: step 2037, loss 0.223443, acc 0.9375\n",
      "2017-04-03T22:16:20.297843: step 2038, loss 0.30274, acc 0.921875\n",
      "2017-04-03T22:16:20.491490: step 2039, loss 0.278957, acc 0.890625\n",
      "2017-04-03T22:16:20.682921: step 2040, loss 0.194009, acc 1\n",
      "2017-04-03T22:16:20.876249: step 2041, loss 0.286573, acc 0.9375\n",
      "2017-04-03T22:16:21.064379: step 2042, loss 0.216096, acc 0.921875\n",
      "2017-04-03T22:16:21.259470: step 2043, loss 0.321296, acc 0.90625\n",
      "2017-04-03T22:16:21.449302: step 2044, loss 0.345219, acc 0.875\n",
      "2017-04-03T22:16:21.637498: step 2045, loss 0.255451, acc 0.90625\n",
      "2017-04-03T22:16:21.833019: step 2046, loss 0.223918, acc 0.921875\n",
      "2017-04-03T22:16:22.026275: step 2047, loss 0.137719, acc 0.96875\n",
      "2017-04-03T22:16:22.220753: step 2048, loss 0.134252, acc 0.953125\n",
      "2017-04-03T22:16:22.417415: step 2049, loss 0.342985, acc 0.859375\n",
      "2017-04-03T22:16:22.610076: step 2050, loss 0.206097, acc 0.96875\n",
      "2017-04-03T22:16:22.803691: step 2051, loss 0.270579, acc 0.9375\n",
      "2017-04-03T22:16:22.963082: step 2052, loss 0.184438, acc 0.942308\n",
      "2017-04-03T22:16:23.154979: step 2053, loss 0.158825, acc 0.96875\n",
      "2017-04-03T22:16:23.349185: step 2054, loss 0.198203, acc 0.9375\n",
      "2017-04-03T22:16:23.546951: step 2055, loss 0.10563, acc 0.984375\n",
      "2017-04-03T22:16:23.738923: step 2056, loss 0.263963, acc 0.890625\n",
      "2017-04-03T22:16:23.934550: step 2057, loss 0.214642, acc 0.921875\n",
      "2017-04-03T22:16:24.126732: step 2058, loss 0.32029, acc 0.890625\n",
      "2017-04-03T22:16:24.317614: step 2059, loss 0.236207, acc 0.90625\n",
      "2017-04-03T22:16:24.509981: step 2060, loss 0.255143, acc 0.9375\n",
      "2017-04-03T22:16:24.700857: step 2061, loss 0.139273, acc 0.96875\n",
      "2017-04-03T22:16:24.899395: step 2062, loss 0.235286, acc 0.921875\n",
      "2017-04-03T22:16:25.093050: step 2063, loss 0.250766, acc 0.90625\n",
      "2017-04-03T22:16:25.285869: step 2064, loss 0.207483, acc 0.9375\n",
      "2017-04-03T22:16:25.478744: step 2065, loss 0.1659, acc 0.953125\n",
      "2017-04-03T22:16:25.673622: step 2066, loss 0.235587, acc 0.90625\n",
      "2017-04-03T22:16:25.865897: step 2067, loss 0.28972, acc 0.921875\n",
      "2017-04-03T22:16:26.056028: step 2068, loss 0.307489, acc 0.890625\n",
      "2017-04-03T22:16:26.247107: step 2069, loss 0.214179, acc 0.953125\n",
      "2017-04-03T22:16:26.442796: step 2070, loss 0.128286, acc 0.953125\n",
      "2017-04-03T22:16:26.633955: step 2071, loss 0.210485, acc 0.96875\n",
      "2017-04-03T22:16:26.828662: step 2072, loss 0.335424, acc 0.921875\n",
      "2017-04-03T22:16:27.019977: step 2073, loss 0.319091, acc 0.921875\n",
      "2017-04-03T22:16:27.213051: step 2074, loss 0.296606, acc 0.921875\n",
      "2017-04-03T22:16:27.406474: step 2075, loss 0.181985, acc 0.953125\n",
      "2017-04-03T22:16:27.599344: step 2076, loss 0.166652, acc 0.953125\n",
      "2017-04-03T22:16:27.791584: step 2077, loss 0.274299, acc 0.890625\n",
      "2017-04-03T22:16:27.984448: step 2078, loss 0.142982, acc 0.984375\n",
      "2017-04-03T22:16:28.178058: step 2079, loss 0.137848, acc 0.96875\n",
      "2017-04-03T22:16:28.372954: step 2080, loss 0.190222, acc 0.9375\n",
      "2017-04-03T22:16:28.565206: step 2081, loss 0.116979, acc 0.96875\n",
      "2017-04-03T22:16:28.765034: step 2082, loss 0.227072, acc 0.921875\n",
      "2017-04-03T22:16:28.955734: step 2083, loss 0.247117, acc 0.921875\n",
      "2017-04-03T22:16:29.150001: step 2084, loss 0.170907, acc 0.953125\n",
      "2017-04-03T22:16:29.343932: step 2085, loss 0.127029, acc 0.96875\n",
      "2017-04-03T22:16:29.539595: step 2086, loss 0.19096, acc 0.9375\n",
      "2017-04-03T22:16:29.732032: step 2087, loss 0.18091, acc 0.953125\n",
      "2017-04-03T22:16:29.925544: step 2088, loss 0.222226, acc 0.953125\n",
      "2017-04-03T22:16:30.118897: step 2089, loss 0.225977, acc 0.96875\n",
      "2017-04-03T22:16:30.309681: step 2090, loss 0.186357, acc 0.96875\n",
      "2017-04-03T22:16:30.499566: step 2091, loss 0.158912, acc 1\n",
      "2017-04-03T22:16:30.691156: step 2092, loss 0.218466, acc 0.953125\n",
      "2017-04-03T22:16:30.881352: step 2093, loss 0.257523, acc 0.921875\n",
      "2017-04-03T22:16:31.076480: step 2094, loss 0.262105, acc 0.921875\n",
      "2017-04-03T22:16:31.268409: step 2095, loss 0.113815, acc 0.96875\n",
      "2017-04-03T22:16:31.458214: step 2096, loss 0.175773, acc 0.96875\n",
      "2017-04-03T22:16:31.647073: step 2097, loss 0.240811, acc 0.9375\n",
      "2017-04-03T22:16:31.842588: step 2098, loss 0.150281, acc 0.984375\n",
      "2017-04-03T22:16:32.037309: step 2099, loss 0.227782, acc 0.9375\n",
      "2017-04-03T22:16:32.226797: step 2100, loss 0.153966, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:16:32.889056: step 2100, loss 1.45156, acc 0.55483\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2100\n",
      "\n",
      "2017-04-03T22:16:33.141684: step 2101, loss 0.192552, acc 0.984375\n",
      "2017-04-03T22:16:33.338179: step 2102, loss 0.162157, acc 0.96875\n",
      "2017-04-03T22:16:33.528213: step 2103, loss 0.181954, acc 0.96875\n",
      "2017-04-03T22:16:33.717037: step 2104, loss 0.140308, acc 1\n",
      "2017-04-03T22:16:33.912842: step 2105, loss 0.0987622, acc 1\n",
      "2017-04-03T22:16:34.102391: step 2106, loss 0.195468, acc 0.96875\n",
      "2017-04-03T22:16:34.294007: step 2107, loss 0.259201, acc 0.90625\n",
      "2017-04-03T22:16:34.488440: step 2108, loss 0.115536, acc 0.96875\n",
      "2017-04-03T22:16:34.683290: step 2109, loss 0.23024, acc 0.921875\n",
      "2017-04-03T22:16:34.875397: step 2110, loss 0.188588, acc 0.9375\n",
      "2017-04-03T22:16:35.071234: step 2111, loss 0.217819, acc 0.9375\n",
      "2017-04-03T22:16:35.262703: step 2112, loss 0.202404, acc 0.953125\n",
      "2017-04-03T22:16:35.452321: step 2113, loss 0.16557, acc 0.984375\n",
      "2017-04-03T22:16:35.643418: step 2114, loss 0.207594, acc 0.953125\n",
      "2017-04-03T22:16:35.836312: step 2115, loss 0.157182, acc 0.953125\n",
      "2017-04-03T22:16:36.030606: step 2116, loss 0.143386, acc 1\n",
      "2017-04-03T22:16:36.225583: step 2117, loss 0.197378, acc 0.9375\n",
      "2017-04-03T22:16:36.420720: step 2118, loss 0.239185, acc 0.9375\n",
      "2017-04-03T22:16:36.613006: step 2119, loss 0.181153, acc 0.9375\n",
      "2017-04-03T22:16:36.807037: step 2120, loss 0.235285, acc 0.9375\n",
      "2017-04-03T22:16:37.000107: step 2121, loss 0.343244, acc 0.875\n",
      "2017-04-03T22:16:37.192190: step 2122, loss 0.252513, acc 0.921875\n",
      "2017-04-03T22:16:37.385228: step 2123, loss 0.2354, acc 0.953125\n",
      "2017-04-03T22:16:37.576212: step 2124, loss 0.166327, acc 0.9375\n",
      "2017-04-03T22:16:37.768890: step 2125, loss 0.208466, acc 0.953125\n",
      "2017-04-03T22:16:37.960400: step 2126, loss 0.232932, acc 0.921875\n",
      "2017-04-03T22:16:38.152302: step 2127, loss 0.224046, acc 0.96875\n",
      "2017-04-03T22:16:38.343051: step 2128, loss 0.20103, acc 0.9375\n",
      "2017-04-03T22:16:38.533669: step 2129, loss 0.172269, acc 0.953125\n",
      "2017-04-03T22:16:38.724550: step 2130, loss 0.246486, acc 0.921875\n",
      "2017-04-03T22:16:38.919281: step 2131, loss 0.114241, acc 0.984375\n",
      "2017-04-03T22:16:39.113619: step 2132, loss 0.273148, acc 0.9375\n",
      "2017-04-03T22:16:39.302799: step 2133, loss 0.20537, acc 0.953125\n",
      "2017-04-03T22:16:39.496957: step 2134, loss 0.188433, acc 0.9375\n",
      "2017-04-03T22:16:39.691934: step 2135, loss 0.203097, acc 0.9375\n",
      "2017-04-03T22:16:39.884660: step 2136, loss 0.275402, acc 0.90625\n",
      "2017-04-03T22:16:40.077553: step 2137, loss 0.185624, acc 0.96875\n",
      "2017-04-03T22:16:40.269389: step 2138, loss 0.189758, acc 0.9375\n",
      "2017-04-03T22:16:40.460837: step 2139, loss 0.142368, acc 0.96875\n",
      "2017-04-03T22:16:40.653065: step 2140, loss 0.184651, acc 0.9375\n",
      "2017-04-03T22:16:40.845189: step 2141, loss 0.0989135, acc 0.984375\n",
      "2017-04-03T22:16:41.036971: step 2142, loss 0.128874, acc 0.984375\n",
      "2017-04-03T22:16:41.231882: step 2143, loss 0.153451, acc 0.9375\n",
      "2017-04-03T22:16:41.422535: step 2144, loss 0.136355, acc 0.96875\n",
      "2017-04-03T22:16:41.616357: step 2145, loss 0.24172, acc 0.953125\n",
      "2017-04-03T22:16:41.809042: step 2146, loss 0.300066, acc 0.859375\n",
      "2017-04-03T22:16:42.003568: step 2147, loss 0.152944, acc 0.96875\n",
      "2017-04-03T22:16:42.197362: step 2148, loss 0.165212, acc 0.921875\n",
      "2017-04-03T22:16:42.392537: step 2149, loss 0.183266, acc 0.921875\n",
      "2017-04-03T22:16:42.582756: step 2150, loss 0.305168, acc 0.90625\n",
      "2017-04-03T22:16:42.774732: step 2151, loss 0.212194, acc 0.9375\n",
      "2017-04-03T22:16:42.964607: step 2152, loss 0.288558, acc 0.921875\n",
      "2017-04-03T22:16:43.155870: step 2153, loss 0.166943, acc 0.9375\n",
      "2017-04-03T22:16:43.348647: step 2154, loss 0.228747, acc 0.9375\n",
      "2017-04-03T22:16:43.540701: step 2155, loss 0.244421, acc 0.921875\n",
      "2017-04-03T22:16:43.736572: step 2156, loss 0.267855, acc 0.921875\n",
      "2017-04-03T22:16:43.926282: step 2157, loss 0.203764, acc 0.9375\n",
      "2017-04-03T22:16:44.121124: step 2158, loss 0.209644, acc 0.953125\n",
      "2017-04-03T22:16:44.314253: step 2159, loss 0.199587, acc 0.953125\n",
      "2017-04-03T22:16:44.474801: step 2160, loss 0.304583, acc 0.884615\n",
      "2017-04-03T22:16:44.665728: step 2161, loss 0.329621, acc 0.9375\n",
      "2017-04-03T22:16:44.858304: step 2162, loss 0.208802, acc 0.9375\n",
      "2017-04-03T22:16:45.051536: step 2163, loss 0.230455, acc 0.953125\n",
      "2017-04-03T22:16:45.246439: step 2164, loss 0.147668, acc 0.96875\n",
      "2017-04-03T22:16:45.440333: step 2165, loss 0.382412, acc 0.90625\n",
      "2017-04-03T22:16:45.631975: step 2166, loss 0.194468, acc 0.9375\n",
      "2017-04-03T22:16:45.825133: step 2167, loss 0.13182, acc 0.984375\n",
      "2017-04-03T22:16:46.015151: step 2168, loss 0.15358, acc 0.984375\n",
      "2017-04-03T22:16:46.210483: step 2169, loss 0.244059, acc 0.921875\n",
      "2017-04-03T22:16:46.400846: step 2170, loss 0.207883, acc 0.921875\n",
      "2017-04-03T22:16:46.588418: step 2171, loss 0.136661, acc 1\n",
      "2017-04-03T22:16:46.782076: step 2172, loss 0.19755, acc 0.9375\n",
      "2017-04-03T22:16:46.974289: step 2173, loss 0.207141, acc 0.9375\n",
      "2017-04-03T22:16:47.168111: step 2174, loss 0.103097, acc 0.984375\n",
      "2017-04-03T22:16:47.364343: step 2175, loss 0.184376, acc 0.96875\n",
      "2017-04-03T22:16:47.554162: step 2176, loss 0.17267, acc 0.96875\n",
      "2017-04-03T22:16:47.746204: step 2177, loss 0.134154, acc 0.96875\n",
      "2017-04-03T22:16:47.940888: step 2178, loss 0.222831, acc 0.9375\n",
      "2017-04-03T22:16:48.142187: step 2179, loss 0.176573, acc 0.953125\n",
      "2017-04-03T22:16:48.340405: step 2180, loss 0.146507, acc 0.96875\n",
      "2017-04-03T22:16:48.530334: step 2181, loss 0.236068, acc 0.90625\n",
      "2017-04-03T22:16:48.724304: step 2182, loss 0.18835, acc 0.984375\n",
      "2017-04-03T22:16:48.915202: step 2183, loss 0.146124, acc 0.96875\n",
      "2017-04-03T22:16:49.106585: step 2184, loss 0.229952, acc 0.953125\n",
      "2017-04-03T22:16:49.301646: step 2185, loss 0.182563, acc 0.953125\n",
      "2017-04-03T22:16:49.492022: step 2186, loss 0.158245, acc 0.984375\n",
      "2017-04-03T22:16:49.686725: step 2187, loss 0.231187, acc 0.921875\n",
      "2017-04-03T22:16:49.879609: step 2188, loss 0.160051, acc 0.96875\n",
      "2017-04-03T22:16:50.080357: step 2189, loss 0.220301, acc 0.9375\n",
      "2017-04-03T22:16:50.270330: step 2190, loss 0.151521, acc 0.96875\n",
      "2017-04-03T22:16:50.456700: step 2191, loss 0.336545, acc 0.875\n",
      "2017-04-03T22:16:50.654211: step 2192, loss 0.145302, acc 0.96875\n",
      "2017-04-03T22:16:50.849938: step 2193, loss 0.260723, acc 0.9375\n",
      "2017-04-03T22:16:51.044449: step 2194, loss 0.268122, acc 0.90625\n",
      "2017-04-03T22:16:51.238719: step 2195, loss 0.119494, acc 1\n",
      "2017-04-03T22:16:51.429971: step 2196, loss 0.148918, acc 0.9375\n",
      "2017-04-03T22:16:51.623622: step 2197, loss 0.153515, acc 0.984375\n",
      "2017-04-03T22:16:51.816026: step 2198, loss 0.115694, acc 0.984375\n",
      "2017-04-03T22:16:52.007301: step 2199, loss 0.106702, acc 0.984375\n",
      "2017-04-03T22:16:52.201397: step 2200, loss 0.143659, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:16:52.857716: step 2200, loss 1.45816, acc 0.56658\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2200\n",
      "\n",
      "2017-04-03T22:16:53.119245: step 2201, loss 0.15838, acc 0.984375\n",
      "2017-04-03T22:16:53.314917: step 2202, loss 0.117486, acc 0.984375\n",
      "2017-04-03T22:16:53.508461: step 2203, loss 0.250014, acc 0.9375\n",
      "2017-04-03T22:16:53.698594: step 2204, loss 0.222257, acc 0.953125\n",
      "2017-04-03T22:16:53.892280: step 2205, loss 0.203113, acc 0.9375\n",
      "2017-04-03T22:16:54.082594: step 2206, loss 0.24348, acc 0.9375\n",
      "2017-04-03T22:16:54.276725: step 2207, loss 0.253966, acc 0.921875\n",
      "2017-04-03T22:16:54.464896: step 2208, loss 0.128677, acc 0.96875\n",
      "2017-04-03T22:16:54.659560: step 2209, loss 0.215502, acc 0.953125\n",
      "2017-04-03T22:16:54.854097: step 2210, loss 0.106773, acc 0.984375\n",
      "2017-04-03T22:16:55.050179: step 2211, loss 0.190487, acc 0.9375\n",
      "2017-04-03T22:16:55.242444: step 2212, loss 0.164503, acc 0.953125\n",
      "2017-04-03T22:16:55.439577: step 2213, loss 0.153224, acc 0.96875\n",
      "2017-04-03T22:16:55.630368: step 2214, loss 0.187971, acc 0.921875\n",
      "2017-04-03T22:16:55.825030: step 2215, loss 0.164638, acc 0.953125\n",
      "2017-04-03T22:16:56.018894: step 2216, loss 0.179674, acc 0.953125\n",
      "2017-04-03T22:16:56.216450: step 2217, loss 0.205904, acc 0.921875\n",
      "2017-04-03T22:16:56.408635: step 2218, loss 0.21018, acc 0.90625\n",
      "2017-04-03T22:16:56.606367: step 2219, loss 0.179177, acc 0.9375\n",
      "2017-04-03T22:16:56.798189: step 2220, loss 0.149958, acc 0.953125\n",
      "2017-04-03T22:16:56.989117: step 2221, loss 0.121706, acc 0.96875\n",
      "2017-04-03T22:16:57.175814: step 2222, loss 0.156083, acc 0.96875\n",
      "2017-04-03T22:16:57.367359: step 2223, loss 0.208202, acc 0.9375\n",
      "2017-04-03T22:16:57.562241: step 2224, loss 0.139656, acc 0.984375\n",
      "2017-04-03T22:16:57.754935: step 2225, loss 0.187399, acc 0.9375\n",
      "2017-04-03T22:16:57.949851: step 2226, loss 0.184024, acc 0.953125\n",
      "2017-04-03T22:16:58.141613: step 2227, loss 0.222022, acc 0.96875\n",
      "2017-04-03T22:16:58.333952: step 2228, loss 0.202783, acc 0.96875\n",
      "2017-04-03T22:16:58.528574: step 2229, loss 0.218227, acc 0.921875\n",
      "2017-04-03T22:16:58.720120: step 2230, loss 0.239477, acc 0.890625\n",
      "2017-04-03T22:16:58.913859: step 2231, loss 0.145016, acc 0.953125\n",
      "2017-04-03T22:16:59.106061: step 2232, loss 0.213548, acc 0.953125\n",
      "2017-04-03T22:16:59.298667: step 2233, loss 0.181974, acc 0.921875\n",
      "2017-04-03T22:16:59.489483: step 2234, loss 0.316284, acc 0.921875\n",
      "2017-04-03T22:16:59.678439: step 2235, loss 0.214689, acc 0.953125\n",
      "2017-04-03T22:16:59.868600: step 2236, loss 0.210822, acc 0.9375\n",
      "2017-04-03T22:17:00.065182: step 2237, loss 0.11323, acc 1\n",
      "2017-04-03T22:17:00.257272: step 2238, loss 0.202731, acc 0.9375\n",
      "2017-04-03T22:17:00.451027: step 2239, loss 0.157149, acc 0.984375\n",
      "2017-04-03T22:17:00.644452: step 2240, loss 0.145911, acc 0.984375\n",
      "2017-04-03T22:17:00.834352: step 2241, loss 0.160603, acc 0.953125\n",
      "2017-04-03T22:17:01.028863: step 2242, loss 0.222025, acc 0.953125\n",
      "2017-04-03T22:17:01.219443: step 2243, loss 0.300163, acc 0.90625\n",
      "2017-04-03T22:17:01.410940: step 2244, loss 0.191281, acc 0.953125\n",
      "2017-04-03T22:17:01.605833: step 2245, loss 0.243102, acc 0.90625\n",
      "2017-04-03T22:17:01.798273: step 2246, loss 0.220232, acc 0.90625\n",
      "2017-04-03T22:17:01.991751: step 2247, loss 0.18527, acc 0.9375\n",
      "2017-04-03T22:17:02.182159: step 2248, loss 0.224022, acc 0.90625\n",
      "2017-04-03T22:17:02.376581: step 2249, loss 0.160748, acc 0.96875\n",
      "2017-04-03T22:17:02.568333: step 2250, loss 0.147756, acc 0.96875\n",
      "2017-04-03T22:17:02.767901: step 2251, loss 0.149898, acc 0.953125\n",
      "2017-04-03T22:17:02.966499: step 2252, loss 0.266306, acc 0.90625\n",
      "2017-04-03T22:17:03.158539: step 2253, loss 0.233781, acc 0.90625\n",
      "2017-04-03T22:17:03.352488: step 2254, loss 0.140453, acc 0.984375\n",
      "2017-04-03T22:17:03.546303: step 2255, loss 0.170726, acc 0.96875\n",
      "2017-04-03T22:17:03.739715: step 2256, loss 0.119103, acc 1\n",
      "2017-04-03T22:17:03.933184: step 2257, loss 0.341363, acc 0.890625\n",
      "2017-04-03T22:17:04.130320: step 2258, loss 0.194792, acc 0.9375\n",
      "2017-04-03T22:17:04.321552: step 2259, loss 0.132133, acc 0.984375\n",
      "2017-04-03T22:17:04.518674: step 2260, loss 0.235092, acc 0.921875\n",
      "2017-04-03T22:17:04.720248: step 2261, loss 0.113436, acc 0.96875\n",
      "2017-04-03T22:17:04.913186: step 2262, loss 0.0710811, acc 0.984375\n",
      "2017-04-03T22:17:05.104245: step 2263, loss 0.182973, acc 0.953125\n",
      "2017-04-03T22:17:05.293787: step 2264, loss 0.311784, acc 0.859375\n",
      "2017-04-03T22:17:05.484702: step 2265, loss 0.266303, acc 0.890625\n",
      "2017-04-03T22:17:05.673957: step 2266, loss 0.239364, acc 0.90625\n",
      "2017-04-03T22:17:05.865684: step 2267, loss 0.122128, acc 0.984375\n",
      "2017-04-03T22:17:06.027583: step 2268, loss 0.288836, acc 0.923077\n",
      "2017-04-03T22:17:06.219257: step 2269, loss 0.137415, acc 0.953125\n",
      "2017-04-03T22:17:06.413187: step 2270, loss 0.112087, acc 1\n",
      "2017-04-03T22:17:06.609771: step 2271, loss 0.111169, acc 0.96875\n",
      "2017-04-03T22:17:06.800404: step 2272, loss 0.115675, acc 0.96875\n",
      "2017-04-03T22:17:06.991141: step 2273, loss 0.11938, acc 0.96875\n",
      "2017-04-03T22:17:07.182402: step 2274, loss 0.154118, acc 0.96875\n",
      "2017-04-03T22:17:07.372931: step 2275, loss 0.128697, acc 0.984375\n",
      "2017-04-03T22:17:07.566265: step 2276, loss 0.148321, acc 0.96875\n",
      "2017-04-03T22:17:07.757274: step 2277, loss 0.192273, acc 0.953125\n",
      "2017-04-03T22:17:07.950572: step 2278, loss 0.0760479, acc 0.984375\n",
      "2017-04-03T22:17:08.146132: step 2279, loss 0.123719, acc 0.96875\n",
      "2017-04-03T22:17:08.339203: step 2280, loss 0.198489, acc 0.96875\n",
      "2017-04-03T22:17:08.531639: step 2281, loss 0.149627, acc 0.96875\n",
      "2017-04-03T22:17:08.726767: step 2282, loss 0.147459, acc 0.96875\n",
      "2017-04-03T22:17:08.921078: step 2283, loss 0.211188, acc 0.96875\n",
      "2017-04-03T22:17:09.116490: step 2284, loss 0.217367, acc 0.953125\n",
      "2017-04-03T22:17:09.307391: step 2285, loss 0.19046, acc 0.9375\n",
      "2017-04-03T22:17:09.500786: step 2286, loss 0.110082, acc 0.953125\n",
      "2017-04-03T22:17:09.693558: step 2287, loss 0.220119, acc 0.9375\n",
      "2017-04-03T22:17:09.883273: step 2288, loss 0.145438, acc 0.96875\n",
      "2017-04-03T22:17:10.073999: step 2289, loss 0.0537259, acc 1\n",
      "2017-04-03T22:17:10.270312: step 2290, loss 0.200085, acc 0.921875\n",
      "2017-04-03T22:17:10.463945: step 2291, loss 0.202306, acc 0.9375\n",
      "2017-04-03T22:17:10.654745: step 2292, loss 0.150253, acc 0.953125\n",
      "2017-04-03T22:17:10.852429: step 2293, loss 0.245035, acc 0.90625\n",
      "2017-04-03T22:17:11.044694: step 2294, loss 0.138981, acc 0.96875\n",
      "2017-04-03T22:17:11.234309: step 2295, loss 0.163641, acc 0.96875\n",
      "2017-04-03T22:17:11.428020: step 2296, loss 0.181962, acc 0.953125\n",
      "2017-04-03T22:17:11.619098: step 2297, loss 0.173528, acc 0.953125\n",
      "2017-04-03T22:17:11.812592: step 2298, loss 0.169226, acc 0.9375\n",
      "2017-04-03T22:17:12.005982: step 2299, loss 0.149389, acc 0.953125\n",
      "2017-04-03T22:17:12.196630: step 2300, loss 0.246386, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:17:12.868173: step 2300, loss 1.48366, acc 0.562663\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2300\n",
      "\n",
      "2017-04-03T22:17:13.121961: step 2301, loss 0.152159, acc 0.953125\n",
      "2017-04-03T22:17:13.313675: step 2302, loss 0.186494, acc 0.921875\n",
      "2017-04-03T22:17:13.507371: step 2303, loss 0.12028, acc 0.96875\n",
      "2017-04-03T22:17:13.697766: step 2304, loss 0.18498, acc 0.96875\n",
      "2017-04-03T22:17:13.890111: step 2305, loss 0.156918, acc 0.953125\n",
      "2017-04-03T22:17:14.082241: step 2306, loss 0.229988, acc 0.953125\n",
      "2017-04-03T22:17:14.275376: step 2307, loss 0.178172, acc 0.953125\n",
      "2017-04-03T22:17:14.465262: step 2308, loss 0.145454, acc 0.984375\n",
      "2017-04-03T22:17:14.656993: step 2309, loss 0.121755, acc 1\n",
      "2017-04-03T22:17:14.848382: step 2310, loss 0.117346, acc 0.984375\n",
      "2017-04-03T22:17:15.041615: step 2311, loss 0.148455, acc 0.984375\n",
      "2017-04-03T22:17:15.235638: step 2312, loss 0.189427, acc 0.953125\n",
      "2017-04-03T22:17:15.426590: step 2313, loss 0.227144, acc 0.921875\n",
      "2017-04-03T22:17:15.620630: step 2314, loss 0.0983129, acc 0.984375\n",
      "2017-04-03T22:17:15.814833: step 2315, loss 0.193952, acc 0.90625\n",
      "2017-04-03T22:17:16.007178: step 2316, loss 0.167579, acc 0.96875\n",
      "2017-04-03T22:17:16.201195: step 2317, loss 0.12535, acc 0.984375\n",
      "2017-04-03T22:17:16.393173: step 2318, loss 0.162778, acc 0.953125\n",
      "2017-04-03T22:17:16.586943: step 2319, loss 0.150143, acc 0.96875\n",
      "2017-04-03T22:17:16.780895: step 2320, loss 0.201133, acc 0.953125\n",
      "2017-04-03T22:17:16.975888: step 2321, loss 0.325423, acc 0.921875\n",
      "2017-04-03T22:17:17.168983: step 2322, loss 0.249396, acc 0.921875\n",
      "2017-04-03T22:17:17.360853: step 2323, loss 0.2776, acc 0.90625\n",
      "2017-04-03T22:17:17.552720: step 2324, loss 0.136235, acc 0.953125\n",
      "2017-04-03T22:17:17.742040: step 2325, loss 0.24975, acc 0.90625\n",
      "2017-04-03T22:17:17.938053: step 2326, loss 0.202414, acc 0.9375\n",
      "2017-04-03T22:17:18.131691: step 2327, loss 0.144965, acc 0.953125\n",
      "2017-04-03T22:17:18.324446: step 2328, loss 0.210082, acc 0.984375\n",
      "2017-04-03T22:17:18.517335: step 2329, loss 0.136766, acc 0.96875\n",
      "2017-04-03T22:17:18.709338: step 2330, loss 0.111903, acc 0.953125\n",
      "2017-04-03T22:17:18.899106: step 2331, loss 0.172283, acc 0.96875\n",
      "2017-04-03T22:17:19.091625: step 2332, loss 0.112456, acc 0.96875\n",
      "2017-04-03T22:17:19.284090: step 2333, loss 0.193255, acc 0.96875\n",
      "2017-04-03T22:17:19.473895: step 2334, loss 0.138757, acc 0.984375\n",
      "2017-04-03T22:17:19.670869: step 2335, loss 0.1774, acc 0.953125\n",
      "2017-04-03T22:17:19.865933: step 2336, loss 0.196863, acc 0.921875\n",
      "2017-04-03T22:17:20.060871: step 2337, loss 0.12865, acc 0.984375\n",
      "2017-04-03T22:17:20.250614: step 2338, loss 0.0697275, acc 1\n",
      "2017-04-03T22:17:20.440518: step 2339, loss 0.230393, acc 0.9375\n",
      "2017-04-03T22:17:20.632806: step 2340, loss 0.156891, acc 0.96875\n",
      "2017-04-03T22:17:20.828156: step 2341, loss 0.139996, acc 0.984375\n",
      "2017-04-03T22:17:21.023716: step 2342, loss 0.198539, acc 0.921875\n",
      "2017-04-03T22:17:21.215787: step 2343, loss 0.273181, acc 0.890625\n",
      "2017-04-03T22:17:21.407139: step 2344, loss 0.326266, acc 0.890625\n",
      "2017-04-03T22:17:21.604128: step 2345, loss 0.127741, acc 0.96875\n",
      "2017-04-03T22:17:21.796826: step 2346, loss 0.201513, acc 0.9375\n",
      "2017-04-03T22:17:21.988556: step 2347, loss 0.115477, acc 1\n",
      "2017-04-03T22:17:22.179805: step 2348, loss 0.15639, acc 0.953125\n",
      "2017-04-03T22:17:22.373973: step 2349, loss 0.213862, acc 0.9375\n",
      "2017-04-03T22:17:22.567642: step 2350, loss 0.13942, acc 0.96875\n",
      "2017-04-03T22:17:22.761266: step 2351, loss 0.119067, acc 0.984375\n",
      "2017-04-03T22:17:22.947863: step 2352, loss 0.198496, acc 0.9375\n",
      "2017-04-03T22:17:23.139710: step 2353, loss 0.154073, acc 0.96875\n",
      "2017-04-03T22:17:23.331038: step 2354, loss 0.194526, acc 0.96875\n",
      "2017-04-03T22:17:23.528718: step 2355, loss 0.0951588, acc 0.984375\n",
      "2017-04-03T22:17:23.720237: step 2356, loss 0.213197, acc 0.9375\n",
      "2017-04-03T22:17:23.914582: step 2357, loss 0.153136, acc 0.96875\n",
      "2017-04-03T22:17:24.108144: step 2358, loss 0.206112, acc 0.953125\n",
      "2017-04-03T22:17:24.303036: step 2359, loss 0.107392, acc 0.984375\n",
      "2017-04-03T22:17:24.498374: step 2360, loss 0.247603, acc 0.9375\n",
      "2017-04-03T22:17:24.694027: step 2361, loss 0.186716, acc 0.90625\n",
      "2017-04-03T22:17:24.889590: step 2362, loss 0.137234, acc 0.96875\n",
      "2017-04-03T22:17:25.081191: step 2363, loss 0.113355, acc 1\n",
      "2017-04-03T22:17:25.271894: step 2364, loss 0.117464, acc 1\n",
      "2017-04-03T22:17:25.462450: step 2365, loss 0.0600763, acc 1\n",
      "2017-04-03T22:17:25.651531: step 2366, loss 0.1086, acc 0.984375\n",
      "2017-04-03T22:17:25.846938: step 2367, loss 0.347939, acc 0.90625\n",
      "2017-04-03T22:17:26.037822: step 2368, loss 0.174817, acc 0.953125\n",
      "2017-04-03T22:17:26.228794: step 2369, loss 0.117157, acc 0.984375\n",
      "2017-04-03T22:17:26.424840: step 2370, loss 0.0955284, acc 0.984375\n",
      "2017-04-03T22:17:26.616165: step 2371, loss 0.215505, acc 0.953125\n",
      "2017-04-03T22:17:26.811012: step 2372, loss 0.213387, acc 0.9375\n",
      "2017-04-03T22:17:27.003284: step 2373, loss 0.123471, acc 0.984375\n",
      "2017-04-03T22:17:27.194252: step 2374, loss 0.171052, acc 0.953125\n",
      "2017-04-03T22:17:27.391504: step 2375, loss 0.117645, acc 0.984375\n",
      "2017-04-03T22:17:27.552991: step 2376, loss 0.11861, acc 1\n",
      "2017-04-03T22:17:27.749417: step 2377, loss 0.125721, acc 0.984375\n",
      "2017-04-03T22:17:27.940206: step 2378, loss 0.22639, acc 0.953125\n",
      "2017-04-03T22:17:28.131653: step 2379, loss 0.117855, acc 0.953125\n",
      "2017-04-03T22:17:28.322674: step 2380, loss 0.084612, acc 1\n",
      "2017-04-03T22:17:28.513991: step 2381, loss 0.100426, acc 0.984375\n",
      "2017-04-03T22:17:28.707377: step 2382, loss 0.0959956, acc 0.984375\n",
      "2017-04-03T22:17:28.898866: step 2383, loss 0.102844, acc 0.984375\n",
      "2017-04-03T22:17:29.091404: step 2384, loss 0.164348, acc 0.953125\n",
      "2017-04-03T22:17:29.284687: step 2385, loss 0.135612, acc 0.984375\n",
      "2017-04-03T22:17:29.477514: step 2386, loss 0.176325, acc 0.96875\n",
      "2017-04-03T22:17:29.668042: step 2387, loss 0.126299, acc 0.96875\n",
      "2017-04-03T22:17:29.860558: step 2388, loss 0.247491, acc 0.90625\n",
      "2017-04-03T22:17:30.053571: step 2389, loss 0.215912, acc 0.9375\n",
      "2017-04-03T22:17:30.244965: step 2390, loss 0.12543, acc 0.984375\n",
      "2017-04-03T22:17:30.439465: step 2391, loss 0.13228, acc 0.96875\n",
      "2017-04-03T22:17:30.628331: step 2392, loss 0.180704, acc 0.953125\n",
      "2017-04-03T22:17:30.822014: step 2393, loss 0.0799653, acc 0.984375\n",
      "2017-04-03T22:17:31.011379: step 2394, loss 0.15468, acc 0.953125\n",
      "2017-04-03T22:17:31.206432: step 2395, loss 0.149953, acc 0.96875\n",
      "2017-04-03T22:17:31.403535: step 2396, loss 0.117206, acc 0.984375\n",
      "2017-04-03T22:17:31.594099: step 2397, loss 0.110646, acc 1\n",
      "2017-04-03T22:17:31.784034: step 2398, loss 0.0956735, acc 0.984375\n",
      "2017-04-03T22:17:31.974344: step 2399, loss 0.0885746, acc 0.984375\n",
      "2017-04-03T22:17:32.172109: step 2400, loss 0.106864, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:17:32.823739: step 2400, loss 1.5135, acc 0.557441\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2400\n",
      "\n",
      "2017-04-03T22:17:33.076553: step 2401, loss 0.132999, acc 0.953125\n",
      "2017-04-03T22:17:33.269499: step 2402, loss 0.219739, acc 0.90625\n",
      "2017-04-03T22:17:33.463570: step 2403, loss 0.192839, acc 0.9375\n",
      "2017-04-03T22:17:33.655399: step 2404, loss 0.0693501, acc 1\n",
      "2017-04-03T22:17:33.846931: step 2405, loss 0.093995, acc 0.984375\n",
      "2017-04-03T22:17:34.040998: step 2406, loss 0.156125, acc 0.953125\n",
      "2017-04-03T22:17:34.236207: step 2407, loss 0.0888476, acc 0.984375\n",
      "2017-04-03T22:17:34.429967: step 2408, loss 0.17446, acc 0.96875\n",
      "2017-04-03T22:17:34.621127: step 2409, loss 0.194326, acc 0.96875\n",
      "2017-04-03T22:17:34.811965: step 2410, loss 0.188265, acc 0.9375\n",
      "2017-04-03T22:17:34.999547: step 2411, loss 0.207563, acc 0.921875\n",
      "2017-04-03T22:17:35.189005: step 2412, loss 0.253279, acc 0.921875\n",
      "2017-04-03T22:17:35.382855: step 2413, loss 0.230289, acc 0.90625\n",
      "2017-04-03T22:17:35.571473: step 2414, loss 0.245146, acc 0.90625\n",
      "2017-04-03T22:17:35.764135: step 2415, loss 0.130688, acc 0.96875\n",
      "2017-04-03T22:17:35.958634: step 2416, loss 0.173799, acc 0.9375\n",
      "2017-04-03T22:17:36.149149: step 2417, loss 0.156961, acc 0.96875\n",
      "2017-04-03T22:17:36.343497: step 2418, loss 0.176435, acc 0.9375\n",
      "2017-04-03T22:17:36.534611: step 2419, loss 0.145951, acc 0.96875\n",
      "2017-04-03T22:17:36.726227: step 2420, loss 0.147515, acc 0.984375\n",
      "2017-04-03T22:17:36.924965: step 2421, loss 0.19363, acc 0.96875\n",
      "2017-04-03T22:17:37.118248: step 2422, loss 0.191832, acc 0.9375\n",
      "2017-04-03T22:17:37.314354: step 2423, loss 0.102913, acc 0.984375\n",
      "2017-04-03T22:17:37.506560: step 2424, loss 0.15231, acc 0.953125\n",
      "2017-04-03T22:17:37.701394: step 2425, loss 0.231723, acc 0.9375\n",
      "2017-04-03T22:17:37.896816: step 2426, loss 0.0976458, acc 0.984375\n",
      "2017-04-03T22:17:38.092740: step 2427, loss 0.141573, acc 0.96875\n",
      "2017-04-03T22:17:38.280595: step 2428, loss 0.0892781, acc 1\n",
      "2017-04-03T22:17:38.468621: step 2429, loss 0.251551, acc 0.921875\n",
      "2017-04-03T22:17:38.658504: step 2430, loss 0.146056, acc 0.984375\n",
      "2017-04-03T22:17:38.851670: step 2431, loss 0.15897, acc 0.96875\n",
      "2017-04-03T22:17:39.044162: step 2432, loss 0.162871, acc 0.953125\n",
      "2017-04-03T22:17:39.233263: step 2433, loss 0.187026, acc 0.953125\n",
      "2017-04-03T22:17:39.428028: step 2434, loss 0.243309, acc 0.921875\n",
      "2017-04-03T22:17:39.618530: step 2435, loss 0.214843, acc 0.921875\n",
      "2017-04-03T22:17:39.808328: step 2436, loss 0.122898, acc 0.984375\n",
      "2017-04-03T22:17:40.002579: step 2437, loss 0.0970795, acc 1\n",
      "2017-04-03T22:17:40.193382: step 2438, loss 0.174606, acc 0.953125\n",
      "2017-04-03T22:17:40.388252: step 2439, loss 0.0936376, acc 1\n",
      "2017-04-03T22:17:40.579714: step 2440, loss 0.105239, acc 0.96875\n",
      "2017-04-03T22:17:40.771135: step 2441, loss 0.183904, acc 0.96875\n",
      "2017-04-03T22:17:40.967073: step 2442, loss 0.184145, acc 0.96875\n",
      "2017-04-03T22:17:41.158711: step 2443, loss 0.136476, acc 0.96875\n",
      "2017-04-03T22:17:41.351657: step 2444, loss 0.16825, acc 0.9375\n",
      "2017-04-03T22:17:41.547082: step 2445, loss 0.228046, acc 0.90625\n",
      "2017-04-03T22:17:41.741453: step 2446, loss 0.20503, acc 0.9375\n",
      "2017-04-03T22:17:41.932307: step 2447, loss 0.174584, acc 0.9375\n",
      "2017-04-03T22:17:42.124915: step 2448, loss 0.137175, acc 0.96875\n",
      "2017-04-03T22:17:42.316183: step 2449, loss 0.115189, acc 0.984375\n",
      "2017-04-03T22:17:42.511903: step 2450, loss 0.186926, acc 0.9375\n",
      "2017-04-03T22:17:42.704732: step 2451, loss 0.193396, acc 0.9375\n",
      "2017-04-03T22:17:42.895880: step 2452, loss 0.143353, acc 1\n",
      "2017-04-03T22:17:43.086111: step 2453, loss 0.177565, acc 0.90625\n",
      "2017-04-03T22:17:43.280724: step 2454, loss 0.114953, acc 0.984375\n",
      "2017-04-03T22:17:43.473468: step 2455, loss 0.0533112, acc 1\n",
      "2017-04-03T22:17:43.668377: step 2456, loss 0.213446, acc 0.9375\n",
      "2017-04-03T22:17:43.858966: step 2457, loss 0.315102, acc 0.875\n",
      "2017-04-03T22:17:44.049547: step 2458, loss 0.272233, acc 0.90625\n",
      "2017-04-03T22:17:44.241077: step 2459, loss 0.145172, acc 0.984375\n",
      "2017-04-03T22:17:44.431121: step 2460, loss 0.146543, acc 0.96875\n",
      "2017-04-03T22:17:44.623731: step 2461, loss 0.197147, acc 0.953125\n",
      "2017-04-03T22:17:44.818426: step 2462, loss 0.143429, acc 0.953125\n",
      "2017-04-03T22:17:45.014056: step 2463, loss 0.177872, acc 0.953125\n",
      "2017-04-03T22:17:45.205172: step 2464, loss 0.194878, acc 0.953125\n",
      "2017-04-03T22:17:45.396814: step 2465, loss 0.0884379, acc 0.984375\n",
      "2017-04-03T22:17:45.590128: step 2466, loss 0.120408, acc 0.984375\n",
      "2017-04-03T22:17:45.783447: step 2467, loss 0.198964, acc 0.9375\n",
      "2017-04-03T22:17:45.978633: step 2468, loss 0.245129, acc 0.96875\n",
      "2017-04-03T22:17:46.169416: step 2469, loss 0.0755972, acc 0.984375\n",
      "2017-04-03T22:17:46.359007: step 2470, loss 0.161989, acc 0.96875\n",
      "2017-04-03T22:17:46.549919: step 2471, loss 0.173906, acc 0.96875\n",
      "2017-04-03T22:17:46.741227: step 2472, loss 0.092322, acc 1\n",
      "2017-04-03T22:17:46.934932: step 2473, loss 0.126654, acc 0.96875\n",
      "2017-04-03T22:17:47.127860: step 2474, loss 0.269302, acc 0.90625\n",
      "2017-04-03T22:17:47.323953: step 2475, loss 0.199619, acc 0.953125\n",
      "2017-04-03T22:17:47.518010: step 2476, loss 0.0906148, acc 0.984375\n",
      "2017-04-03T22:17:47.709987: step 2477, loss 0.177202, acc 0.953125\n",
      "2017-04-03T22:17:47.905861: step 2478, loss 0.23686, acc 0.953125\n",
      "2017-04-03T22:17:48.101927: step 2479, loss 0.125827, acc 0.953125\n",
      "2017-04-03T22:17:48.293728: step 2480, loss 0.230184, acc 0.921875\n",
      "2017-04-03T22:17:48.488790: step 2481, loss 0.0942052, acc 0.984375\n",
      "2017-04-03T22:17:48.683964: step 2482, loss 0.138197, acc 0.953125\n",
      "2017-04-03T22:17:48.877292: step 2483, loss 0.125821, acc 0.96875\n",
      "2017-04-03T22:17:49.041257: step 2484, loss 0.276991, acc 0.903846\n",
      "2017-04-03T22:17:49.235924: step 2485, loss 0.114223, acc 0.984375\n",
      "2017-04-03T22:17:49.426354: step 2486, loss 0.208234, acc 0.984375\n",
      "2017-04-03T22:17:49.619795: step 2487, loss 0.218347, acc 0.921875\n",
      "2017-04-03T22:17:49.812113: step 2488, loss 0.129908, acc 0.984375\n",
      "2017-04-03T22:17:50.010288: step 2489, loss 0.202972, acc 0.9375\n",
      "2017-04-03T22:17:50.203496: step 2490, loss 0.156098, acc 0.953125\n",
      "2017-04-03T22:17:50.402812: step 2491, loss 0.0884215, acc 0.96875\n",
      "2017-04-03T22:17:50.605309: step 2492, loss 0.0749732, acc 0.984375\n",
      "2017-04-03T22:17:50.807173: step 2493, loss 0.150885, acc 0.953125\n",
      "2017-04-03T22:17:51.009846: step 2494, loss 0.156764, acc 0.9375\n",
      "2017-04-03T22:17:51.206907: step 2495, loss 0.046958, acc 1\n",
      "2017-04-03T22:17:51.408990: step 2496, loss 0.130533, acc 0.96875\n",
      "2017-04-03T22:17:51.620361: step 2497, loss 0.0681739, acc 0.984375\n",
      "2017-04-03T22:17:51.817870: step 2498, loss 0.069622, acc 1\n",
      "2017-04-03T22:17:52.016341: step 2499, loss 0.189491, acc 0.921875\n",
      "2017-04-03T22:17:52.226400: step 2500, loss 0.101058, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:17:52.917168: step 2500, loss 1.515, acc 0.567885\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2500\n",
      "\n",
      "2017-04-03T22:17:53.174122: step 2501, loss 0.123663, acc 0.984375\n",
      "2017-04-03T22:17:53.367511: step 2502, loss 0.114618, acc 0.96875\n",
      "2017-04-03T22:17:53.566040: step 2503, loss 0.172535, acc 0.953125\n",
      "2017-04-03T22:17:53.758218: step 2504, loss 0.147106, acc 0.96875\n",
      "2017-04-03T22:17:53.950117: step 2505, loss 0.103501, acc 0.96875\n",
      "2017-04-03T22:17:54.141955: step 2506, loss 0.0982069, acc 0.96875\n",
      "2017-04-03T22:17:54.334338: step 2507, loss 0.179823, acc 0.921875\n",
      "2017-04-03T22:17:54.530598: step 2508, loss 0.131636, acc 0.953125\n",
      "2017-04-03T22:17:54.722536: step 2509, loss 0.218804, acc 0.953125\n",
      "2017-04-03T22:17:54.913493: step 2510, loss 0.161987, acc 0.96875\n",
      "2017-04-03T22:17:55.104151: step 2511, loss 0.215998, acc 0.9375\n",
      "2017-04-03T22:17:55.299712: step 2512, loss 0.109369, acc 0.984375\n",
      "2017-04-03T22:17:55.494968: step 2513, loss 0.0972331, acc 0.984375\n",
      "2017-04-03T22:17:55.684585: step 2514, loss 0.142389, acc 0.96875\n",
      "2017-04-03T22:17:55.875856: step 2515, loss 0.157573, acc 0.96875\n",
      "2017-04-03T22:17:56.070621: step 2516, loss 0.0709095, acc 0.984375\n",
      "2017-04-03T22:17:56.263006: step 2517, loss 0.149528, acc 0.984375\n",
      "2017-04-03T22:17:56.452557: step 2518, loss 0.140822, acc 0.953125\n",
      "2017-04-03T22:17:56.641791: step 2519, loss 0.161865, acc 0.953125\n",
      "2017-04-03T22:17:56.836495: step 2520, loss 0.0976836, acc 0.984375\n",
      "2017-04-03T22:17:57.032177: step 2521, loss 0.14817, acc 0.96875\n",
      "2017-04-03T22:17:57.224415: step 2522, loss 0.183532, acc 0.921875\n",
      "2017-04-03T22:17:57.420168: step 2523, loss 0.120533, acc 0.953125\n",
      "2017-04-03T22:17:57.611756: step 2524, loss 0.117165, acc 0.96875\n",
      "2017-04-03T22:17:57.806308: step 2525, loss 0.176478, acc 0.9375\n",
      "2017-04-03T22:17:57.997105: step 2526, loss 0.110516, acc 0.984375\n",
      "2017-04-03T22:17:58.192931: step 2527, loss 0.171344, acc 0.96875\n",
      "2017-04-03T22:17:58.383273: step 2528, loss 0.188039, acc 0.9375\n",
      "2017-04-03T22:17:58.576290: step 2529, loss 0.219407, acc 0.921875\n",
      "2017-04-03T22:17:58.770096: step 2530, loss 0.0779384, acc 0.984375\n",
      "2017-04-03T22:17:58.969193: step 2531, loss 0.103876, acc 0.96875\n",
      "2017-04-03T22:17:59.172272: step 2532, loss 0.112533, acc 0.96875\n",
      "2017-04-03T22:17:59.365540: step 2533, loss 0.0975507, acc 0.984375\n",
      "2017-04-03T22:17:59.570490: step 2534, loss 0.249693, acc 0.921875\n",
      "2017-04-03T22:17:59.766870: step 2535, loss 0.116083, acc 0.984375\n",
      "2017-04-03T22:17:59.965219: step 2536, loss 0.182781, acc 0.96875\n",
      "2017-04-03T22:18:00.156487: step 2537, loss 0.107383, acc 0.984375\n",
      "2017-04-03T22:18:00.348623: step 2538, loss 0.119921, acc 0.953125\n",
      "2017-04-03T22:18:00.540465: step 2539, loss 0.0868535, acc 0.984375\n",
      "2017-04-03T22:18:00.734752: step 2540, loss 0.168014, acc 0.9375\n",
      "2017-04-03T22:18:00.927197: step 2541, loss 0.100109, acc 0.96875\n",
      "2017-04-03T22:18:01.123454: step 2542, loss 0.182887, acc 0.953125\n",
      "2017-04-03T22:18:01.316757: step 2543, loss 0.121774, acc 0.96875\n",
      "2017-04-03T22:18:01.508716: step 2544, loss 0.115295, acc 0.96875\n",
      "2017-04-03T22:18:01.698238: step 2545, loss 0.232319, acc 0.921875\n",
      "2017-04-03T22:18:01.890811: step 2546, loss 0.150447, acc 0.96875\n",
      "2017-04-03T22:18:02.082546: step 2547, loss 0.125208, acc 0.96875\n",
      "2017-04-03T22:18:02.276375: step 2548, loss 0.078875, acc 0.984375\n",
      "2017-04-03T22:18:02.470189: step 2549, loss 0.073149, acc 0.984375\n",
      "2017-04-03T22:18:02.660555: step 2550, loss 0.149828, acc 0.984375\n",
      "2017-04-03T22:18:02.854789: step 2551, loss 0.226074, acc 0.90625\n",
      "2017-04-03T22:18:03.048644: step 2552, loss 0.143724, acc 0.953125\n",
      "2017-04-03T22:18:03.239487: step 2553, loss 0.178022, acc 0.96875\n",
      "2017-04-03T22:18:03.430965: step 2554, loss 0.222134, acc 0.921875\n",
      "2017-04-03T22:18:03.623516: step 2555, loss 0.221765, acc 0.953125\n",
      "2017-04-03T22:18:03.815853: step 2556, loss 0.112363, acc 0.984375\n",
      "2017-04-03T22:18:04.011391: step 2557, loss 0.172392, acc 0.90625\n",
      "2017-04-03T22:18:04.204306: step 2558, loss 0.170587, acc 0.921875\n",
      "2017-04-03T22:18:04.400390: step 2559, loss 0.105339, acc 0.96875\n",
      "2017-04-03T22:18:04.596190: step 2560, loss 0.208448, acc 0.9375\n",
      "2017-04-03T22:18:04.790095: step 2561, loss 0.137987, acc 0.96875\n",
      "2017-04-03T22:18:04.984668: step 2562, loss 0.150821, acc 0.953125\n",
      "2017-04-03T22:18:05.182121: step 2563, loss 0.0750418, acc 0.984375\n",
      "2017-04-03T22:18:05.373491: step 2564, loss 0.102876, acc 0.984375\n",
      "2017-04-03T22:18:05.564511: step 2565, loss 0.126338, acc 0.953125\n",
      "2017-04-03T22:18:05.756774: step 2566, loss 0.146511, acc 0.96875\n",
      "2017-04-03T22:18:05.951960: step 2567, loss 0.17648, acc 0.953125\n",
      "2017-04-03T22:18:06.143798: step 2568, loss 0.161814, acc 0.9375\n",
      "2017-04-03T22:18:06.332033: step 2569, loss 0.0771235, acc 0.984375\n",
      "2017-04-03T22:18:06.527495: step 2570, loss 0.0910724, acc 1\n",
      "2017-04-03T22:18:06.714276: step 2571, loss 0.202212, acc 0.953125\n",
      "2017-04-03T22:18:06.909004: step 2572, loss 0.101013, acc 0.984375\n",
      "2017-04-03T22:18:07.100117: step 2573, loss 0.171005, acc 0.921875\n",
      "2017-04-03T22:18:07.293276: step 2574, loss 0.123911, acc 0.96875\n",
      "2017-04-03T22:18:07.487693: step 2575, loss 0.125655, acc 0.984375\n",
      "2017-04-03T22:18:07.680733: step 2576, loss 0.134126, acc 0.96875\n",
      "2017-04-03T22:18:07.873834: step 2577, loss 0.123075, acc 0.96875\n",
      "2017-04-03T22:18:08.065062: step 2578, loss 0.0638744, acc 1\n",
      "2017-04-03T22:18:08.260431: step 2579, loss 0.14741, acc 0.984375\n",
      "2017-04-03T22:18:08.451278: step 2580, loss 0.131647, acc 0.953125\n",
      "2017-04-03T22:18:08.642146: step 2581, loss 0.101748, acc 0.96875\n",
      "2017-04-03T22:18:08.833010: step 2582, loss 0.236097, acc 0.9375\n",
      "2017-04-03T22:18:09.026124: step 2583, loss 0.180097, acc 0.9375\n",
      "2017-04-03T22:18:09.221196: step 2584, loss 0.181329, acc 0.953125\n",
      "2017-04-03T22:18:09.411446: step 2585, loss 0.252587, acc 0.953125\n",
      "2017-04-03T22:18:09.611130: step 2586, loss 0.18981, acc 0.921875\n",
      "2017-04-03T22:18:09.804945: step 2587, loss 0.173442, acc 0.953125\n",
      "2017-04-03T22:18:09.999204: step 2588, loss 0.132259, acc 0.96875\n",
      "2017-04-03T22:18:10.192618: step 2589, loss 0.324637, acc 0.890625\n",
      "2017-04-03T22:18:10.384031: step 2590, loss 0.185791, acc 0.953125\n",
      "2017-04-03T22:18:10.581607: step 2591, loss 0.229723, acc 0.90625\n",
      "2017-04-03T22:18:10.743944: step 2592, loss 0.211853, acc 0.903846\n",
      "2017-04-03T22:18:10.939697: step 2593, loss 0.201623, acc 0.9375\n",
      "2017-04-03T22:18:11.133478: step 2594, loss 0.122101, acc 0.953125\n",
      "2017-04-03T22:18:11.340937: step 2595, loss 0.0725825, acc 1\n",
      "2017-04-03T22:18:11.570359: step 2596, loss 0.0840943, acc 0.984375\n",
      "2017-04-03T22:18:11.772999: step 2597, loss 0.361645, acc 0.875\n",
      "2017-04-03T22:18:11.986585: step 2598, loss 0.133024, acc 0.96875\n",
      "2017-04-03T22:18:12.188709: step 2599, loss 0.0826344, acc 0.96875\n",
      "2017-04-03T22:18:12.394232: step 2600, loss 0.156257, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:18:13.060697: step 2600, loss 1.53757, acc 0.563969\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2600\n",
      "\n",
      "2017-04-03T22:18:13.333369: step 2601, loss 0.123311, acc 0.984375\n",
      "2017-04-03T22:18:13.538251: step 2602, loss 0.126073, acc 0.984375\n",
      "2017-04-03T22:18:13.744563: step 2603, loss 0.143304, acc 0.984375\n",
      "2017-04-03T22:18:13.952449: step 2604, loss 0.0778438, acc 1\n",
      "2017-04-03T22:18:14.157190: step 2605, loss 0.153072, acc 0.96875\n",
      "2017-04-03T22:18:14.350019: step 2606, loss 0.160865, acc 0.96875\n",
      "2017-04-03T22:18:14.548997: step 2607, loss 0.0766997, acc 1\n",
      "2017-04-03T22:18:14.744880: step 2608, loss 0.0661288, acc 1\n",
      "2017-04-03T22:18:14.938996: step 2609, loss 0.208699, acc 0.96875\n",
      "2017-04-03T22:18:15.126970: step 2610, loss 0.106059, acc 0.984375\n",
      "2017-04-03T22:18:15.321609: step 2611, loss 0.0792962, acc 0.984375\n",
      "2017-04-03T22:18:15.524810: step 2612, loss 0.126854, acc 0.953125\n",
      "2017-04-03T22:18:15.718343: step 2613, loss 0.232385, acc 0.96875\n",
      "2017-04-03T22:18:15.911993: step 2614, loss 0.0951888, acc 0.984375\n",
      "2017-04-03T22:18:16.104792: step 2615, loss 0.0944702, acc 0.984375\n",
      "2017-04-03T22:18:16.296040: step 2616, loss 0.221184, acc 0.96875\n",
      "2017-04-03T22:18:16.484174: step 2617, loss 0.094289, acc 1\n",
      "2017-04-03T22:18:16.676449: step 2618, loss 0.080712, acc 0.984375\n",
      "2017-04-03T22:18:16.869761: step 2619, loss 0.198502, acc 0.921875\n",
      "2017-04-03T22:18:17.067071: step 2620, loss 0.087254, acc 0.96875\n",
      "2017-04-03T22:18:17.264053: step 2621, loss 0.10974, acc 0.96875\n",
      "2017-04-03T22:18:17.461739: step 2622, loss 0.153623, acc 0.96875\n",
      "2017-04-03T22:18:17.650554: step 2623, loss 0.134456, acc 0.953125\n",
      "2017-04-03T22:18:17.843771: step 2624, loss 0.112073, acc 0.984375\n",
      "2017-04-03T22:18:18.032809: step 2625, loss 0.128896, acc 0.953125\n",
      "2017-04-03T22:18:18.227209: step 2626, loss 0.100808, acc 0.984375\n",
      "2017-04-03T22:18:18.418639: step 2627, loss 0.080533, acc 0.984375\n",
      "2017-04-03T22:18:18.612374: step 2628, loss 0.214663, acc 0.90625\n",
      "2017-04-03T22:18:18.804753: step 2629, loss 0.0654354, acc 1\n",
      "2017-04-03T22:18:18.995312: step 2630, loss 0.0706705, acc 1\n",
      "2017-04-03T22:18:19.184875: step 2631, loss 0.138963, acc 0.953125\n",
      "2017-04-03T22:18:19.379005: step 2632, loss 0.0252733, acc 1\n",
      "2017-04-03T22:18:19.573413: step 2633, loss 0.107193, acc 0.953125\n",
      "2017-04-03T22:18:19.767194: step 2634, loss 0.148716, acc 0.9375\n",
      "2017-04-03T22:18:19.961507: step 2635, loss 0.0872437, acc 1\n",
      "2017-04-03T22:18:20.154961: step 2636, loss 0.146952, acc 0.953125\n",
      "2017-04-03T22:18:20.344704: step 2637, loss 0.118602, acc 0.984375\n",
      "2017-04-03T22:18:20.536583: step 2638, loss 0.0996104, acc 0.984375\n",
      "2017-04-03T22:18:20.726774: step 2639, loss 0.0841765, acc 1\n",
      "2017-04-03T22:18:20.921365: step 2640, loss 0.117559, acc 0.984375\n",
      "2017-04-03T22:18:21.115040: step 2641, loss 0.0671492, acc 1\n",
      "2017-04-03T22:18:21.310078: step 2642, loss 0.0786076, acc 1\n",
      "2017-04-03T22:18:21.498523: step 2643, loss 0.199034, acc 0.9375\n",
      "2017-04-03T22:18:21.690535: step 2644, loss 0.133834, acc 0.96875\n",
      "2017-04-03T22:18:21.883446: step 2645, loss 0.170317, acc 0.953125\n",
      "2017-04-03T22:18:22.073194: step 2646, loss 0.0821254, acc 0.984375\n",
      "2017-04-03T22:18:22.265452: step 2647, loss 0.093853, acc 0.984375\n",
      "2017-04-03T22:18:22.460634: step 2648, loss 0.11047, acc 0.984375\n",
      "2017-04-03T22:18:22.653561: step 2649, loss 0.0694129, acc 0.96875\n",
      "2017-04-03T22:18:22.847981: step 2650, loss 0.209075, acc 0.921875\n",
      "2017-04-03T22:18:23.035136: step 2651, loss 0.163581, acc 0.953125\n",
      "2017-04-03T22:18:23.227529: step 2652, loss 0.110668, acc 0.984375\n",
      "2017-04-03T22:18:23.419009: step 2653, loss 0.157625, acc 0.9375\n",
      "2017-04-03T22:18:23.618316: step 2654, loss 0.130907, acc 0.953125\n",
      "2017-04-03T22:18:23.811569: step 2655, loss 0.132114, acc 0.9375\n",
      "2017-04-03T22:18:23.999927: step 2656, loss 0.076071, acc 1\n",
      "2017-04-03T22:18:24.188440: step 2657, loss 0.111911, acc 0.96875\n",
      "2017-04-03T22:18:24.384620: step 2658, loss 0.1703, acc 0.9375\n",
      "2017-04-03T22:18:24.581257: step 2659, loss 0.128603, acc 0.96875\n",
      "2017-04-03T22:18:24.772231: step 2660, loss 0.0954336, acc 0.96875\n",
      "2017-04-03T22:18:24.963352: step 2661, loss 0.168384, acc 0.9375\n",
      "2017-04-03T22:18:25.155967: step 2662, loss 0.0794077, acc 1\n",
      "2017-04-03T22:18:25.345091: step 2663, loss 0.113601, acc 0.984375\n",
      "2017-04-03T22:18:25.543318: step 2664, loss 0.223364, acc 0.90625\n",
      "2017-04-03T22:18:25.736140: step 2665, loss 0.08501, acc 0.984375\n",
      "2017-04-03T22:18:25.929210: step 2666, loss 0.20492, acc 0.921875\n",
      "2017-04-03T22:18:26.120917: step 2667, loss 0.176413, acc 0.9375\n",
      "2017-04-03T22:18:26.314980: step 2668, loss 0.171523, acc 0.953125\n",
      "2017-04-03T22:18:26.504752: step 2669, loss 0.133193, acc 0.96875\n",
      "2017-04-03T22:18:26.698808: step 2670, loss 0.147903, acc 0.96875\n",
      "2017-04-03T22:18:26.886945: step 2671, loss 0.148833, acc 0.953125\n",
      "2017-04-03T22:18:27.084492: step 2672, loss 0.129758, acc 0.96875\n",
      "2017-04-03T22:18:27.277482: step 2673, loss 0.0832145, acc 0.96875\n",
      "2017-04-03T22:18:27.469575: step 2674, loss 0.224027, acc 0.890625\n",
      "2017-04-03T22:18:27.662169: step 2675, loss 0.122964, acc 0.96875\n",
      "2017-04-03T22:18:27.854329: step 2676, loss 0.0958759, acc 0.96875\n",
      "2017-04-03T22:18:28.047273: step 2677, loss 0.0902859, acc 0.984375\n",
      "2017-04-03T22:18:28.240692: step 2678, loss 0.0738827, acc 0.984375\n",
      "2017-04-03T22:18:28.435435: step 2679, loss 0.144643, acc 0.953125\n",
      "2017-04-03T22:18:28.625812: step 2680, loss 0.127344, acc 0.96875\n",
      "2017-04-03T22:18:28.816637: step 2681, loss 0.165984, acc 0.984375\n",
      "2017-04-03T22:18:29.012559: step 2682, loss 0.0922766, acc 0.984375\n",
      "2017-04-03T22:18:29.205234: step 2683, loss 0.128787, acc 0.984375\n",
      "2017-04-03T22:18:29.400091: step 2684, loss 0.0783003, acc 0.984375\n",
      "2017-04-03T22:18:29.591552: step 2685, loss 0.243134, acc 0.921875\n",
      "2017-04-03T22:18:29.784549: step 2686, loss 0.144452, acc 0.953125\n",
      "2017-04-03T22:18:29.974306: step 2687, loss 0.0991317, acc 0.984375\n",
      "2017-04-03T22:18:30.168257: step 2688, loss 0.148413, acc 0.96875\n",
      "2017-04-03T22:18:30.366321: step 2689, loss 0.113843, acc 0.96875\n",
      "2017-04-03T22:18:30.561244: step 2690, loss 0.213858, acc 0.9375\n",
      "2017-04-03T22:18:30.755475: step 2691, loss 0.182648, acc 0.96875\n",
      "2017-04-03T22:18:30.950524: step 2692, loss 0.161521, acc 0.953125\n",
      "2017-04-03T22:18:31.139569: step 2693, loss 0.0720556, acc 0.984375\n",
      "2017-04-03T22:18:31.328746: step 2694, loss 0.20373, acc 0.9375\n",
      "2017-04-03T22:18:31.519796: step 2695, loss 0.152947, acc 0.953125\n",
      "2017-04-03T22:18:31.712413: step 2696, loss 0.154265, acc 0.953125\n",
      "2017-04-03T22:18:31.904533: step 2697, loss 0.0883218, acc 0.984375\n",
      "2017-04-03T22:18:32.099821: step 2698, loss 0.0767042, acc 1\n",
      "2017-04-03T22:18:32.292465: step 2699, loss 0.199353, acc 0.90625\n",
      "2017-04-03T22:18:32.457878: step 2700, loss 0.133618, acc 0.942308\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:18:33.106717: step 2700, loss 1.56816, acc 0.567885\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2700\n",
      "\n",
      "2017-04-03T22:18:33.358054: step 2701, loss 0.106909, acc 0.96875\n",
      "2017-04-03T22:18:33.557286: step 2702, loss 0.0672662, acc 1\n",
      "2017-04-03T22:18:33.754044: step 2703, loss 0.138455, acc 0.96875\n",
      "2017-04-03T22:18:33.945805: step 2704, loss 0.160231, acc 0.9375\n",
      "2017-04-03T22:18:34.135488: step 2705, loss 0.12976, acc 0.96875\n",
      "2017-04-03T22:18:34.328852: step 2706, loss 0.0650363, acc 1\n",
      "2017-04-03T22:18:34.520341: step 2707, loss 0.0511387, acc 1\n",
      "2017-04-03T22:18:34.708698: step 2708, loss 0.139761, acc 0.96875\n",
      "2017-04-03T22:18:34.901119: step 2709, loss 0.0832913, acc 0.984375\n",
      "2017-04-03T22:18:35.092593: step 2710, loss 0.14428, acc 0.953125\n",
      "2017-04-03T22:18:35.285901: step 2711, loss 0.101097, acc 1\n",
      "2017-04-03T22:18:35.481287: step 2712, loss 0.10223, acc 0.984375\n",
      "2017-04-03T22:18:35.673777: step 2713, loss 0.118675, acc 0.96875\n",
      "2017-04-03T22:18:35.870979: step 2714, loss 0.110092, acc 0.984375\n",
      "2017-04-03T22:18:36.067471: step 2715, loss 0.116093, acc 0.984375\n",
      "2017-04-03T22:18:36.263220: step 2716, loss 0.129601, acc 0.984375\n",
      "2017-04-03T22:18:36.457150: step 2717, loss 0.095605, acc 0.96875\n",
      "2017-04-03T22:18:36.648879: step 2718, loss 0.132539, acc 0.96875\n",
      "2017-04-03T22:18:36.840367: step 2719, loss 0.196808, acc 0.921875\n",
      "2017-04-03T22:18:37.037153: step 2720, loss 0.234754, acc 0.953125\n",
      "2017-04-03T22:18:37.229661: step 2721, loss 0.147109, acc 0.96875\n",
      "2017-04-03T22:18:37.422655: step 2722, loss 0.175495, acc 0.953125\n",
      "2017-04-03T22:18:37.615116: step 2723, loss 0.0637519, acc 1\n",
      "2017-04-03T22:18:37.808228: step 2724, loss 0.115922, acc 0.96875\n",
      "2017-04-03T22:18:38.003606: step 2725, loss 0.139846, acc 0.953125\n",
      "2017-04-03T22:18:38.195369: step 2726, loss 0.102254, acc 0.984375\n",
      "2017-04-03T22:18:38.387166: step 2727, loss 0.322072, acc 0.859375\n",
      "2017-04-03T22:18:38.575765: step 2728, loss 0.100573, acc 0.984375\n",
      "2017-04-03T22:18:38.771381: step 2729, loss 0.0608845, acc 1\n",
      "2017-04-03T22:18:38.962775: step 2730, loss 0.119325, acc 0.9375\n",
      "2017-04-03T22:18:39.156151: step 2731, loss 0.0749287, acc 0.984375\n",
      "2017-04-03T22:18:39.346117: step 2732, loss 0.107908, acc 0.96875\n",
      "2017-04-03T22:18:39.548137: step 2733, loss 0.0689645, acc 1\n",
      "2017-04-03T22:18:39.742453: step 2734, loss 0.100816, acc 0.984375\n",
      "2017-04-03T22:18:39.938219: step 2735, loss 0.155705, acc 0.953125\n",
      "2017-04-03T22:18:40.135825: step 2736, loss 0.124239, acc 0.953125\n",
      "2017-04-03T22:18:40.330683: step 2737, loss 0.0915887, acc 0.984375\n",
      "2017-04-03T22:18:40.526859: step 2738, loss 0.116493, acc 0.96875\n",
      "2017-04-03T22:18:40.720017: step 2739, loss 0.0609364, acc 0.984375\n",
      "2017-04-03T22:18:40.913087: step 2740, loss 0.12454, acc 0.984375\n",
      "2017-04-03T22:18:41.112308: step 2741, loss 0.108789, acc 0.96875\n",
      "2017-04-03T22:18:41.302198: step 2742, loss 0.112097, acc 0.96875\n",
      "2017-04-03T22:18:41.495584: step 2743, loss 0.133013, acc 0.984375\n",
      "2017-04-03T22:18:41.686098: step 2744, loss 0.117803, acc 0.984375\n",
      "2017-04-03T22:18:41.883583: step 2745, loss 0.129651, acc 0.953125\n",
      "2017-04-03T22:18:42.077690: step 2746, loss 0.0504, acc 1\n",
      "2017-04-03T22:18:42.267385: step 2747, loss 0.0730624, acc 1\n",
      "2017-04-03T22:18:42.456559: step 2748, loss 0.214574, acc 0.9375\n",
      "2017-04-03T22:18:42.649193: step 2749, loss 0.05775, acc 1\n",
      "2017-04-03T22:18:42.844003: step 2750, loss 0.201919, acc 0.953125\n",
      "2017-04-03T22:18:43.037868: step 2751, loss 0.116687, acc 0.984375\n",
      "2017-04-03T22:18:43.231816: step 2752, loss 0.211745, acc 0.921875\n",
      "2017-04-03T22:18:43.421177: step 2753, loss 0.163027, acc 0.953125\n",
      "2017-04-03T22:18:43.612003: step 2754, loss 0.107947, acc 0.96875\n",
      "2017-04-03T22:18:43.805446: step 2755, loss 0.110267, acc 1\n",
      "2017-04-03T22:18:44.002703: step 2756, loss 0.0930982, acc 0.984375\n",
      "2017-04-03T22:18:44.198407: step 2757, loss 0.106705, acc 0.984375\n",
      "2017-04-03T22:18:44.394582: step 2758, loss 0.136234, acc 0.96875\n",
      "2017-04-03T22:18:44.586006: step 2759, loss 0.18411, acc 0.953125\n",
      "2017-04-03T22:18:44.776006: step 2760, loss 0.117578, acc 0.953125\n",
      "2017-04-03T22:18:44.967326: step 2761, loss 0.0855309, acc 0.984375\n",
      "2017-04-03T22:18:45.157671: step 2762, loss 0.100223, acc 0.96875\n",
      "2017-04-03T22:18:45.350309: step 2763, loss 0.106875, acc 0.984375\n",
      "2017-04-03T22:18:45.542576: step 2764, loss 0.143947, acc 0.96875\n",
      "2017-04-03T22:18:45.737126: step 2765, loss 0.183454, acc 0.921875\n",
      "2017-04-03T22:18:45.923718: step 2766, loss 0.19157, acc 0.953125\n",
      "2017-04-03T22:18:46.117680: step 2767, loss 0.0938628, acc 0.96875\n",
      "2017-04-03T22:18:46.306772: step 2768, loss 0.0694703, acc 1\n",
      "2017-04-03T22:18:46.497372: step 2769, loss 0.0933324, acc 0.984375\n",
      "2017-04-03T22:18:46.689575: step 2770, loss 0.110656, acc 0.96875\n",
      "2017-04-03T22:18:46.885427: step 2771, loss 0.125883, acc 0.96875\n",
      "2017-04-03T22:18:47.082954: step 2772, loss 0.230399, acc 0.90625\n",
      "2017-04-03T22:18:47.276130: step 2773, loss 0.147693, acc 0.96875\n",
      "2017-04-03T22:18:47.469610: step 2774, loss 0.162774, acc 0.96875\n",
      "2017-04-03T22:18:47.660906: step 2775, loss 0.0883407, acc 0.96875\n",
      "2017-04-03T22:18:47.853394: step 2776, loss 0.147713, acc 0.953125\n",
      "2017-04-03T22:18:48.050559: step 2777, loss 0.13644, acc 0.984375\n",
      "2017-04-03T22:18:48.244221: step 2778, loss 0.0970059, acc 0.984375\n",
      "2017-04-03T22:18:48.440250: step 2779, loss 0.0627314, acc 0.984375\n",
      "2017-04-03T22:18:48.634200: step 2780, loss 0.126137, acc 0.953125\n",
      "2017-04-03T22:18:48.828674: step 2781, loss 0.0686581, acc 0.984375\n",
      "2017-04-03T22:18:49.016153: step 2782, loss 0.0565448, acc 0.984375\n",
      "2017-04-03T22:18:49.208284: step 2783, loss 0.0970922, acc 1\n",
      "2017-04-03T22:18:49.400480: step 2784, loss 0.122594, acc 0.96875\n",
      "2017-04-03T22:18:49.590745: step 2785, loss 0.157812, acc 0.96875\n",
      "2017-04-03T22:18:49.783702: step 2786, loss 0.14293, acc 0.9375\n",
      "2017-04-03T22:18:49.972826: step 2787, loss 0.101442, acc 0.984375\n",
      "2017-04-03T22:18:50.166077: step 2788, loss 0.0728604, acc 1\n",
      "2017-04-03T22:18:50.358158: step 2789, loss 0.158801, acc 0.9375\n",
      "2017-04-03T22:18:50.551425: step 2790, loss 0.0502345, acc 1\n",
      "2017-04-03T22:18:50.748178: step 2791, loss 0.0480675, acc 1\n",
      "2017-04-03T22:18:50.941133: step 2792, loss 0.165503, acc 0.953125\n",
      "2017-04-03T22:18:51.134653: step 2793, loss 0.149911, acc 0.953125\n",
      "2017-04-03T22:18:51.326280: step 2794, loss 0.182801, acc 0.953125\n",
      "2017-04-03T22:18:51.516172: step 2795, loss 0.0952363, acc 0.96875\n",
      "2017-04-03T22:18:51.717745: step 2796, loss 0.115487, acc 0.953125\n",
      "2017-04-03T22:18:51.907694: step 2797, loss 0.140484, acc 0.96875\n",
      "2017-04-03T22:18:52.098537: step 2798, loss 0.196316, acc 0.9375\n",
      "2017-04-03T22:18:52.292514: step 2799, loss 0.13656, acc 0.953125\n",
      "2017-04-03T22:18:52.485446: step 2800, loss 0.0955112, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:18:53.138796: step 2800, loss 1.56462, acc 0.563969\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2800\n",
      "\n",
      "2017-04-03T22:18:53.389166: step 2801, loss 0.260587, acc 0.9375\n",
      "2017-04-03T22:18:53.579426: step 2802, loss 0.134508, acc 0.9375\n",
      "2017-04-03T22:18:53.769455: step 2803, loss 0.0712967, acc 0.984375\n",
      "2017-04-03T22:18:53.963445: step 2804, loss 0.150971, acc 0.953125\n",
      "2017-04-03T22:18:54.153124: step 2805, loss 0.128227, acc 0.953125\n",
      "2017-04-03T22:18:54.345203: step 2806, loss 0.18187, acc 0.96875\n",
      "2017-04-03T22:18:54.537467: step 2807, loss 0.051921, acc 1\n",
      "2017-04-03T22:18:54.699195: step 2808, loss 0.202124, acc 0.942308\n",
      "2017-04-03T22:18:54.893925: step 2809, loss 0.0608784, acc 1\n",
      "2017-04-03T22:18:55.087007: step 2810, loss 0.106202, acc 0.96875\n",
      "2017-04-03T22:18:55.278327: step 2811, loss 0.121796, acc 0.96875\n",
      "2017-04-03T22:18:55.471004: step 2812, loss 0.126765, acc 0.9375\n",
      "2017-04-03T22:18:55.664756: step 2813, loss 0.148887, acc 0.921875\n",
      "2017-04-03T22:18:55.857822: step 2814, loss 0.119874, acc 0.953125\n",
      "2017-04-03T22:18:56.057906: step 2815, loss 0.131074, acc 0.9375\n",
      "2017-04-03T22:18:56.248769: step 2816, loss 0.0633415, acc 0.984375\n",
      "2017-04-03T22:18:56.442146: step 2817, loss 0.170634, acc 0.953125\n",
      "2017-04-03T22:18:56.633265: step 2818, loss 0.0422235, acc 1\n",
      "2017-04-03T22:18:56.824590: step 2819, loss 0.119054, acc 0.984375\n",
      "2017-04-03T22:18:57.013883: step 2820, loss 0.10861, acc 0.984375\n",
      "2017-04-03T22:18:57.205172: step 2821, loss 0.0880081, acc 0.984375\n",
      "2017-04-03T22:18:57.395081: step 2822, loss 0.180912, acc 0.96875\n",
      "2017-04-03T22:18:57.589614: step 2823, loss 0.0801949, acc 0.96875\n",
      "2017-04-03T22:18:57.783214: step 2824, loss 0.114636, acc 0.96875\n",
      "2017-04-03T22:18:57.976919: step 2825, loss 0.0579886, acc 1\n",
      "2017-04-03T22:18:58.171608: step 2826, loss 0.0881788, acc 0.984375\n",
      "2017-04-03T22:18:58.363634: step 2827, loss 0.0888849, acc 1\n",
      "2017-04-03T22:18:58.557142: step 2828, loss 0.0539824, acc 0.984375\n",
      "2017-04-03T22:18:58.754619: step 2829, loss 0.168406, acc 0.9375\n",
      "2017-04-03T22:18:58.949477: step 2830, loss 0.144815, acc 0.953125\n",
      "2017-04-03T22:18:59.143318: step 2831, loss 0.13662, acc 0.96875\n",
      "2017-04-03T22:18:59.337793: step 2832, loss 0.238699, acc 0.90625\n",
      "2017-04-03T22:18:59.532096: step 2833, loss 0.156561, acc 0.953125\n",
      "2017-04-03T22:18:59.721643: step 2834, loss 0.0687976, acc 0.984375\n",
      "2017-04-03T22:18:59.917440: step 2835, loss 0.0652204, acc 1\n",
      "2017-04-03T22:19:00.117615: step 2836, loss 0.125491, acc 0.9375\n",
      "2017-04-03T22:19:00.316333: step 2837, loss 0.0895137, acc 1\n",
      "2017-04-03T22:19:00.515095: step 2838, loss 0.203951, acc 0.9375\n",
      "2017-04-03T22:19:00.714836: step 2839, loss 0.0440635, acc 1\n",
      "2017-04-03T22:19:00.910714: step 2840, loss 0.13368, acc 0.96875\n",
      "2017-04-03T22:19:01.105921: step 2841, loss 0.0623427, acc 0.96875\n",
      "2017-04-03T22:19:01.301074: step 2842, loss 0.122611, acc 0.9375\n",
      "2017-04-03T22:19:01.509578: step 2843, loss 0.0927257, acc 0.984375\n",
      "2017-04-03T22:19:01.698111: step 2844, loss 0.165302, acc 0.96875\n",
      "2017-04-03T22:19:01.889921: step 2845, loss 0.0949631, acc 0.984375\n",
      "2017-04-03T22:19:02.082264: step 2846, loss 0.193339, acc 0.921875\n",
      "2017-04-03T22:19:02.276182: step 2847, loss 0.157658, acc 0.9375\n",
      "2017-04-03T22:19:02.468172: step 2848, loss 0.0471596, acc 1\n",
      "2017-04-03T22:19:02.667561: step 2849, loss 0.0900918, acc 0.96875\n",
      "2017-04-03T22:19:02.861395: step 2850, loss 0.134476, acc 0.96875\n",
      "2017-04-03T22:19:03.051042: step 2851, loss 0.0305157, acc 1\n",
      "2017-04-03T22:19:03.241958: step 2852, loss 0.172921, acc 0.953125\n",
      "2017-04-03T22:19:03.435699: step 2853, loss 0.151055, acc 0.953125\n",
      "2017-04-03T22:19:03.626850: step 2854, loss 0.140007, acc 0.96875\n",
      "2017-04-03T22:19:03.822100: step 2855, loss 0.118829, acc 0.96875\n",
      "2017-04-03T22:19:04.015409: step 2856, loss 0.14441, acc 0.953125\n",
      "2017-04-03T22:19:04.210136: step 2857, loss 0.100918, acc 0.984375\n",
      "2017-04-03T22:19:04.401985: step 2858, loss 0.122769, acc 0.984375\n",
      "2017-04-03T22:19:04.601324: step 2859, loss 0.105514, acc 0.984375\n",
      "2017-04-03T22:19:04.794897: step 2860, loss 0.0862993, acc 1\n",
      "2017-04-03T22:19:04.987529: step 2861, loss 0.0698732, acc 0.984375\n",
      "2017-04-03T22:19:05.178953: step 2862, loss 0.0733849, acc 1\n",
      "2017-04-03T22:19:05.372008: step 2863, loss 0.110508, acc 0.984375\n",
      "2017-04-03T22:19:05.562808: step 2864, loss 0.083029, acc 1\n",
      "2017-04-03T22:19:05.754931: step 2865, loss 0.129188, acc 0.953125\n",
      "2017-04-03T22:19:05.953853: step 2866, loss 0.0666695, acc 1\n",
      "2017-04-03T22:19:06.147618: step 2867, loss 0.0853399, acc 0.984375\n",
      "2017-04-03T22:19:06.338075: step 2868, loss 0.137445, acc 0.9375\n",
      "2017-04-03T22:19:06.534994: step 2869, loss 0.104071, acc 0.984375\n",
      "2017-04-03T22:19:06.725543: step 2870, loss 0.186536, acc 0.9375\n",
      "2017-04-03T22:19:06.920784: step 2871, loss 0.103761, acc 1\n",
      "2017-04-03T22:19:07.109477: step 2872, loss 0.045144, acc 1\n",
      "2017-04-03T22:19:07.300150: step 2873, loss 0.097856, acc 0.96875\n",
      "2017-04-03T22:19:07.493863: step 2874, loss 0.195856, acc 0.953125\n",
      "2017-04-03T22:19:07.693350: step 2875, loss 0.094543, acc 0.984375\n",
      "2017-04-03T22:19:07.884842: step 2876, loss 0.0822142, acc 1\n",
      "2017-04-03T22:19:08.084636: step 2877, loss 0.110811, acc 0.984375\n",
      "2017-04-03T22:19:08.275155: step 2878, loss 0.0981466, acc 0.984375\n",
      "2017-04-03T22:19:08.470275: step 2879, loss 0.0803865, acc 0.984375\n",
      "2017-04-03T22:19:08.663322: step 2880, loss 0.159392, acc 0.96875\n",
      "2017-04-03T22:19:08.862518: step 2881, loss 0.144493, acc 0.984375\n",
      "2017-04-03T22:19:09.057016: step 2882, loss 0.0940694, acc 0.96875\n",
      "2017-04-03T22:19:09.250928: step 2883, loss 0.119428, acc 0.96875\n",
      "2017-04-03T22:19:09.441589: step 2884, loss 0.0624741, acc 1\n",
      "2017-04-03T22:19:09.634184: step 2885, loss 0.175736, acc 0.921875\n",
      "2017-04-03T22:19:09.827569: step 2886, loss 0.125037, acc 0.984375\n",
      "2017-04-03T22:19:10.017080: step 2887, loss 0.0789687, acc 0.96875\n",
      "2017-04-03T22:19:10.209677: step 2888, loss 0.110938, acc 0.96875\n",
      "2017-04-03T22:19:10.399379: step 2889, loss 0.0426312, acc 1\n",
      "2017-04-03T22:19:10.593639: step 2890, loss 0.177199, acc 0.953125\n",
      "2017-04-03T22:19:10.788605: step 2891, loss 0.182424, acc 0.953125\n",
      "2017-04-03T22:19:10.985327: step 2892, loss 0.0946125, acc 0.984375\n",
      "2017-04-03T22:19:11.177895: step 2893, loss 0.0818877, acc 0.96875\n",
      "2017-04-03T22:19:11.372366: step 2894, loss 0.233297, acc 0.90625\n",
      "2017-04-03T22:19:11.568127: step 2895, loss 0.134398, acc 0.9375\n",
      "2017-04-03T22:19:11.760991: step 2896, loss 0.12702, acc 0.9375\n",
      "2017-04-03T22:19:11.950926: step 2897, loss 0.142811, acc 0.96875\n",
      "2017-04-03T22:19:12.139086: step 2898, loss 0.168347, acc 0.921875\n",
      "2017-04-03T22:19:12.333250: step 2899, loss 0.146412, acc 0.96875\n",
      "2017-04-03T22:19:12.523241: step 2900, loss 0.159877, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:19:13.178751: step 2900, loss 1.63355, acc 0.560052\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-2900\n",
      "\n",
      "2017-04-03T22:19:13.432139: step 2901, loss 0.167781, acc 0.921875\n",
      "2017-04-03T22:19:13.632157: step 2902, loss 0.123008, acc 0.96875\n",
      "2017-04-03T22:19:13.849241: step 2903, loss 0.151799, acc 0.953125\n",
      "2017-04-03T22:19:14.055323: step 2904, loss 0.0679237, acc 0.984375\n",
      "2017-04-03T22:19:14.249365: step 2905, loss 0.21088, acc 0.921875\n",
      "2017-04-03T22:19:14.452691: step 2906, loss 0.106897, acc 0.96875\n",
      "2017-04-03T22:19:14.654037: step 2907, loss 0.138934, acc 0.9375\n",
      "2017-04-03T22:19:14.854181: step 2908, loss 0.112905, acc 0.96875\n",
      "2017-04-03T22:19:15.056811: step 2909, loss 0.123481, acc 0.984375\n",
      "2017-04-03T22:19:15.248758: step 2910, loss 0.0664276, acc 0.96875\n",
      "2017-04-03T22:19:15.440545: step 2911, loss 0.147091, acc 0.9375\n",
      "2017-04-03T22:19:15.632302: step 2912, loss 0.14596, acc 0.96875\n",
      "2017-04-03T22:19:15.828415: step 2913, loss 0.204363, acc 0.9375\n",
      "2017-04-03T22:19:16.019605: step 2914, loss 0.109479, acc 0.984375\n",
      "2017-04-03T22:19:16.214915: step 2915, loss 0.0879652, acc 0.984375\n",
      "2017-04-03T22:19:16.376573: step 2916, loss 0.100442, acc 0.980769\n",
      "2017-04-03T22:19:16.567372: step 2917, loss 0.0768961, acc 0.984375\n",
      "2017-04-03T22:19:16.759930: step 2918, loss 0.0681928, acc 0.984375\n",
      "2017-04-03T22:19:16.951868: step 2919, loss 0.177851, acc 0.9375\n",
      "2017-04-03T22:19:17.145349: step 2920, loss 0.0706809, acc 0.984375\n",
      "2017-04-03T22:19:17.339185: step 2921, loss 0.12267, acc 0.9375\n",
      "2017-04-03T22:19:17.529578: step 2922, loss 0.163574, acc 0.9375\n",
      "2017-04-03T22:19:17.722815: step 2923, loss 0.12115, acc 0.953125\n",
      "2017-04-03T22:19:17.915962: step 2924, loss 0.0753292, acc 1\n",
      "2017-04-03T22:19:18.115696: step 2925, loss 0.0998005, acc 0.984375\n",
      "2017-04-03T22:19:18.311037: step 2926, loss 0.0915054, acc 0.984375\n",
      "2017-04-03T22:19:18.506716: step 2927, loss 0.132361, acc 0.96875\n",
      "2017-04-03T22:19:18.699254: step 2928, loss 0.156141, acc 0.953125\n",
      "2017-04-03T22:19:18.889254: step 2929, loss 0.0755475, acc 1\n",
      "2017-04-03T22:19:19.081591: step 2930, loss 0.119341, acc 0.953125\n",
      "2017-04-03T22:19:19.276307: step 2931, loss 0.197236, acc 0.953125\n",
      "2017-04-03T22:19:19.472700: step 2932, loss 0.160345, acc 0.96875\n",
      "2017-04-03T22:19:19.666346: step 2933, loss 0.0955163, acc 1\n",
      "2017-04-03T22:19:19.862290: step 2934, loss 0.0921419, acc 0.96875\n",
      "2017-04-03T22:19:20.054384: step 2935, loss 0.184932, acc 0.921875\n",
      "2017-04-03T22:19:20.247312: step 2936, loss 0.151315, acc 0.96875\n",
      "2017-04-03T22:19:20.441081: step 2937, loss 0.149508, acc 0.953125\n",
      "2017-04-03T22:19:20.633207: step 2938, loss 0.115221, acc 0.96875\n",
      "2017-04-03T22:19:20.827486: step 2939, loss 0.147997, acc 0.96875\n",
      "2017-04-03T22:19:21.017762: step 2940, loss 0.0732194, acc 0.96875\n",
      "2017-04-03T22:19:21.211183: step 2941, loss 0.0867104, acc 0.984375\n",
      "2017-04-03T22:19:21.400520: step 2942, loss 0.118098, acc 0.96875\n",
      "2017-04-03T22:19:21.592500: step 2943, loss 0.110264, acc 0.96875\n",
      "2017-04-03T22:19:21.782845: step 2944, loss 0.0661555, acc 1\n",
      "2017-04-03T22:19:21.972679: step 2945, loss 0.134465, acc 0.96875\n",
      "2017-04-03T22:19:22.165198: step 2946, loss 0.0670444, acc 0.984375\n",
      "2017-04-03T22:19:22.364814: step 2947, loss 0.143692, acc 0.921875\n",
      "2017-04-03T22:19:22.556611: step 2948, loss 0.23335, acc 0.90625\n",
      "2017-04-03T22:19:22.750687: step 2949, loss 0.0416283, acc 1\n",
      "2017-04-03T22:19:22.941168: step 2950, loss 0.120599, acc 0.96875\n",
      "2017-04-03T22:19:23.137225: step 2951, loss 0.128362, acc 0.953125\n",
      "2017-04-03T22:19:23.330617: step 2952, loss 0.105885, acc 0.984375\n",
      "2017-04-03T22:19:23.519740: step 2953, loss 0.0663479, acc 0.96875\n",
      "2017-04-03T22:19:23.711606: step 2954, loss 0.113437, acc 0.984375\n",
      "2017-04-03T22:19:23.905952: step 2955, loss 0.114155, acc 0.921875\n",
      "2017-04-03T22:19:24.097143: step 2956, loss 0.0799728, acc 0.984375\n",
      "2017-04-03T22:19:24.287002: step 2957, loss 0.0772014, acc 0.984375\n",
      "2017-04-03T22:19:24.480305: step 2958, loss 0.0340891, acc 1\n",
      "2017-04-03T22:19:24.673220: step 2959, loss 0.145822, acc 0.953125\n",
      "2017-04-03T22:19:24.862861: step 2960, loss 0.0691145, acc 1\n",
      "2017-04-03T22:19:25.059156: step 2961, loss 0.0996398, acc 0.96875\n",
      "2017-04-03T22:19:25.246554: step 2962, loss 0.0930949, acc 0.984375\n",
      "2017-04-03T22:19:25.438016: step 2963, loss 0.1248, acc 0.96875\n",
      "2017-04-03T22:19:25.630870: step 2964, loss 0.0520214, acc 1\n",
      "2017-04-03T22:19:25.825151: step 2965, loss 0.10877, acc 0.96875\n",
      "2017-04-03T22:19:26.017618: step 2966, loss 0.0551404, acc 1\n",
      "2017-04-03T22:19:26.209575: step 2967, loss 0.141845, acc 0.953125\n",
      "2017-04-03T22:19:26.400070: step 2968, loss 0.0858967, acc 0.984375\n",
      "2017-04-03T22:19:26.589480: step 2969, loss 0.0711065, acc 0.984375\n",
      "2017-04-03T22:19:26.781649: step 2970, loss 0.109103, acc 0.984375\n",
      "2017-04-03T22:19:26.972181: step 2971, loss 0.100906, acc 0.984375\n",
      "2017-04-03T22:19:27.165133: step 2972, loss 0.0317846, acc 1\n",
      "2017-04-03T22:19:27.356491: step 2973, loss 0.109299, acc 0.953125\n",
      "2017-04-03T22:19:27.553068: step 2974, loss 0.145698, acc 0.953125\n",
      "2017-04-03T22:19:27.746459: step 2975, loss 0.0871895, acc 0.96875\n",
      "2017-04-03T22:19:27.937035: step 2976, loss 0.131594, acc 0.9375\n",
      "2017-04-03T22:19:28.130105: step 2977, loss 0.0828624, acc 0.984375\n",
      "2017-04-03T22:19:28.322447: step 2978, loss 0.160267, acc 0.953125\n",
      "2017-04-03T22:19:28.516593: step 2979, loss 0.214194, acc 0.921875\n",
      "2017-04-03T22:19:28.710302: step 2980, loss 0.234484, acc 0.9375\n",
      "2017-04-03T22:19:28.909848: step 2981, loss 0.146336, acc 0.9375\n",
      "2017-04-03T22:19:29.101721: step 2982, loss 0.100491, acc 0.96875\n",
      "2017-04-03T22:19:29.293084: step 2983, loss 0.138108, acc 0.96875\n",
      "2017-04-03T22:19:29.487687: step 2984, loss 0.0669682, acc 0.96875\n",
      "2017-04-03T22:19:29.679489: step 2985, loss 0.128184, acc 0.953125\n",
      "2017-04-03T22:19:29.879855: step 2986, loss 0.0731885, acc 0.984375\n",
      "2017-04-03T22:19:30.076436: step 2987, loss 0.0729642, acc 0.984375\n",
      "2017-04-03T22:19:30.271922: step 2988, loss 0.0833131, acc 0.96875\n",
      "2017-04-03T22:19:30.463743: step 2989, loss 0.0638791, acc 0.984375\n",
      "2017-04-03T22:19:30.661303: step 2990, loss 0.0942035, acc 0.984375\n",
      "2017-04-03T22:19:30.854554: step 2991, loss 0.0673194, acc 1\n",
      "2017-04-03T22:19:31.045246: step 2992, loss 0.0557139, acc 1\n",
      "2017-04-03T22:19:31.233230: step 2993, loss 0.142917, acc 0.96875\n",
      "2017-04-03T22:19:31.427298: step 2994, loss 0.0863794, acc 0.984375\n",
      "2017-04-03T22:19:31.616619: step 2995, loss 0.199811, acc 0.9375\n",
      "2017-04-03T22:19:31.809812: step 2996, loss 0.0857089, acc 1\n",
      "2017-04-03T22:19:32.000649: step 2997, loss 0.050998, acc 1\n",
      "2017-04-03T22:19:32.194647: step 2998, loss 0.109607, acc 0.96875\n",
      "2017-04-03T22:19:32.385618: step 2999, loss 0.108764, acc 0.96875\n",
      "2017-04-03T22:19:32.575833: step 3000, loss 0.10521, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:19:33.229105: step 3000, loss 1.62946, acc 0.590078\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3000\n",
      "\n",
      "2017-04-03T22:19:33.524273: step 3001, loss 0.162713, acc 0.953125\n",
      "2017-04-03T22:19:33.715727: step 3002, loss 0.178071, acc 0.96875\n",
      "2017-04-03T22:19:33.904287: step 3003, loss 0.110511, acc 0.984375\n",
      "2017-04-03T22:19:34.096848: step 3004, loss 0.0473804, acc 1\n",
      "2017-04-03T22:19:34.294463: step 3005, loss 0.113018, acc 0.953125\n",
      "2017-04-03T22:19:34.490304: step 3006, loss 0.121186, acc 0.96875\n",
      "2017-04-03T22:19:34.681255: step 3007, loss 0.0990444, acc 0.984375\n",
      "2017-04-03T22:19:34.872147: step 3008, loss 0.0874329, acc 1\n",
      "2017-04-03T22:19:35.067959: step 3009, loss 0.0447469, acc 1\n",
      "2017-04-03T22:19:35.257438: step 3010, loss 0.0788332, acc 0.984375\n",
      "2017-04-03T22:19:35.450450: step 3011, loss 0.215707, acc 0.953125\n",
      "2017-04-03T22:19:35.647022: step 3012, loss 0.137358, acc 0.953125\n",
      "2017-04-03T22:19:35.840986: step 3013, loss 0.176143, acc 0.953125\n",
      "2017-04-03T22:19:36.033680: step 3014, loss 0.0841593, acc 0.984375\n",
      "2017-04-03T22:19:36.227417: step 3015, loss 0.122957, acc 0.984375\n",
      "2017-04-03T22:19:36.420842: step 3016, loss 0.142214, acc 0.9375\n",
      "2017-04-03T22:19:36.613691: step 3017, loss 0.133923, acc 0.953125\n",
      "2017-04-03T22:19:36.809317: step 3018, loss 0.111403, acc 0.953125\n",
      "2017-04-03T22:19:37.002202: step 3019, loss 0.121619, acc 0.96875\n",
      "2017-04-03T22:19:37.192674: step 3020, loss 0.0663686, acc 0.984375\n",
      "2017-04-03T22:19:37.382737: step 3021, loss 0.128955, acc 0.953125\n",
      "2017-04-03T22:19:37.576664: step 3022, loss 0.100073, acc 0.984375\n",
      "2017-04-03T22:19:37.770668: step 3023, loss 0.195165, acc 0.953125\n",
      "2017-04-03T22:19:37.936077: step 3024, loss 0.095929, acc 0.961538\n",
      "2017-04-03T22:19:38.135823: step 3025, loss 0.192788, acc 0.953125\n",
      "2017-04-03T22:19:38.328049: step 3026, loss 0.069504, acc 0.984375\n",
      "2017-04-03T22:19:38.520794: step 3027, loss 0.174931, acc 0.9375\n",
      "2017-04-03T22:19:38.715848: step 3028, loss 0.0558119, acc 1\n",
      "2017-04-03T22:19:38.909501: step 3029, loss 0.128103, acc 0.96875\n",
      "2017-04-03T22:19:39.100354: step 3030, loss 0.0546596, acc 1\n",
      "2017-04-03T22:19:39.293728: step 3031, loss 0.102481, acc 0.96875\n",
      "2017-04-03T22:19:39.486026: step 3032, loss 0.0262993, acc 1\n",
      "2017-04-03T22:19:39.677241: step 3033, loss 0.104341, acc 0.96875\n",
      "2017-04-03T22:19:39.870507: step 3034, loss 0.0811728, acc 0.984375\n",
      "2017-04-03T22:19:40.062041: step 3035, loss 0.11457, acc 0.953125\n",
      "2017-04-03T22:19:40.250075: step 3036, loss 0.0993866, acc 0.96875\n",
      "2017-04-03T22:19:40.441750: step 3037, loss 0.0979303, acc 0.984375\n",
      "2017-04-03T22:19:40.632194: step 3038, loss 0.109989, acc 0.96875\n",
      "2017-04-03T22:19:40.822366: step 3039, loss 0.126492, acc 0.96875\n",
      "2017-04-03T22:19:41.015461: step 3040, loss 0.040216, acc 1\n",
      "2017-04-03T22:19:41.206994: step 3041, loss 0.153659, acc 0.953125\n",
      "2017-04-03T22:19:41.394190: step 3042, loss 0.129823, acc 0.96875\n",
      "2017-04-03T22:19:41.581966: step 3043, loss 0.10937, acc 0.984375\n",
      "2017-04-03T22:19:41.773480: step 3044, loss 0.208138, acc 0.90625\n",
      "2017-04-03T22:19:41.967282: step 3045, loss 0.0850518, acc 0.953125\n",
      "2017-04-03T22:19:42.161720: step 3046, loss 0.0623734, acc 0.984375\n",
      "2017-04-03T22:19:42.354051: step 3047, loss 0.0929577, acc 0.96875\n",
      "2017-04-03T22:19:42.546190: step 3048, loss 0.112384, acc 0.96875\n",
      "2017-04-03T22:19:42.741205: step 3049, loss 0.120256, acc 0.921875\n",
      "2017-04-03T22:19:42.929978: step 3050, loss 0.0683768, acc 1\n",
      "2017-04-03T22:19:43.125526: step 3051, loss 0.108441, acc 0.984375\n",
      "2017-04-03T22:19:43.316503: step 3052, loss 0.0759919, acc 0.984375\n",
      "2017-04-03T22:19:43.508043: step 3053, loss 0.0581899, acc 0.96875\n",
      "2017-04-03T22:19:43.697784: step 3054, loss 0.116889, acc 0.953125\n",
      "2017-04-03T22:19:43.895245: step 3055, loss 0.136471, acc 0.96875\n",
      "2017-04-03T22:19:44.090857: step 3056, loss 0.0345839, acc 1\n",
      "2017-04-03T22:19:44.287127: step 3057, loss 0.0728729, acc 0.984375\n",
      "2017-04-03T22:19:44.484570: step 3058, loss 0.0337062, acc 1\n",
      "2017-04-03T22:19:44.676362: step 3059, loss 0.0744522, acc 0.984375\n",
      "2017-04-03T22:19:44.867974: step 3060, loss 0.0205118, acc 1\n",
      "2017-04-03T22:19:45.058151: step 3061, loss 0.112644, acc 0.984375\n",
      "2017-04-03T22:19:45.250941: step 3062, loss 0.0899986, acc 0.96875\n",
      "2017-04-03T22:19:45.444707: step 3063, loss 0.123248, acc 0.9375\n",
      "2017-04-03T22:19:45.635486: step 3064, loss 0.0814375, acc 0.984375\n",
      "2017-04-03T22:19:45.829279: step 3065, loss 0.0659672, acc 0.984375\n",
      "2017-04-03T22:19:46.019886: step 3066, loss 0.0975227, acc 0.984375\n",
      "2017-04-03T22:19:46.214438: step 3067, loss 0.07093, acc 1\n",
      "2017-04-03T22:19:46.404887: step 3068, loss 0.115623, acc 0.953125\n",
      "2017-04-03T22:19:46.600798: step 3069, loss 0.0712003, acc 0.984375\n",
      "2017-04-03T22:19:46.792117: step 3070, loss 0.0407115, acc 1\n",
      "2017-04-03T22:19:46.983743: step 3071, loss 0.0811863, acc 0.984375\n",
      "2017-04-03T22:19:47.175897: step 3072, loss 0.0683145, acc 0.984375\n",
      "2017-04-03T22:19:47.368302: step 3073, loss 0.0647249, acc 0.984375\n",
      "2017-04-03T22:19:47.563148: step 3074, loss 0.111649, acc 0.96875\n",
      "2017-04-03T22:19:47.752900: step 3075, loss 0.106817, acc 0.984375\n",
      "2017-04-03T22:19:47.940708: step 3076, loss 0.177047, acc 0.9375\n",
      "2017-04-03T22:19:48.137147: step 3077, loss 0.0748668, acc 0.984375\n",
      "2017-04-03T22:19:48.327714: step 3078, loss 0.0853211, acc 0.96875\n",
      "2017-04-03T22:19:48.521165: step 3079, loss 0.138727, acc 0.953125\n",
      "2017-04-03T22:19:48.718160: step 3080, loss 0.0836236, acc 0.984375\n",
      "2017-04-03T22:19:48.910286: step 3081, loss 0.0708888, acc 0.984375\n",
      "2017-04-03T22:19:49.102885: step 3082, loss 0.0437882, acc 1\n",
      "2017-04-03T22:19:49.296132: step 3083, loss 0.184812, acc 0.9375\n",
      "2017-04-03T22:19:49.485848: step 3084, loss 0.078057, acc 0.984375\n",
      "2017-04-03T22:19:49.677133: step 3085, loss 0.108463, acc 0.953125\n",
      "2017-04-03T22:19:49.868809: step 3086, loss 0.112745, acc 0.96875\n",
      "2017-04-03T22:19:50.063323: step 3087, loss 0.0460409, acc 1\n",
      "2017-04-03T22:19:50.259469: step 3088, loss 0.0531072, acc 0.984375\n",
      "2017-04-03T22:19:50.453142: step 3089, loss 0.243872, acc 0.90625\n",
      "2017-04-03T22:19:50.648009: step 3090, loss 0.0836919, acc 1\n",
      "2017-04-03T22:19:50.838538: step 3091, loss 0.054735, acc 1\n",
      "2017-04-03T22:19:51.031917: step 3092, loss 0.201229, acc 0.921875\n",
      "2017-04-03T22:19:51.222831: step 3093, loss 0.141484, acc 0.96875\n",
      "2017-04-03T22:19:51.418004: step 3094, loss 0.0545076, acc 0.984375\n",
      "2017-04-03T22:19:51.611917: step 3095, loss 0.0723567, acc 1\n",
      "2017-04-03T22:19:51.805129: step 3096, loss 0.0846644, acc 0.984375\n",
      "2017-04-03T22:19:51.998263: step 3097, loss 0.0592619, acc 0.984375\n",
      "2017-04-03T22:19:52.188369: step 3098, loss 0.0690447, acc 0.984375\n",
      "2017-04-03T22:19:52.380114: step 3099, loss 0.0595254, acc 1\n",
      "2017-04-03T22:19:52.572150: step 3100, loss 0.144728, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:19:53.227006: step 3100, loss 1.61763, acc 0.578329\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3100\n",
      "\n",
      "2017-04-03T22:19:53.488678: step 3101, loss 0.0756578, acc 0.96875\n",
      "2017-04-03T22:19:53.680678: step 3102, loss 0.089384, acc 0.984375\n",
      "2017-04-03T22:19:53.869349: step 3103, loss 0.191522, acc 0.953125\n",
      "2017-04-03T22:19:54.067060: step 3104, loss 0.0451363, acc 1\n",
      "2017-04-03T22:19:54.259949: step 3105, loss 0.0379236, acc 1\n",
      "2017-04-03T22:19:54.449413: step 3106, loss 0.105023, acc 0.953125\n",
      "2017-04-03T22:19:54.644357: step 3107, loss 0.0961308, acc 0.984375\n",
      "2017-04-03T22:19:54.838340: step 3108, loss 0.056893, acc 1\n",
      "2017-04-03T22:19:55.030122: step 3109, loss 0.110925, acc 1\n",
      "2017-04-03T22:19:55.224462: step 3110, loss 0.121267, acc 0.953125\n",
      "2017-04-03T22:19:55.419631: step 3111, loss 0.105678, acc 0.96875\n",
      "2017-04-03T22:19:55.614426: step 3112, loss 0.104473, acc 0.984375\n",
      "2017-04-03T22:19:55.804112: step 3113, loss 0.0702142, acc 1\n",
      "2017-04-03T22:19:56.000324: step 3114, loss 0.146718, acc 0.953125\n",
      "2017-04-03T22:19:56.197568: step 3115, loss 0.135788, acc 0.96875\n",
      "2017-04-03T22:19:56.384843: step 3116, loss 0.114934, acc 0.953125\n",
      "2017-04-03T22:19:56.578676: step 3117, loss 0.118431, acc 0.96875\n",
      "2017-04-03T22:19:56.772840: step 3118, loss 0.15733, acc 0.9375\n",
      "2017-04-03T22:19:56.966122: step 3119, loss 0.159428, acc 0.953125\n",
      "2017-04-03T22:19:57.156921: step 3120, loss 0.119121, acc 0.96875\n",
      "2017-04-03T22:19:57.349128: step 3121, loss 0.125978, acc 0.9375\n",
      "2017-04-03T22:19:57.544293: step 3122, loss 0.136163, acc 0.96875\n",
      "2017-04-03T22:19:57.733463: step 3123, loss 0.0589382, acc 1\n",
      "2017-04-03T22:19:57.925028: step 3124, loss 0.0408111, acc 1\n",
      "2017-04-03T22:19:58.113780: step 3125, loss 0.0961096, acc 0.96875\n",
      "2017-04-03T22:19:58.305115: step 3126, loss 0.102314, acc 0.984375\n",
      "2017-04-03T22:19:58.500060: step 3127, loss 0.030048, acc 1\n",
      "2017-04-03T22:19:58.698295: step 3128, loss 0.0959111, acc 0.984375\n",
      "2017-04-03T22:19:58.892706: step 3129, loss 0.0916557, acc 0.984375\n",
      "2017-04-03T22:19:59.082384: step 3130, loss 0.135142, acc 0.96875\n",
      "2017-04-03T22:19:59.276556: step 3131, loss 0.0648232, acc 0.96875\n",
      "2017-04-03T22:19:59.435570: step 3132, loss 0.158318, acc 0.942308\n",
      "2017-04-03T22:19:59.631348: step 3133, loss 0.0354795, acc 1\n",
      "2017-04-03T22:19:59.827188: step 3134, loss 0.0715773, acc 1\n",
      "2017-04-03T22:20:00.017487: step 3135, loss 0.134815, acc 0.984375\n",
      "2017-04-03T22:20:00.209951: step 3136, loss 0.0769527, acc 0.984375\n",
      "2017-04-03T22:20:00.403847: step 3137, loss 0.0798257, acc 0.96875\n",
      "2017-04-03T22:20:00.599240: step 3138, loss 0.120785, acc 0.96875\n",
      "2017-04-03T22:20:00.789861: step 3139, loss 0.102075, acc 0.96875\n",
      "2017-04-03T22:20:00.983735: step 3140, loss 0.0747104, acc 0.984375\n",
      "2017-04-03T22:20:01.180073: step 3141, loss 0.130334, acc 0.96875\n",
      "2017-04-03T22:20:01.371796: step 3142, loss 0.0689575, acc 0.984375\n",
      "2017-04-03T22:20:01.567726: step 3143, loss 0.179056, acc 0.953125\n",
      "2017-04-03T22:20:01.769951: step 3144, loss 0.169342, acc 0.953125\n",
      "2017-04-03T22:20:01.975723: step 3145, loss 0.0814043, acc 0.96875\n",
      "2017-04-03T22:20:02.193677: step 3146, loss 0.0334466, acc 1\n",
      "2017-04-03T22:20:02.395482: step 3147, loss 0.0955111, acc 0.96875\n",
      "2017-04-03T22:20:02.628934: step 3148, loss 0.124251, acc 0.96875\n",
      "2017-04-03T22:20:02.823789: step 3149, loss 0.0808746, acc 0.96875\n",
      "2017-04-03T22:20:03.031569: step 3150, loss 0.100288, acc 0.96875\n",
      "2017-04-03T22:20:03.243057: step 3151, loss 0.0672043, acc 1\n",
      "2017-04-03T22:20:03.432534: step 3152, loss 0.178537, acc 0.9375\n",
      "2017-04-03T22:20:03.624237: step 3153, loss 0.131849, acc 0.96875\n",
      "2017-04-03T22:20:03.815140: step 3154, loss 0.117011, acc 0.953125\n",
      "2017-04-03T22:20:04.008877: step 3155, loss 0.0971297, acc 0.953125\n",
      "2017-04-03T22:20:04.202672: step 3156, loss 0.126314, acc 0.96875\n",
      "2017-04-03T22:20:04.396980: step 3157, loss 0.141622, acc 0.9375\n",
      "2017-04-03T22:20:04.587240: step 3158, loss 0.0484968, acc 1\n",
      "2017-04-03T22:20:04.782422: step 3159, loss 0.0545787, acc 1\n",
      "2017-04-03T22:20:04.972694: step 3160, loss 0.135867, acc 0.953125\n",
      "2017-04-03T22:20:05.168040: step 3161, loss 0.103146, acc 0.96875\n",
      "2017-04-03T22:20:05.360264: step 3162, loss 0.0855497, acc 0.96875\n",
      "2017-04-03T22:20:05.555101: step 3163, loss 0.0685218, acc 0.984375\n",
      "2017-04-03T22:20:05.743408: step 3164, loss 0.0546018, acc 1\n",
      "2017-04-03T22:20:05.936343: step 3165, loss 0.0944188, acc 0.96875\n",
      "2017-04-03T22:20:06.133679: step 3166, loss 0.101626, acc 0.953125\n",
      "2017-04-03T22:20:06.329794: step 3167, loss 0.0803102, acc 1\n",
      "2017-04-03T22:20:06.526153: step 3168, loss 0.0933914, acc 0.953125\n",
      "2017-04-03T22:20:06.723100: step 3169, loss 0.136546, acc 0.953125\n",
      "2017-04-03T22:20:06.927115: step 3170, loss 0.0580992, acc 0.984375\n",
      "2017-04-03T22:20:07.143146: step 3171, loss 0.093547, acc 0.984375\n",
      "2017-04-03T22:20:07.353195: step 3172, loss 0.0418432, acc 1\n",
      "2017-04-03T22:20:07.554068: step 3173, loss 0.112868, acc 0.96875\n",
      "2017-04-03T22:20:07.757233: step 3174, loss 0.099454, acc 0.96875\n",
      "2017-04-03T22:20:07.979471: step 3175, loss 0.147603, acc 0.96875\n",
      "2017-04-03T22:20:08.197658: step 3176, loss 0.0992419, acc 0.984375\n",
      "2017-04-03T22:20:08.413757: step 3177, loss 0.0720946, acc 0.96875\n",
      "2017-04-03T22:20:08.627374: step 3178, loss 0.0481291, acc 1\n",
      "2017-04-03T22:20:08.827258: step 3179, loss 0.0736866, acc 0.984375\n",
      "2017-04-03T22:20:09.032392: step 3180, loss 0.201118, acc 0.953125\n",
      "2017-04-03T22:20:09.254128: step 3181, loss 0.0421848, acc 1\n",
      "2017-04-03T22:20:09.477985: step 3182, loss 0.220054, acc 0.90625\n",
      "2017-04-03T22:20:09.694991: step 3183, loss 0.0437171, acc 1\n",
      "2017-04-03T22:20:09.905812: step 3184, loss 0.103897, acc 0.96875\n",
      "2017-04-03T22:20:10.097734: step 3185, loss 0.0512189, acc 1\n",
      "2017-04-03T22:20:10.290246: step 3186, loss 0.184408, acc 0.890625\n",
      "2017-04-03T22:20:10.488323: step 3187, loss 0.115725, acc 0.96875\n",
      "2017-04-03T22:20:10.681989: step 3188, loss 0.0572119, acc 0.984375\n",
      "2017-04-03T22:20:10.874232: step 3189, loss 0.1237, acc 0.96875\n",
      "2017-04-03T22:20:11.068348: step 3190, loss 0.0820055, acc 0.984375\n",
      "2017-04-03T22:20:11.262338: step 3191, loss 0.0877988, acc 0.984375\n",
      "2017-04-03T22:20:11.462997: step 3192, loss 0.145448, acc 0.9375\n",
      "2017-04-03T22:20:11.653371: step 3193, loss 0.0687209, acc 0.984375\n",
      "2017-04-03T22:20:11.852896: step 3194, loss 0.0641558, acc 0.984375\n",
      "2017-04-03T22:20:12.047953: step 3195, loss 0.104333, acc 0.984375\n",
      "2017-04-03T22:20:12.241120: step 3196, loss 0.0767445, acc 0.984375\n",
      "2017-04-03T22:20:12.438810: step 3197, loss 0.117518, acc 0.984375\n",
      "2017-04-03T22:20:12.632678: step 3198, loss 0.114125, acc 0.96875\n",
      "2017-04-03T22:20:12.827945: step 3199, loss 0.134341, acc 0.9375\n",
      "2017-04-03T22:20:13.029424: step 3200, loss 0.0689811, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:20:13.736961: step 3200, loss 1.63872, acc 0.560052\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3200\n",
      "\n",
      "2017-04-03T22:20:14.023035: step 3201, loss 0.0345362, acc 1\n",
      "2017-04-03T22:20:14.233269: step 3202, loss 0.0724676, acc 0.96875\n",
      "2017-04-03T22:20:14.454092: step 3203, loss 0.194814, acc 0.953125\n",
      "2017-04-03T22:20:14.657699: step 3204, loss 0.105171, acc 0.96875\n",
      "2017-04-03T22:20:14.848804: step 3205, loss 0.0830085, acc 0.96875\n",
      "2017-04-03T22:20:15.047357: step 3206, loss 0.12118, acc 0.953125\n",
      "2017-04-03T22:20:15.238973: step 3207, loss 0.0755137, acc 0.984375\n",
      "2017-04-03T22:20:15.431573: step 3208, loss 0.103326, acc 0.96875\n",
      "2017-04-03T22:20:15.629732: step 3209, loss 0.146348, acc 0.96875\n",
      "2017-04-03T22:20:15.849423: step 3210, loss 0.119437, acc 0.96875\n",
      "2017-04-03T22:20:16.057500: step 3211, loss 0.0608023, acc 1\n",
      "2017-04-03T22:20:16.264983: step 3212, loss 0.0414778, acc 0.984375\n",
      "2017-04-03T22:20:16.466357: step 3213, loss 0.108879, acc 0.953125\n",
      "2017-04-03T22:20:16.656280: step 3214, loss 0.104561, acc 0.96875\n",
      "2017-04-03T22:20:16.845528: step 3215, loss 0.0475427, acc 1\n",
      "2017-04-03T22:20:17.036108: step 3216, loss 0.10367, acc 0.953125\n",
      "2017-04-03T22:20:17.231447: step 3217, loss 0.0901976, acc 0.984375\n",
      "2017-04-03T22:20:17.429157: step 3218, loss 0.0716119, acc 0.984375\n",
      "2017-04-03T22:20:17.632684: step 3219, loss 0.0654047, acc 0.984375\n",
      "2017-04-03T22:20:17.830212: step 3220, loss 0.046818, acc 1\n",
      "2017-04-03T22:20:18.032568: step 3221, loss 0.0996292, acc 0.984375\n",
      "2017-04-03T22:20:18.227041: step 3222, loss 0.0749179, acc 0.984375\n",
      "2017-04-03T22:20:18.425582: step 3223, loss 0.0886, acc 0.984375\n",
      "2017-04-03T22:20:18.637074: step 3224, loss 0.0527414, acc 0.984375\n",
      "2017-04-03T22:20:18.833457: step 3225, loss 0.0712348, acc 0.984375\n",
      "2017-04-03T22:20:19.024008: step 3226, loss 0.0793518, acc 0.984375\n",
      "2017-04-03T22:20:19.214645: step 3227, loss 0.0466995, acc 0.96875\n",
      "2017-04-03T22:20:19.409229: step 3228, loss 0.0661446, acc 0.984375\n",
      "2017-04-03T22:20:19.599162: step 3229, loss 0.0978656, acc 0.96875\n",
      "2017-04-03T22:20:19.790155: step 3230, loss 0.18601, acc 0.96875\n",
      "2017-04-03T22:20:19.980629: step 3231, loss 0.0502835, acc 1\n",
      "2017-04-03T22:20:20.172670: step 3232, loss 0.103382, acc 0.96875\n",
      "2017-04-03T22:20:20.367893: step 3233, loss 0.131244, acc 0.953125\n",
      "2017-04-03T22:20:20.559656: step 3234, loss 0.0770914, acc 0.96875\n",
      "2017-04-03T22:20:20.751861: step 3235, loss 0.0828386, acc 0.984375\n",
      "2017-04-03T22:20:20.943992: step 3236, loss 0.0766118, acc 0.96875\n",
      "2017-04-03T22:20:21.143849: step 3237, loss 0.123891, acc 0.96875\n",
      "2017-04-03T22:20:21.343544: step 3238, loss 0.0716949, acc 0.984375\n",
      "2017-04-03T22:20:21.536054: step 3239, loss 0.0989474, acc 0.984375\n",
      "2017-04-03T22:20:21.698472: step 3240, loss 0.212464, acc 0.961538\n",
      "2017-04-03T22:20:21.890481: step 3241, loss 0.0569922, acc 0.984375\n",
      "2017-04-03T22:20:22.084821: step 3242, loss 0.132243, acc 0.953125\n",
      "2017-04-03T22:20:22.276272: step 3243, loss 0.0608037, acc 0.984375\n",
      "2017-04-03T22:20:22.471827: step 3244, loss 0.0785241, acc 0.953125\n",
      "2017-04-03T22:20:22.665704: step 3245, loss 0.0642434, acc 1\n",
      "2017-04-03T22:20:22.860146: step 3246, loss 0.0616383, acc 0.984375\n",
      "2017-04-03T22:20:23.050289: step 3247, loss 0.0576418, acc 0.984375\n",
      "2017-04-03T22:20:23.245419: step 3248, loss 0.112325, acc 0.96875\n",
      "2017-04-03T22:20:23.438146: step 3249, loss 0.100821, acc 0.96875\n",
      "2017-04-03T22:20:23.632785: step 3250, loss 0.0716154, acc 0.96875\n",
      "2017-04-03T22:20:23.838747: step 3251, loss 0.106108, acc 0.953125\n",
      "2017-04-03T22:20:24.032330: step 3252, loss 0.031299, acc 1\n",
      "2017-04-03T22:20:24.221378: step 3253, loss 0.115543, acc 0.953125\n",
      "2017-04-03T22:20:24.414575: step 3254, loss 0.0307791, acc 1\n",
      "2017-04-03T22:20:24.604677: step 3255, loss 0.0463163, acc 0.984375\n",
      "2017-04-03T22:20:24.798402: step 3256, loss 0.0489224, acc 1\n",
      "2017-04-03T22:20:24.994377: step 3257, loss 0.122991, acc 0.96875\n",
      "2017-04-03T22:20:25.186154: step 3258, loss 0.105394, acc 0.984375\n",
      "2017-04-03T22:20:25.380357: step 3259, loss 0.0795246, acc 0.96875\n",
      "2017-04-03T22:20:25.573959: step 3260, loss 0.115153, acc 0.96875\n",
      "2017-04-03T22:20:25.765783: step 3261, loss 0.109783, acc 0.953125\n",
      "2017-04-03T22:20:25.958253: step 3262, loss 0.0584394, acc 0.984375\n",
      "2017-04-03T22:20:26.149637: step 3263, loss 0.0669444, acc 0.984375\n",
      "2017-04-03T22:20:26.337341: step 3264, loss 0.0578141, acc 1\n",
      "2017-04-03T22:20:26.528973: step 3265, loss 0.0754405, acc 0.96875\n",
      "2017-04-03T22:20:26.719176: step 3266, loss 0.101841, acc 0.984375\n",
      "2017-04-03T22:20:26.910167: step 3267, loss 0.152872, acc 0.96875\n",
      "2017-04-03T22:20:27.107304: step 3268, loss 0.0359947, acc 1\n",
      "2017-04-03T22:20:27.302514: step 3269, loss 0.128881, acc 0.953125\n",
      "2017-04-03T22:20:27.500503: step 3270, loss 0.0899483, acc 0.984375\n",
      "2017-04-03T22:20:27.693145: step 3271, loss 0.192709, acc 0.953125\n",
      "2017-04-03T22:20:27.884403: step 3272, loss 0.110429, acc 0.96875\n",
      "2017-04-03T22:20:28.082974: step 3273, loss 0.0601992, acc 0.984375\n",
      "2017-04-03T22:20:28.275852: step 3274, loss 0.0771449, acc 0.984375\n",
      "2017-04-03T22:20:28.469602: step 3275, loss 0.0773341, acc 0.984375\n",
      "2017-04-03T22:20:28.662236: step 3276, loss 0.0822259, acc 0.984375\n",
      "2017-04-03T22:20:28.856745: step 3277, loss 0.0671347, acc 0.984375\n",
      "2017-04-03T22:20:29.050262: step 3278, loss 0.150745, acc 0.96875\n",
      "2017-04-03T22:20:29.247038: step 3279, loss 0.157888, acc 0.96875\n",
      "2017-04-03T22:20:29.438660: step 3280, loss 0.0525085, acc 1\n",
      "2017-04-03T22:20:29.633618: step 3281, loss 0.0870228, acc 0.984375\n",
      "2017-04-03T22:20:29.831349: step 3282, loss 0.0554666, acc 0.984375\n",
      "2017-04-03T22:20:30.024015: step 3283, loss 0.10567, acc 0.953125\n",
      "2017-04-03T22:20:30.216259: step 3284, loss 0.121734, acc 0.953125\n",
      "2017-04-03T22:20:30.408875: step 3285, loss 0.0909119, acc 0.953125\n",
      "2017-04-03T22:20:30.600655: step 3286, loss 0.0798356, acc 0.96875\n",
      "2017-04-03T22:20:30.792898: step 3287, loss 0.170539, acc 0.9375\n",
      "2017-04-03T22:20:30.985356: step 3288, loss 0.0677388, acc 0.96875\n",
      "2017-04-03T22:20:31.175699: step 3289, loss 0.0887038, acc 0.984375\n",
      "2017-04-03T22:20:31.367606: step 3290, loss 0.0653205, acc 0.984375\n",
      "2017-04-03T22:20:31.560234: step 3291, loss 0.0664099, acc 0.984375\n",
      "2017-04-03T22:20:31.750404: step 3292, loss 0.0345347, acc 1\n",
      "2017-04-03T22:20:31.954169: step 3293, loss 0.0447413, acc 0.984375\n",
      "2017-04-03T22:20:32.147595: step 3294, loss 0.197375, acc 0.953125\n",
      "2017-04-03T22:20:32.339961: step 3295, loss 0.11064, acc 0.96875\n",
      "2017-04-03T22:20:32.532347: step 3296, loss 0.0676918, acc 1\n",
      "2017-04-03T22:20:32.726937: step 3297, loss 0.0661994, acc 0.984375\n",
      "2017-04-03T22:20:32.920152: step 3298, loss 0.132531, acc 0.953125\n",
      "2017-04-03T22:20:33.113713: step 3299, loss 0.090172, acc 0.984375\n",
      "2017-04-03T22:20:33.305899: step 3300, loss 0.122632, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:20:33.954045: step 3300, loss 1.67279, acc 0.561358\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3300\n",
      "\n",
      "2017-04-03T22:20:34.205106: step 3301, loss 0.276319, acc 0.953125\n",
      "2017-04-03T22:20:34.396508: step 3302, loss 0.0723419, acc 1\n",
      "2017-04-03T22:20:34.588083: step 3303, loss 0.121005, acc 0.984375\n",
      "2017-04-03T22:20:34.777556: step 3304, loss 0.0553606, acc 1\n",
      "2017-04-03T22:20:34.974016: step 3305, loss 0.0874967, acc 0.984375\n",
      "2017-04-03T22:20:35.168542: step 3306, loss 0.0763233, acc 0.984375\n",
      "2017-04-03T22:20:35.364554: step 3307, loss 0.0836893, acc 0.984375\n",
      "2017-04-03T22:20:35.560096: step 3308, loss 0.0843271, acc 0.984375\n",
      "2017-04-03T22:20:35.749830: step 3309, loss 0.0602349, acc 1\n",
      "2017-04-03T22:20:35.937877: step 3310, loss 0.0634235, acc 0.984375\n",
      "2017-04-03T22:20:36.133557: step 3311, loss 0.082653, acc 0.96875\n",
      "2017-04-03T22:20:36.324384: step 3312, loss 0.126562, acc 0.96875\n",
      "2017-04-03T22:20:36.515798: step 3313, loss 0.0960639, acc 0.96875\n",
      "2017-04-03T22:20:36.707645: step 3314, loss 0.0708541, acc 0.984375\n",
      "2017-04-03T22:20:36.896165: step 3315, loss 0.246935, acc 0.921875\n",
      "2017-04-03T22:20:37.085394: step 3316, loss 0.112027, acc 0.953125\n",
      "2017-04-03T22:20:37.284941: step 3317, loss 0.104344, acc 0.96875\n",
      "2017-04-03T22:20:37.476663: step 3318, loss 0.0596687, acc 1\n",
      "2017-04-03T22:20:37.669767: step 3319, loss 0.0862803, acc 0.953125\n",
      "2017-04-03T22:20:37.858352: step 3320, loss 0.203438, acc 0.9375\n",
      "2017-04-03T22:20:38.048754: step 3321, loss 0.109948, acc 0.96875\n",
      "2017-04-03T22:20:38.244286: step 3322, loss 0.127928, acc 0.953125\n",
      "2017-04-03T22:20:38.435063: step 3323, loss 0.071383, acc 0.96875\n",
      "2017-04-03T22:20:38.629219: step 3324, loss 0.0507169, acc 1\n",
      "2017-04-03T22:20:38.821213: step 3325, loss 0.114856, acc 0.96875\n",
      "2017-04-03T22:20:39.012743: step 3326, loss 0.0865365, acc 0.96875\n",
      "2017-04-03T22:20:39.208605: step 3327, loss 0.105798, acc 0.96875\n",
      "2017-04-03T22:20:39.399120: step 3328, loss 0.109522, acc 0.953125\n",
      "2017-04-03T22:20:39.588384: step 3329, loss 0.0394312, acc 1\n",
      "2017-04-03T22:20:39.782633: step 3330, loss 0.10277, acc 0.96875\n",
      "2017-04-03T22:20:39.979670: step 3331, loss 0.0716667, acc 0.96875\n",
      "2017-04-03T22:20:40.172037: step 3332, loss 0.0519498, acc 0.984375\n",
      "2017-04-03T22:20:40.367579: step 3333, loss 0.0975768, acc 0.953125\n",
      "2017-04-03T22:20:40.563728: step 3334, loss 0.0843183, acc 0.984375\n",
      "2017-04-03T22:20:40.759482: step 3335, loss 0.0360866, acc 1\n",
      "2017-04-03T22:20:40.952902: step 3336, loss 0.117186, acc 0.96875\n",
      "2017-04-03T22:20:41.149552: step 3337, loss 0.11401, acc 0.953125\n",
      "2017-04-03T22:20:41.340719: step 3338, loss 0.0219006, acc 1\n",
      "2017-04-03T22:20:41.535754: step 3339, loss 0.058805, acc 0.984375\n",
      "2017-04-03T22:20:41.726642: step 3340, loss 0.1097, acc 0.96875\n",
      "2017-04-03T22:20:41.923457: step 3341, loss 0.0929029, acc 0.953125\n",
      "2017-04-03T22:20:42.115371: step 3342, loss 0.0598474, acc 0.984375\n",
      "2017-04-03T22:20:42.312353: step 3343, loss 0.117354, acc 0.953125\n",
      "2017-04-03T22:20:42.508147: step 3344, loss 0.127257, acc 0.96875\n",
      "2017-04-03T22:20:42.699219: step 3345, loss 0.158142, acc 0.953125\n",
      "2017-04-03T22:20:42.893557: step 3346, loss 0.0761468, acc 0.984375\n",
      "2017-04-03T22:20:43.088895: step 3347, loss 0.160402, acc 0.9375\n",
      "2017-04-03T22:20:43.251456: step 3348, loss 0.143877, acc 0.961538\n",
      "2017-04-03T22:20:43.443193: step 3349, loss 0.0673555, acc 0.953125\n",
      "2017-04-03T22:20:43.634417: step 3350, loss 0.10947, acc 0.984375\n",
      "2017-04-03T22:20:43.831500: step 3351, loss 0.0343098, acc 1\n",
      "2017-04-03T22:20:44.022349: step 3352, loss 0.0931275, acc 0.96875\n",
      "2017-04-03T22:20:44.211968: step 3353, loss 0.0765819, acc 0.984375\n",
      "2017-04-03T22:20:44.402828: step 3354, loss 0.119284, acc 0.9375\n",
      "2017-04-03T22:20:44.597693: step 3355, loss 0.0529412, acc 0.984375\n",
      "2017-04-03T22:20:44.787657: step 3356, loss 0.0823158, acc 0.953125\n",
      "2017-04-03T22:20:44.979437: step 3357, loss 0.175232, acc 0.96875\n",
      "2017-04-03T22:20:45.171842: step 3358, loss 0.0469933, acc 1\n",
      "2017-04-03T22:20:45.364303: step 3359, loss 0.120189, acc 0.984375\n",
      "2017-04-03T22:20:45.558459: step 3360, loss 0.0891923, acc 0.96875\n",
      "2017-04-03T22:20:45.751387: step 3361, loss 0.096681, acc 0.96875\n",
      "2017-04-03T22:20:45.942961: step 3362, loss 0.12214, acc 0.96875\n",
      "2017-04-03T22:20:46.136119: step 3363, loss 0.080188, acc 0.984375\n",
      "2017-04-03T22:20:46.332567: step 3364, loss 0.125756, acc 0.953125\n",
      "2017-04-03T22:20:46.527690: step 3365, loss 0.0932019, acc 0.984375\n",
      "2017-04-03T22:20:46.721323: step 3366, loss 0.0606147, acc 0.984375\n",
      "2017-04-03T22:20:46.916258: step 3367, loss 0.0217224, acc 1\n",
      "2017-04-03T22:20:47.109434: step 3368, loss 0.0622872, acc 1\n",
      "2017-04-03T22:20:47.304798: step 3369, loss 0.068321, acc 0.984375\n",
      "2017-04-03T22:20:47.499935: step 3370, loss 0.122481, acc 0.9375\n",
      "2017-04-03T22:20:47.693760: step 3371, loss 0.109558, acc 0.953125\n",
      "2017-04-03T22:20:47.899851: step 3372, loss 0.170231, acc 0.9375\n",
      "2017-04-03T22:20:48.093289: step 3373, loss 0.0749897, acc 0.96875\n",
      "2017-04-03T22:20:48.288292: step 3374, loss 0.0709411, acc 0.96875\n",
      "2017-04-03T22:20:48.482002: step 3375, loss 0.110788, acc 0.953125\n",
      "2017-04-03T22:20:48.679925: step 3376, loss 0.0860998, acc 0.96875\n",
      "2017-04-03T22:20:48.872712: step 3377, loss 0.0525754, acc 0.984375\n",
      "2017-04-03T22:20:49.064643: step 3378, loss 0.0664102, acc 0.984375\n",
      "2017-04-03T22:20:49.260182: step 3379, loss 0.120144, acc 0.96875\n",
      "2017-04-03T22:20:49.450895: step 3380, loss 0.0990526, acc 0.984375\n",
      "2017-04-03T22:20:49.640871: step 3381, loss 0.0555492, acc 0.984375\n",
      "2017-04-03T22:20:49.832776: step 3382, loss 0.0911409, acc 0.984375\n",
      "2017-04-03T22:20:50.026464: step 3383, loss 0.0629419, acc 0.96875\n",
      "2017-04-03T22:20:50.221882: step 3384, loss 0.0745851, acc 0.96875\n",
      "2017-04-03T22:20:50.413086: step 3385, loss 0.0615971, acc 1\n",
      "2017-04-03T22:20:50.607142: step 3386, loss 0.0328013, acc 1\n",
      "2017-04-03T22:20:50.802704: step 3387, loss 0.0883012, acc 0.96875\n",
      "2017-04-03T22:20:50.996298: step 3388, loss 0.153967, acc 0.953125\n",
      "2017-04-03T22:20:51.192063: step 3389, loss 0.0558658, acc 0.984375\n",
      "2017-04-03T22:20:51.380505: step 3390, loss 0.0302389, acc 1\n",
      "2017-04-03T22:20:51.570359: step 3391, loss 0.0339938, acc 1\n",
      "2017-04-03T22:20:51.764115: step 3392, loss 0.0755256, acc 0.96875\n",
      "2017-04-03T22:20:51.960130: step 3393, loss 0.0707399, acc 0.984375\n",
      "2017-04-03T22:20:52.147992: step 3394, loss 0.104052, acc 0.96875\n",
      "2017-04-03T22:20:52.340998: step 3395, loss 0.233497, acc 0.96875\n",
      "2017-04-03T22:20:52.536764: step 3396, loss 0.0449668, acc 1\n",
      "2017-04-03T22:20:52.726122: step 3397, loss 0.130208, acc 0.96875\n",
      "2017-04-03T22:20:52.919952: step 3398, loss 0.101178, acc 0.984375\n",
      "2017-04-03T22:20:53.108924: step 3399, loss 0.0899241, acc 0.96875\n",
      "2017-04-03T22:20:53.304019: step 3400, loss 0.0692355, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:20:53.981492: step 3400, loss 1.71169, acc 0.56658\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3400\n",
      "\n",
      "2017-04-03T22:20:54.244963: step 3401, loss 0.0499978, acc 1\n",
      "2017-04-03T22:20:54.435900: step 3402, loss 0.0823495, acc 0.96875\n",
      "2017-04-03T22:20:54.626897: step 3403, loss 0.0702075, acc 0.984375\n",
      "2017-04-03T22:20:54.818779: step 3404, loss 0.0939686, acc 0.96875\n",
      "2017-04-03T22:20:55.009288: step 3405, loss 0.0515435, acc 0.984375\n",
      "2017-04-03T22:20:55.205762: step 3406, loss 0.07869, acc 0.96875\n",
      "2017-04-03T22:20:55.391713: step 3407, loss 0.0930975, acc 0.953125\n",
      "2017-04-03T22:20:55.587632: step 3408, loss 0.0921478, acc 0.953125\n",
      "2017-04-03T22:20:55.781036: step 3409, loss 0.167032, acc 0.953125\n",
      "2017-04-03T22:20:55.969510: step 3410, loss 0.0539752, acc 0.984375\n",
      "2017-04-03T22:20:56.162963: step 3411, loss 0.0767504, acc 0.984375\n",
      "2017-04-03T22:20:56.359465: step 3412, loss 0.13794, acc 0.96875\n",
      "2017-04-03T22:20:56.550649: step 3413, loss 0.137986, acc 0.953125\n",
      "2017-04-03T22:20:56.739966: step 3414, loss 0.0498711, acc 1\n",
      "2017-04-03T22:20:56.933438: step 3415, loss 0.11274, acc 0.96875\n",
      "2017-04-03T22:20:57.122753: step 3416, loss 0.0269514, acc 1\n",
      "2017-04-03T22:20:57.314703: step 3417, loss 0.0414628, acc 1\n",
      "2017-04-03T22:20:57.508158: step 3418, loss 0.0621358, acc 0.984375\n",
      "2017-04-03T22:20:57.700548: step 3419, loss 0.126087, acc 0.953125\n",
      "2017-04-03T22:20:57.892431: step 3420, loss 0.0464234, acc 1\n",
      "2017-04-03T22:20:58.085039: step 3421, loss 0.0942658, acc 0.953125\n",
      "2017-04-03T22:20:58.276726: step 3422, loss 0.111009, acc 0.96875\n",
      "2017-04-03T22:20:58.470023: step 3423, loss 0.0356431, acc 1\n",
      "2017-04-03T22:20:58.667335: step 3424, loss 0.116747, acc 0.953125\n",
      "2017-04-03T22:20:58.858733: step 3425, loss 0.136316, acc 0.953125\n",
      "2017-04-03T22:20:59.048434: step 3426, loss 0.0236336, acc 1\n",
      "2017-04-03T22:20:59.240757: step 3427, loss 0.0807223, acc 0.96875\n",
      "2017-04-03T22:20:59.432496: step 3428, loss 0.160515, acc 0.921875\n",
      "2017-04-03T22:20:59.624440: step 3429, loss 0.0843007, acc 0.984375\n",
      "2017-04-03T22:20:59.818365: step 3430, loss 0.161219, acc 0.9375\n",
      "2017-04-03T22:21:00.014408: step 3431, loss 0.0659902, acc 0.96875\n",
      "2017-04-03T22:21:00.208957: step 3432, loss 0.0653196, acc 0.96875\n",
      "2017-04-03T22:21:00.403880: step 3433, loss 0.108284, acc 0.984375\n",
      "2017-04-03T22:21:00.594168: step 3434, loss 0.135153, acc 0.96875\n",
      "2017-04-03T22:21:00.789886: step 3435, loss 0.11699, acc 0.96875\n",
      "2017-04-03T22:21:00.977809: step 3436, loss 0.0427509, acc 1\n",
      "2017-04-03T22:21:01.172404: step 3437, loss 0.0892098, acc 0.984375\n",
      "2017-04-03T22:21:01.364393: step 3438, loss 0.153258, acc 0.953125\n",
      "2017-04-03T22:21:01.556055: step 3439, loss 0.148718, acc 0.9375\n",
      "2017-04-03T22:21:01.743797: step 3440, loss 0.0772617, acc 0.984375\n",
      "2017-04-03T22:21:01.939456: step 3441, loss 0.134976, acc 0.953125\n",
      "2017-04-03T22:21:02.129857: step 3442, loss 0.0549939, acc 0.984375\n",
      "2017-04-03T22:21:02.327365: step 3443, loss 0.0279291, acc 1\n",
      "2017-04-03T22:21:02.521168: step 3444, loss 0.0701118, acc 0.984375\n",
      "2017-04-03T22:21:02.720574: step 3445, loss 0.0703664, acc 0.984375\n",
      "2017-04-03T22:21:02.924241: step 3446, loss 0.0532021, acc 1\n",
      "2017-04-03T22:21:03.120681: step 3447, loss 0.0945373, acc 0.953125\n",
      "2017-04-03T22:21:03.317886: step 3448, loss 0.10142, acc 0.953125\n",
      "2017-04-03T22:21:03.511537: step 3449, loss 0.141917, acc 0.953125\n",
      "2017-04-03T22:21:03.704093: step 3450, loss 0.086848, acc 0.984375\n",
      "2017-04-03T22:21:03.901092: step 3451, loss 0.0522741, acc 0.984375\n",
      "2017-04-03T22:21:04.091829: step 3452, loss 0.0530883, acc 0.984375\n",
      "2017-04-03T22:21:04.287143: step 3453, loss 0.0979797, acc 0.953125\n",
      "2017-04-03T22:21:04.478438: step 3454, loss 0.0828243, acc 0.984375\n",
      "2017-04-03T22:21:04.669825: step 3455, loss 0.158292, acc 0.921875\n",
      "2017-04-03T22:21:04.830136: step 3456, loss 0.0756393, acc 0.980769\n",
      "2017-04-03T22:21:05.020480: step 3457, loss 0.0356009, acc 1\n",
      "2017-04-03T22:21:05.217344: step 3458, loss 0.0508361, acc 1\n",
      "2017-04-03T22:21:05.409236: step 3459, loss 0.0707971, acc 0.96875\n",
      "2017-04-03T22:21:05.603147: step 3460, loss 0.0418094, acc 1\n",
      "2017-04-03T22:21:05.794232: step 3461, loss 0.122885, acc 0.953125\n",
      "2017-04-03T22:21:05.992782: step 3462, loss 0.0185421, acc 1\n",
      "2017-04-03T22:21:06.185166: step 3463, loss 0.02284, acc 1\n",
      "2017-04-03T22:21:06.379979: step 3464, loss 0.142171, acc 0.9375\n",
      "2017-04-03T22:21:06.572603: step 3465, loss 0.0284913, acc 1\n",
      "2017-04-03T22:21:06.770022: step 3466, loss 0.0562157, acc 0.96875\n",
      "2017-04-03T22:21:06.962366: step 3467, loss 0.0600548, acc 0.984375\n",
      "2017-04-03T22:21:07.151953: step 3468, loss 0.0972055, acc 0.96875\n",
      "2017-04-03T22:21:07.347119: step 3469, loss 0.116786, acc 0.984375\n",
      "2017-04-03T22:21:07.536992: step 3470, loss 0.0407905, acc 1\n",
      "2017-04-03T22:21:07.729714: step 3471, loss 0.085505, acc 0.984375\n",
      "2017-04-03T22:21:07.921837: step 3472, loss 0.0845668, acc 0.96875\n",
      "2017-04-03T22:21:08.109713: step 3473, loss 0.108837, acc 0.96875\n",
      "2017-04-03T22:21:08.303143: step 3474, loss 0.04061, acc 0.984375\n",
      "2017-04-03T22:21:08.497975: step 3475, loss 0.0435999, acc 1\n",
      "2017-04-03T22:21:08.692238: step 3476, loss 0.185573, acc 0.96875\n",
      "2017-04-03T22:21:08.885383: step 3477, loss 0.0957448, acc 0.96875\n",
      "2017-04-03T22:21:09.076790: step 3478, loss 0.0365569, acc 0.984375\n",
      "2017-04-03T22:21:09.268135: step 3479, loss 0.135119, acc 0.953125\n",
      "2017-04-03T22:21:09.459638: step 3480, loss 0.0429714, acc 1\n",
      "2017-04-03T22:21:09.650123: step 3481, loss 0.101708, acc 0.984375\n",
      "2017-04-03T22:21:09.844307: step 3482, loss 0.0649636, acc 0.984375\n",
      "2017-04-03T22:21:10.037827: step 3483, loss 0.057544, acc 0.984375\n",
      "2017-04-03T22:21:10.230971: step 3484, loss 0.0720377, acc 0.984375\n",
      "2017-04-03T22:21:10.425906: step 3485, loss 0.068503, acc 0.984375\n",
      "2017-04-03T22:21:10.616624: step 3486, loss 0.035861, acc 1\n",
      "2017-04-03T22:21:10.805880: step 3487, loss 0.0537446, acc 1\n",
      "2017-04-03T22:21:11.002849: step 3488, loss 0.12602, acc 0.9375\n",
      "2017-04-03T22:21:11.195790: step 3489, loss 0.0805793, acc 0.96875\n",
      "2017-04-03T22:21:11.392445: step 3490, loss 0.108806, acc 0.96875\n",
      "2017-04-03T22:21:11.583729: step 3491, loss 0.155329, acc 0.953125\n",
      "2017-04-03T22:21:11.774248: step 3492, loss 0.142357, acc 0.96875\n",
      "2017-04-03T22:21:11.964176: step 3493, loss 0.0869916, acc 0.984375\n",
      "2017-04-03T22:21:12.153136: step 3494, loss 0.156905, acc 0.9375\n",
      "2017-04-03T22:21:12.345152: step 3495, loss 0.073237, acc 0.96875\n",
      "2017-04-03T22:21:12.536702: step 3496, loss 0.0950133, acc 0.96875\n",
      "2017-04-03T22:21:12.730383: step 3497, loss 0.13074, acc 0.9375\n",
      "2017-04-03T22:21:12.925366: step 3498, loss 0.155774, acc 0.96875\n",
      "2017-04-03T22:21:13.115676: step 3499, loss 0.0935953, acc 0.984375\n",
      "2017-04-03T22:21:13.309953: step 3500, loss 0.0770655, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:21:13.961272: step 3500, loss 1.72529, acc 0.570496\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3500\n",
      "\n",
      "2017-04-03T22:21:14.216873: step 3501, loss 0.113172, acc 0.96875\n",
      "2017-04-03T22:21:14.409224: step 3502, loss 0.11632, acc 0.953125\n",
      "2017-04-03T22:21:14.597376: step 3503, loss 0.0896915, acc 0.96875\n",
      "2017-04-03T22:21:14.786657: step 3504, loss 0.0513184, acc 0.984375\n",
      "2017-04-03T22:21:14.980475: step 3505, loss 0.0605488, acc 0.984375\n",
      "2017-04-03T22:21:15.170758: step 3506, loss 0.0499837, acc 0.984375\n",
      "2017-04-03T22:21:15.362147: step 3507, loss 0.0515622, acc 1\n",
      "2017-04-03T22:21:15.552460: step 3508, loss 0.0344083, acc 0.984375\n",
      "2017-04-03T22:21:15.748893: step 3509, loss 0.119104, acc 0.96875\n",
      "2017-04-03T22:21:15.940327: step 3510, loss 0.160503, acc 0.953125\n",
      "2017-04-03T22:21:16.129609: step 3511, loss 0.122543, acc 0.96875\n",
      "2017-04-03T22:21:16.321855: step 3512, loss 0.0735698, acc 0.984375\n",
      "2017-04-03T22:21:16.513338: step 3513, loss 0.0662429, acc 0.96875\n",
      "2017-04-03T22:21:16.708490: step 3514, loss 0.159027, acc 0.953125\n",
      "2017-04-03T22:21:16.900777: step 3515, loss 0.0385693, acc 1\n",
      "2017-04-03T22:21:17.092578: step 3516, loss 0.0522266, acc 0.984375\n",
      "2017-04-03T22:21:17.286322: step 3517, loss 0.160786, acc 0.96875\n",
      "2017-04-03T22:21:17.481833: step 3518, loss 0.117169, acc 0.96875\n",
      "2017-04-03T22:21:17.675461: step 3519, loss 0.102859, acc 0.96875\n",
      "2017-04-03T22:21:17.863362: step 3520, loss 0.0547558, acc 1\n",
      "2017-04-03T22:21:18.060863: step 3521, loss 0.0221767, acc 1\n",
      "2017-04-03T22:21:18.249426: step 3522, loss 0.0457739, acc 1\n",
      "2017-04-03T22:21:18.440734: step 3523, loss 0.152013, acc 0.96875\n",
      "2017-04-03T22:21:18.636643: step 3524, loss 0.0623663, acc 0.96875\n",
      "2017-04-03T22:21:18.829430: step 3525, loss 0.0349901, acc 1\n",
      "2017-04-03T22:21:19.023817: step 3526, loss 0.117973, acc 0.953125\n",
      "2017-04-03T22:21:19.217573: step 3527, loss 0.0566332, acc 0.984375\n",
      "2017-04-03T22:21:19.408072: step 3528, loss 0.0757778, acc 0.96875\n",
      "2017-04-03T22:21:19.600584: step 3529, loss 0.025002, acc 1\n",
      "2017-04-03T22:21:19.797341: step 3530, loss 0.0827865, acc 0.96875\n",
      "2017-04-03T22:21:19.997079: step 3531, loss 0.127517, acc 0.9375\n",
      "2017-04-03T22:21:20.192112: step 3532, loss 0.0990268, acc 0.984375\n",
      "2017-04-03T22:21:20.386587: step 3533, loss 0.0524854, acc 1\n",
      "2017-04-03T22:21:20.576630: step 3534, loss 0.137027, acc 0.953125\n",
      "2017-04-03T22:21:20.769959: step 3535, loss 0.125159, acc 0.96875\n",
      "2017-04-03T22:21:20.962075: step 3536, loss 0.0641423, acc 0.984375\n",
      "2017-04-03T22:21:21.150092: step 3537, loss 0.0873225, acc 0.984375\n",
      "2017-04-03T22:21:21.343413: step 3538, loss 0.0291358, acc 1\n",
      "2017-04-03T22:21:21.541599: step 3539, loss 0.0533654, acc 1\n",
      "2017-04-03T22:21:21.736944: step 3540, loss 0.14099, acc 0.953125\n",
      "2017-04-03T22:21:21.928225: step 3541, loss 0.0777796, acc 0.984375\n",
      "2017-04-03T22:21:22.122466: step 3542, loss 0.0641122, acc 0.984375\n",
      "2017-04-03T22:21:22.317078: step 3543, loss 0.101736, acc 0.953125\n",
      "2017-04-03T22:21:22.509521: step 3544, loss 0.0497973, acc 1\n",
      "2017-04-03T22:21:22.706699: step 3545, loss 0.0358946, acc 1\n",
      "2017-04-03T22:21:22.897678: step 3546, loss 0.0367065, acc 0.984375\n",
      "2017-04-03T22:21:23.089210: step 3547, loss 0.137464, acc 0.9375\n",
      "2017-04-03T22:21:23.281660: step 3548, loss 0.0605449, acc 1\n",
      "2017-04-03T22:21:23.476328: step 3549, loss 0.24271, acc 0.90625\n",
      "2017-04-03T22:21:23.671260: step 3550, loss 0.119504, acc 0.984375\n",
      "2017-04-03T22:21:23.866484: step 3551, loss 0.0702418, acc 0.953125\n",
      "2017-04-03T22:21:24.062634: step 3552, loss 0.0743065, acc 0.984375\n",
      "2017-04-03T22:21:24.255577: step 3553, loss 0.0631101, acc 0.984375\n",
      "2017-04-03T22:21:24.445672: step 3554, loss 0.0568082, acc 0.96875\n",
      "2017-04-03T22:21:24.635635: step 3555, loss 0.158465, acc 0.9375\n",
      "2017-04-03T22:21:24.828558: step 3556, loss 0.0682415, acc 0.96875\n",
      "2017-04-03T22:21:25.020959: step 3557, loss 0.0988344, acc 0.96875\n",
      "2017-04-03T22:21:25.213616: step 3558, loss 0.0229811, acc 1\n",
      "2017-04-03T22:21:25.405311: step 3559, loss 0.14436, acc 0.953125\n",
      "2017-04-03T22:21:25.598614: step 3560, loss 0.0727288, acc 0.953125\n",
      "2017-04-03T22:21:25.790955: step 3561, loss 0.0924697, acc 0.96875\n",
      "2017-04-03T22:21:25.982353: step 3562, loss 0.109839, acc 0.96875\n",
      "2017-04-03T22:21:26.172362: step 3563, loss 0.100818, acc 0.96875\n",
      "2017-04-03T22:21:26.334663: step 3564, loss 0.144518, acc 0.923077\n",
      "2017-04-03T22:21:26.527927: step 3565, loss 0.0773439, acc 0.984375\n",
      "2017-04-03T22:21:26.716578: step 3566, loss 0.0608543, acc 0.984375\n",
      "2017-04-03T22:21:26.911564: step 3567, loss 0.0247807, acc 1\n",
      "2017-04-03T22:21:27.116020: step 3568, loss 0.105691, acc 0.96875\n",
      "2017-04-03T22:21:27.307689: step 3569, loss 0.0671925, acc 0.984375\n",
      "2017-04-03T22:21:27.503259: step 3570, loss 0.0915093, acc 0.96875\n",
      "2017-04-03T22:21:27.710196: step 3571, loss 0.0882316, acc 0.984375\n",
      "2017-04-03T22:21:27.909570: step 3572, loss 0.0517126, acc 0.984375\n",
      "2017-04-03T22:21:28.120018: step 3573, loss 0.0858747, acc 1\n",
      "2017-04-03T22:21:28.320723: step 3574, loss 0.0779211, acc 0.984375\n",
      "2017-04-03T22:21:28.519670: step 3575, loss 0.0335872, acc 1\n",
      "2017-04-03T22:21:28.712839: step 3576, loss 0.135722, acc 0.953125\n",
      "2017-04-03T22:21:28.909869: step 3577, loss 0.0835261, acc 0.984375\n",
      "2017-04-03T22:21:29.105243: step 3578, loss 0.11655, acc 0.96875\n",
      "2017-04-03T22:21:29.298517: step 3579, loss 0.0838833, acc 0.984375\n",
      "2017-04-03T22:21:29.488265: step 3580, loss 0.0485671, acc 1\n",
      "2017-04-03T22:21:29.679279: step 3581, loss 0.0976225, acc 0.96875\n",
      "2017-04-03T22:21:29.878198: step 3582, loss 0.0827882, acc 0.96875\n",
      "2017-04-03T22:21:30.066449: step 3583, loss 0.11422, acc 0.96875\n",
      "2017-04-03T22:21:30.265796: step 3584, loss 0.0102437, acc 1\n",
      "2017-04-03T22:21:30.462752: step 3585, loss 0.0959635, acc 0.96875\n",
      "2017-04-03T22:21:30.657973: step 3586, loss 0.048777, acc 0.984375\n",
      "2017-04-03T22:21:30.853367: step 3587, loss 0.0990856, acc 0.953125\n",
      "2017-04-03T22:21:31.052148: step 3588, loss 0.0536079, acc 0.984375\n",
      "2017-04-03T22:21:31.250979: step 3589, loss 0.033751, acc 1\n",
      "2017-04-03T22:21:31.450637: step 3590, loss 0.117965, acc 0.96875\n",
      "2017-04-03T22:21:31.639939: step 3591, loss 0.0412014, acc 0.96875\n",
      "2017-04-03T22:21:31.836276: step 3592, loss 0.114941, acc 0.9375\n",
      "2017-04-03T22:21:32.027372: step 3593, loss 0.0615064, acc 0.984375\n",
      "2017-04-03T22:21:32.221952: step 3594, loss 0.072551, acc 0.984375\n",
      "2017-04-03T22:21:32.412729: step 3595, loss 0.110207, acc 0.96875\n",
      "2017-04-03T22:21:32.612177: step 3596, loss 0.0345328, acc 1\n",
      "2017-04-03T22:21:32.814664: step 3597, loss 0.0987987, acc 0.953125\n",
      "2017-04-03T22:21:33.021749: step 3598, loss 0.0699265, acc 0.96875\n",
      "2017-04-03T22:21:33.216463: step 3599, loss 0.102444, acc 0.96875\n",
      "2017-04-03T22:21:33.436328: step 3600, loss 0.105356, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:21:34.118173: step 3600, loss 1.76982, acc 0.569191\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3600\n",
      "\n",
      "2017-04-03T22:21:34.375223: step 3601, loss 0.0369281, acc 0.984375\n",
      "2017-04-03T22:21:34.566038: step 3602, loss 0.0854993, acc 0.96875\n",
      "2017-04-03T22:21:34.758293: step 3603, loss 0.165707, acc 0.96875\n",
      "2017-04-03T22:21:34.951016: step 3604, loss 0.0484969, acc 0.984375\n",
      "2017-04-03T22:21:35.146703: step 3605, loss 0.0400004, acc 1\n",
      "2017-04-03T22:21:35.340878: step 3606, loss 0.118141, acc 0.96875\n",
      "2017-04-03T22:21:35.532867: step 3607, loss 0.0412057, acc 0.984375\n",
      "2017-04-03T22:21:35.729664: step 3608, loss 0.0664946, acc 0.984375\n",
      "2017-04-03T22:21:35.922891: step 3609, loss 0.0396628, acc 1\n",
      "2017-04-03T22:21:36.117557: step 3610, loss 0.0796744, acc 0.96875\n",
      "2017-04-03T22:21:36.310960: step 3611, loss 0.0474979, acc 1\n",
      "2017-04-03T22:21:36.508189: step 3612, loss 0.0757136, acc 0.984375\n",
      "2017-04-03T22:21:36.699522: step 3613, loss 0.0917452, acc 0.953125\n",
      "2017-04-03T22:21:36.891833: step 3614, loss 0.0426098, acc 1\n",
      "2017-04-03T22:21:37.083040: step 3615, loss 0.0935405, acc 0.96875\n",
      "2017-04-03T22:21:37.279810: step 3616, loss 0.05339, acc 0.984375\n",
      "2017-04-03T22:21:37.472769: step 3617, loss 0.0404996, acc 0.984375\n",
      "2017-04-03T22:21:37.669134: step 3618, loss 0.0496251, acc 0.984375\n",
      "2017-04-03T22:21:37.859801: step 3619, loss 0.103476, acc 0.953125\n",
      "2017-04-03T22:21:38.053978: step 3620, loss 0.169031, acc 0.9375\n",
      "2017-04-03T22:21:38.249265: step 3621, loss 0.0651366, acc 0.984375\n",
      "2017-04-03T22:21:38.446395: step 3622, loss 0.0226437, acc 1\n",
      "2017-04-03T22:21:38.645105: step 3623, loss 0.102134, acc 0.96875\n",
      "2017-04-03T22:21:38.838626: step 3624, loss 0.0295765, acc 1\n",
      "2017-04-03T22:21:39.030595: step 3625, loss 0.0707972, acc 0.984375\n",
      "2017-04-03T22:21:39.223786: step 3626, loss 0.0559609, acc 0.984375\n",
      "2017-04-03T22:21:39.415055: step 3627, loss 0.0344504, acc 1\n",
      "2017-04-03T22:21:39.610184: step 3628, loss 0.0247879, acc 1\n",
      "2017-04-03T22:21:39.805523: step 3629, loss 0.0319686, acc 0.984375\n",
      "2017-04-03T22:21:40.001575: step 3630, loss 0.22086, acc 0.984375\n",
      "2017-04-03T22:21:40.192819: step 3631, loss 0.0913379, acc 0.984375\n",
      "2017-04-03T22:21:40.385499: step 3632, loss 0.0727761, acc 0.984375\n",
      "2017-04-03T22:21:40.577871: step 3633, loss 0.0539108, acc 0.984375\n",
      "2017-04-03T22:21:40.771701: step 3634, loss 0.119972, acc 0.96875\n",
      "2017-04-03T22:21:40.965747: step 3635, loss 0.102767, acc 0.984375\n",
      "2017-04-03T22:21:41.156994: step 3636, loss 0.0390583, acc 1\n",
      "2017-04-03T22:21:41.346369: step 3637, loss 0.101194, acc 0.96875\n",
      "2017-04-03T22:21:41.540605: step 3638, loss 0.13451, acc 0.96875\n",
      "2017-04-03T22:21:41.733936: step 3639, loss 0.0633439, acc 1\n",
      "2017-04-03T22:21:41.927163: step 3640, loss 0.118958, acc 0.953125\n",
      "2017-04-03T22:21:42.119644: step 3641, loss 0.0291948, acc 1\n",
      "2017-04-03T22:21:42.311970: step 3642, loss 0.0670095, acc 0.984375\n",
      "2017-04-03T22:21:42.505850: step 3643, loss 0.0179786, acc 1\n",
      "2017-04-03T22:21:42.694015: step 3644, loss 0.0689881, acc 0.984375\n",
      "2017-04-03T22:21:42.889280: step 3645, loss 0.0345172, acc 1\n",
      "2017-04-03T22:21:43.087458: step 3646, loss 0.0608663, acc 0.984375\n",
      "2017-04-03T22:21:43.285235: step 3647, loss 0.0837592, acc 0.984375\n",
      "2017-04-03T22:21:43.484213: step 3648, loss 0.118023, acc 0.96875\n",
      "2017-04-03T22:21:43.684447: step 3649, loss 0.0723938, acc 0.984375\n",
      "2017-04-03T22:21:43.897803: step 3650, loss 0.0358594, acc 1\n",
      "2017-04-03T22:21:44.107134: step 3651, loss 0.0636423, acc 0.984375\n",
      "2017-04-03T22:21:44.331201: step 3652, loss 0.0378828, acc 0.984375\n",
      "2017-04-03T22:21:44.521812: step 3653, loss 0.0693126, acc 0.984375\n",
      "2017-04-03T22:21:44.719270: step 3654, loss 0.0314873, acc 1\n",
      "2017-04-03T22:21:44.912539: step 3655, loss 0.0753714, acc 0.96875\n",
      "2017-04-03T22:21:45.105984: step 3656, loss 0.0321942, acc 1\n",
      "2017-04-03T22:21:45.300065: step 3657, loss 0.0885234, acc 0.984375\n",
      "2017-04-03T22:21:45.497162: step 3658, loss 0.0644339, acc 0.984375\n",
      "2017-04-03T22:21:45.687275: step 3659, loss 0.0546209, acc 1\n",
      "2017-04-03T22:21:45.883218: step 3660, loss 0.162498, acc 0.953125\n",
      "2017-04-03T22:21:46.076460: step 3661, loss 0.16982, acc 0.921875\n",
      "2017-04-03T22:21:46.272591: step 3662, loss 0.0541538, acc 0.984375\n",
      "2017-04-03T22:21:46.476524: step 3663, loss 0.0474869, acc 0.984375\n",
      "2017-04-03T22:21:46.672096: step 3664, loss 0.0307258, acc 1\n",
      "2017-04-03T22:21:46.868366: step 3665, loss 0.0554529, acc 0.984375\n",
      "2017-04-03T22:21:47.080598: step 3666, loss 0.0469692, acc 0.984375\n",
      "2017-04-03T22:21:47.295975: step 3667, loss 0.149194, acc 0.9375\n",
      "2017-04-03T22:21:47.504855: step 3668, loss 0.117004, acc 0.953125\n",
      "2017-04-03T22:21:47.715891: step 3669, loss 0.0697302, acc 0.984375\n",
      "2017-04-03T22:21:47.912505: step 3670, loss 0.0521022, acc 0.984375\n",
      "2017-04-03T22:21:48.110238: step 3671, loss 0.032865, acc 1\n",
      "2017-04-03T22:21:48.270953: step 3672, loss 0.0743526, acc 0.961538\n",
      "2017-04-03T22:21:48.464031: step 3673, loss 0.0995699, acc 0.96875\n",
      "2017-04-03T22:21:48.672292: step 3674, loss 0.0771292, acc 0.96875\n",
      "2017-04-03T22:21:48.867807: step 3675, loss 0.0766017, acc 0.984375\n",
      "2017-04-03T22:21:49.067639: step 3676, loss 0.0521232, acc 0.984375\n",
      "2017-04-03T22:21:49.273955: step 3677, loss 0.0542918, acc 1\n",
      "2017-04-03T22:21:49.467825: step 3678, loss 0.0408238, acc 1\n",
      "2017-04-03T22:21:49.676326: step 3679, loss 0.0541644, acc 0.984375\n",
      "2017-04-03T22:21:49.874267: step 3680, loss 0.0269276, acc 1\n",
      "2017-04-03T22:21:50.071269: step 3681, loss 0.0437312, acc 0.984375\n",
      "2017-04-03T22:21:50.271478: step 3682, loss 0.0732245, acc 0.984375\n",
      "2017-04-03T22:21:50.485323: step 3683, loss 0.0727858, acc 0.984375\n",
      "2017-04-03T22:21:50.691611: step 3684, loss 0.0413288, acc 1\n",
      "2017-04-03T22:21:50.887092: step 3685, loss 0.210721, acc 0.953125\n",
      "2017-04-03T22:21:51.082035: step 3686, loss 0.22477, acc 0.90625\n",
      "2017-04-03T22:21:51.272487: step 3687, loss 0.0827657, acc 0.984375\n",
      "2017-04-03T22:21:51.464139: step 3688, loss 0.103185, acc 0.96875\n",
      "2017-04-03T22:21:51.659881: step 3689, loss 0.025302, acc 1\n",
      "2017-04-03T22:21:51.858299: step 3690, loss 0.0924186, acc 0.96875\n",
      "2017-04-03T22:21:52.051623: step 3691, loss 0.0997473, acc 0.953125\n",
      "2017-04-03T22:21:52.240023: step 3692, loss 0.199581, acc 0.953125\n",
      "2017-04-03T22:21:52.445782: step 3693, loss 0.0217091, acc 1\n",
      "2017-04-03T22:21:52.653648: step 3694, loss 0.036104, acc 1\n",
      "2017-04-03T22:21:52.847609: step 3695, loss 0.0839368, acc 0.984375\n",
      "2017-04-03T22:21:53.044914: step 3696, loss 0.143278, acc 0.953125\n",
      "2017-04-03T22:21:53.239064: step 3697, loss 0.0434735, acc 0.984375\n",
      "2017-04-03T22:21:53.436516: step 3698, loss 0.134838, acc 0.9375\n",
      "2017-04-03T22:21:53.626408: step 3699, loss 0.0756261, acc 0.984375\n",
      "2017-04-03T22:21:53.816734: step 3700, loss 0.0409543, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:21:54.477816: step 3700, loss 1.77481, acc 0.571802\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3700\n",
      "\n",
      "2017-04-03T22:21:54.729516: step 3701, loss 0.0124417, acc 1\n",
      "2017-04-03T22:21:54.928179: step 3702, loss 0.0512864, acc 0.96875\n",
      "2017-04-03T22:21:55.126165: step 3703, loss 0.049643, acc 1\n",
      "2017-04-03T22:21:55.318281: step 3704, loss 0.102884, acc 0.984375\n",
      "2017-04-03T22:21:55.515988: step 3705, loss 0.0750584, acc 0.984375\n",
      "2017-04-03T22:21:55.713423: step 3706, loss 0.0413779, acc 1\n",
      "2017-04-03T22:21:55.902883: step 3707, loss 0.035706, acc 1\n",
      "2017-04-03T22:21:56.102361: step 3708, loss 0.167227, acc 0.9375\n",
      "2017-04-03T22:21:56.297068: step 3709, loss 0.060949, acc 0.984375\n",
      "2017-04-03T22:21:56.487439: step 3710, loss 0.051383, acc 0.984375\n",
      "2017-04-03T22:21:56.681710: step 3711, loss 0.0713285, acc 0.984375\n",
      "2017-04-03T22:21:56.894382: step 3712, loss 0.0493442, acc 0.984375\n",
      "2017-04-03T22:21:57.103535: step 3713, loss 0.0257232, acc 1\n",
      "2017-04-03T22:21:57.308552: step 3714, loss 0.1428, acc 0.953125\n",
      "2017-04-03T22:21:57.515767: step 3715, loss 0.0863647, acc 0.96875\n",
      "2017-04-03T22:21:57.712051: step 3716, loss 0.0963814, acc 0.953125\n",
      "2017-04-03T22:21:57.910282: step 3717, loss 0.0517259, acc 1\n",
      "2017-04-03T22:21:58.099330: step 3718, loss 0.0936611, acc 0.984375\n",
      "2017-04-03T22:21:58.294020: step 3719, loss 0.0343399, acc 1\n",
      "2017-04-03T22:21:58.488369: step 3720, loss 0.0560656, acc 1\n",
      "2017-04-03T22:21:58.681546: step 3721, loss 0.0816199, acc 0.96875\n",
      "2017-04-03T22:21:58.879942: step 3722, loss 0.185114, acc 0.921875\n",
      "2017-04-03T22:21:59.074647: step 3723, loss 0.0333991, acc 0.984375\n",
      "2017-04-03T22:21:59.268273: step 3724, loss 0.0461019, acc 1\n",
      "2017-04-03T22:21:59.458388: step 3725, loss 0.116043, acc 0.96875\n",
      "2017-04-03T22:21:59.650678: step 3726, loss 0.0937868, acc 0.953125\n",
      "2017-04-03T22:21:59.840308: step 3727, loss 0.110887, acc 0.96875\n",
      "2017-04-03T22:22:00.030946: step 3728, loss 0.0436771, acc 0.96875\n",
      "2017-04-03T22:22:00.224320: step 3729, loss 0.0671311, acc 0.984375\n",
      "2017-04-03T22:22:00.417247: step 3730, loss 0.0535753, acc 0.984375\n",
      "2017-04-03T22:22:00.612569: step 3731, loss 0.0723447, acc 0.984375\n",
      "2017-04-03T22:22:00.805887: step 3732, loss 0.018167, acc 1\n",
      "2017-04-03T22:22:00.999166: step 3733, loss 0.0366091, acc 0.984375\n",
      "2017-04-03T22:22:01.190265: step 3734, loss 0.029339, acc 1\n",
      "2017-04-03T22:22:01.383625: step 3735, loss 0.0217207, acc 1\n",
      "2017-04-03T22:22:01.574705: step 3736, loss 0.0173793, acc 1\n",
      "2017-04-03T22:22:01.770580: step 3737, loss 0.0532437, acc 0.984375\n",
      "2017-04-03T22:22:01.961563: step 3738, loss 0.0715519, acc 0.984375\n",
      "2017-04-03T22:22:02.151917: step 3739, loss 0.087182, acc 0.96875\n",
      "2017-04-03T22:22:02.345039: step 3740, loss 0.0684884, acc 0.984375\n",
      "2017-04-03T22:22:02.535183: step 3741, loss 0.0443575, acc 1\n",
      "2017-04-03T22:22:02.726831: step 3742, loss 0.134352, acc 0.953125\n",
      "2017-04-03T22:22:02.919783: step 3743, loss 0.0610377, acc 0.984375\n",
      "2017-04-03T22:22:03.110999: step 3744, loss 0.0721828, acc 0.96875\n",
      "2017-04-03T22:22:03.305037: step 3745, loss 0.078819, acc 0.984375\n",
      "2017-04-03T22:22:03.499359: step 3746, loss 0.113144, acc 0.953125\n",
      "2017-04-03T22:22:03.693125: step 3747, loss 0.0716237, acc 0.984375\n",
      "2017-04-03T22:22:03.882063: step 3748, loss 0.0967807, acc 0.984375\n",
      "2017-04-03T22:22:04.071706: step 3749, loss 0.0290452, acc 1\n",
      "2017-04-03T22:22:04.268625: step 3750, loss 0.0889567, acc 0.96875\n",
      "2017-04-03T22:22:04.461360: step 3751, loss 0.0881326, acc 0.96875\n",
      "2017-04-03T22:22:04.653708: step 3752, loss 0.104593, acc 0.96875\n",
      "2017-04-03T22:22:04.847907: step 3753, loss 0.087771, acc 0.953125\n",
      "2017-04-03T22:22:05.040955: step 3754, loss 0.0861623, acc 0.984375\n",
      "2017-04-03T22:22:05.235956: step 3755, loss 0.0553235, acc 0.984375\n",
      "2017-04-03T22:22:05.427257: step 3756, loss 0.0156799, acc 1\n",
      "2017-04-03T22:22:05.621687: step 3757, loss 0.101227, acc 0.96875\n",
      "2017-04-03T22:22:05.823045: step 3758, loss 0.0487883, acc 1\n",
      "2017-04-03T22:22:06.016183: step 3759, loss 0.128451, acc 0.984375\n",
      "2017-04-03T22:22:06.206154: step 3760, loss 0.0753158, acc 0.984375\n",
      "2017-04-03T22:22:06.396796: step 3761, loss 0.142044, acc 0.9375\n",
      "2017-04-03T22:22:06.590203: step 3762, loss 0.158586, acc 0.953125\n",
      "2017-04-03T22:22:06.783438: step 3763, loss 0.0745413, acc 0.984375\n",
      "2017-04-03T22:22:06.977588: step 3764, loss 0.0974424, acc 0.96875\n",
      "2017-04-03T22:22:07.170647: step 3765, loss 0.0785173, acc 0.984375\n",
      "2017-04-03T22:22:07.367357: step 3766, loss 0.126005, acc 0.96875\n",
      "2017-04-03T22:22:07.557726: step 3767, loss 0.0202357, acc 1\n",
      "2017-04-03T22:22:07.751556: step 3768, loss 0.0673209, acc 0.96875\n",
      "2017-04-03T22:22:07.943682: step 3769, loss 0.0602614, acc 0.984375\n",
      "2017-04-03T22:22:08.138590: step 3770, loss 0.078113, acc 0.984375\n",
      "2017-04-03T22:22:08.329313: step 3771, loss 0.0313448, acc 1\n",
      "2017-04-03T22:22:08.521612: step 3772, loss 0.0807712, acc 0.96875\n",
      "2017-04-03T22:22:08.711836: step 3773, loss 0.0755124, acc 0.984375\n",
      "2017-04-03T22:22:08.904580: step 3774, loss 0.10683, acc 0.96875\n",
      "2017-04-03T22:22:09.091933: step 3775, loss 0.141788, acc 0.953125\n",
      "2017-04-03T22:22:09.285250: step 3776, loss 0.0929874, acc 0.984375\n",
      "2017-04-03T22:22:09.476974: step 3777, loss 0.0621991, acc 0.984375\n",
      "2017-04-03T22:22:09.669856: step 3778, loss 0.042831, acc 0.984375\n",
      "2017-04-03T22:22:09.863520: step 3779, loss 0.108069, acc 0.96875\n",
      "2017-04-03T22:22:10.025607: step 3780, loss 0.0590801, acc 0.980769\n",
      "2017-04-03T22:22:10.217114: step 3781, loss 0.131274, acc 0.96875\n",
      "2017-04-03T22:22:10.410493: step 3782, loss 0.0614567, acc 1\n",
      "2017-04-03T22:22:10.603309: step 3783, loss 0.0388956, acc 1\n",
      "2017-04-03T22:22:10.797917: step 3784, loss 0.0515763, acc 0.984375\n",
      "2017-04-03T22:22:10.991804: step 3785, loss 0.0250942, acc 1\n",
      "2017-04-03T22:22:11.182229: step 3786, loss 0.0539186, acc 0.984375\n",
      "2017-04-03T22:22:11.374663: step 3787, loss 0.0438199, acc 0.984375\n",
      "2017-04-03T22:22:11.568487: step 3788, loss 0.0343115, acc 1\n",
      "2017-04-03T22:22:11.760913: step 3789, loss 0.0835811, acc 0.984375\n",
      "2017-04-03T22:22:11.955839: step 3790, loss 0.0691054, acc 0.953125\n",
      "2017-04-03T22:22:12.145502: step 3791, loss 0.086665, acc 0.984375\n",
      "2017-04-03T22:22:12.339012: step 3792, loss 0.044436, acc 1\n",
      "2017-04-03T22:22:12.531649: step 3793, loss 0.0798823, acc 0.9375\n",
      "2017-04-03T22:22:12.723880: step 3794, loss 0.134628, acc 0.953125\n",
      "2017-04-03T22:22:12.921851: step 3795, loss 0.0972102, acc 0.953125\n",
      "2017-04-03T22:22:13.118207: step 3796, loss 0.071255, acc 0.96875\n",
      "2017-04-03T22:22:13.309297: step 3797, loss 0.0484846, acc 0.984375\n",
      "2017-04-03T22:22:13.501100: step 3798, loss 0.0182433, acc 1\n",
      "2017-04-03T22:22:13.701437: step 3799, loss 0.0803022, acc 0.96875\n",
      "2017-04-03T22:22:13.901938: step 3800, loss 0.0817393, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:22:14.556439: step 3800, loss 1.8187, acc 0.562663\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3800\n",
      "\n",
      "2017-04-03T22:22:14.808150: step 3801, loss 0.0091766, acc 1\n",
      "2017-04-03T22:22:14.999710: step 3802, loss 0.0940511, acc 0.984375\n",
      "2017-04-03T22:22:15.194444: step 3803, loss 0.0950458, acc 0.953125\n",
      "2017-04-03T22:22:15.391087: step 3804, loss 0.0540948, acc 0.96875\n",
      "2017-04-03T22:22:15.586464: step 3805, loss 0.0137465, acc 1\n",
      "2017-04-03T22:22:15.776938: step 3806, loss 0.164518, acc 0.9375\n",
      "2017-04-03T22:22:15.968631: step 3807, loss 0.0617798, acc 0.984375\n",
      "2017-04-03T22:22:16.163910: step 3808, loss 0.073885, acc 0.96875\n",
      "2017-04-03T22:22:16.355645: step 3809, loss 0.141463, acc 0.9375\n",
      "2017-04-03T22:22:16.552179: step 3810, loss 0.102846, acc 0.96875\n",
      "2017-04-03T22:22:16.742775: step 3811, loss 0.0869435, acc 0.96875\n",
      "2017-04-03T22:22:16.934609: step 3812, loss 0.124651, acc 0.96875\n",
      "2017-04-03T22:22:17.128188: step 3813, loss 0.0396309, acc 1\n",
      "2017-04-03T22:22:17.322780: step 3814, loss 0.0646164, acc 0.984375\n",
      "2017-04-03T22:22:17.512698: step 3815, loss 0.038236, acc 1\n",
      "2017-04-03T22:22:17.706847: step 3816, loss 0.103597, acc 0.953125\n",
      "2017-04-03T22:22:17.899467: step 3817, loss 0.0478292, acc 0.984375\n",
      "2017-04-03T22:22:18.092034: step 3818, loss 0.0557642, acc 0.984375\n",
      "2017-04-03T22:22:18.284362: step 3819, loss 0.0394913, acc 1\n",
      "2017-04-03T22:22:18.476688: step 3820, loss 0.0856535, acc 0.984375\n",
      "2017-04-03T22:22:18.674497: step 3821, loss 0.0713308, acc 0.984375\n",
      "2017-04-03T22:22:18.868866: step 3822, loss 0.0876132, acc 0.953125\n",
      "2017-04-03T22:22:19.058462: step 3823, loss 0.0289258, acc 1\n",
      "2017-04-03T22:22:19.247809: step 3824, loss 0.0464043, acc 0.984375\n",
      "2017-04-03T22:22:19.443355: step 3825, loss 0.0507912, acc 0.984375\n",
      "2017-04-03T22:22:19.637740: step 3826, loss 0.0498169, acc 0.984375\n",
      "2017-04-03T22:22:19.831854: step 3827, loss 0.0987141, acc 0.953125\n",
      "2017-04-03T22:22:20.024631: step 3828, loss 0.0810177, acc 0.984375\n",
      "2017-04-03T22:22:20.220131: step 3829, loss 0.0622902, acc 1\n",
      "2017-04-03T22:22:20.411232: step 3830, loss 0.123255, acc 0.96875\n",
      "2017-04-03T22:22:20.605900: step 3831, loss 0.0738633, acc 0.96875\n",
      "2017-04-03T22:22:20.792705: step 3832, loss 0.0368846, acc 0.984375\n",
      "2017-04-03T22:22:20.989097: step 3833, loss 0.0571976, acc 0.984375\n",
      "2017-04-03T22:22:21.178937: step 3834, loss 0.0837688, acc 0.96875\n",
      "2017-04-03T22:22:21.373930: step 3835, loss 0.0537388, acc 0.984375\n",
      "2017-04-03T22:22:21.567359: step 3836, loss 0.0836203, acc 0.96875\n",
      "2017-04-03T22:22:21.759799: step 3837, loss 0.103523, acc 0.953125\n",
      "2017-04-03T22:22:21.952166: step 3838, loss 0.133811, acc 0.9375\n",
      "2017-04-03T22:22:22.144630: step 3839, loss 0.116291, acc 0.953125\n",
      "2017-04-03T22:22:22.338524: step 3840, loss 0.0357471, acc 1\n",
      "2017-04-03T22:22:22.534206: step 3841, loss 0.0971184, acc 0.96875\n",
      "2017-04-03T22:22:22.731649: step 3842, loss 0.0750686, acc 1\n",
      "2017-04-03T22:22:22.924661: step 3843, loss 0.0797961, acc 0.96875\n",
      "2017-04-03T22:22:23.117608: step 3844, loss 0.110397, acc 0.96875\n",
      "2017-04-03T22:22:23.306291: step 3845, loss 0.0557632, acc 0.96875\n",
      "2017-04-03T22:22:23.497993: step 3846, loss 0.198866, acc 0.984375\n",
      "2017-04-03T22:22:23.691978: step 3847, loss 0.0303839, acc 1\n",
      "2017-04-03T22:22:23.884643: step 3848, loss 0.0365091, acc 0.984375\n",
      "2017-04-03T22:22:24.076242: step 3849, loss 0.106399, acc 0.984375\n",
      "2017-04-03T22:22:24.268932: step 3850, loss 0.0595894, acc 0.984375\n",
      "2017-04-03T22:22:24.465982: step 3851, loss 0.102863, acc 0.96875\n",
      "2017-04-03T22:22:24.659129: step 3852, loss 0.0511374, acc 0.96875\n",
      "2017-04-03T22:22:24.854037: step 3853, loss 0.054047, acc 0.984375\n",
      "2017-04-03T22:22:25.046044: step 3854, loss 0.117288, acc 0.96875\n",
      "2017-04-03T22:22:25.244562: step 3855, loss 0.0655514, acc 0.984375\n",
      "2017-04-03T22:22:25.439467: step 3856, loss 0.119261, acc 0.953125\n",
      "2017-04-03T22:22:25.636059: step 3857, loss 0.0602839, acc 0.96875\n",
      "2017-04-03T22:22:25.838638: step 3858, loss 0.124056, acc 0.953125\n",
      "2017-04-03T22:22:26.029442: step 3859, loss 0.0464652, acc 1\n",
      "2017-04-03T22:22:26.221352: step 3860, loss 0.0518979, acc 0.984375\n",
      "2017-04-03T22:22:26.416594: step 3861, loss 0.0964516, acc 0.953125\n",
      "2017-04-03T22:22:26.624324: step 3862, loss 0.105901, acc 0.984375\n",
      "2017-04-03T22:22:26.816215: step 3863, loss 0.0239473, acc 1\n",
      "2017-04-03T22:22:27.011968: step 3864, loss 0.0474163, acc 0.984375\n",
      "2017-04-03T22:22:27.205792: step 3865, loss 0.0478888, acc 1\n",
      "2017-04-03T22:22:27.399539: step 3866, loss 0.0901624, acc 0.953125\n",
      "2017-04-03T22:22:27.589583: step 3867, loss 0.0613612, acc 0.984375\n",
      "2017-04-03T22:22:27.779181: step 3868, loss 0.0678692, acc 0.984375\n",
      "2017-04-03T22:22:27.968187: step 3869, loss 0.0219402, acc 1\n",
      "2017-04-03T22:22:28.158868: step 3870, loss 0.0745819, acc 0.984375\n",
      "2017-04-03T22:22:28.351645: step 3871, loss 0.0809097, acc 0.96875\n",
      "2017-04-03T22:22:28.553211: step 3872, loss 0.0669158, acc 0.96875\n",
      "2017-04-03T22:22:28.751383: step 3873, loss 0.033521, acc 1\n",
      "2017-04-03T22:22:28.947260: step 3874, loss 0.089638, acc 0.953125\n",
      "2017-04-03T22:22:29.140040: step 3875, loss 0.0525542, acc 1\n",
      "2017-04-03T22:22:29.335803: step 3876, loss 0.142755, acc 0.953125\n",
      "2017-04-03T22:22:29.525028: step 3877, loss 0.0789925, acc 0.984375\n",
      "2017-04-03T22:22:29.716588: step 3878, loss 0.0492804, acc 0.984375\n",
      "2017-04-03T22:22:29.905566: step 3879, loss 0.0790335, acc 0.96875\n",
      "2017-04-03T22:22:30.099603: step 3880, loss 0.0647762, acc 0.96875\n",
      "2017-04-03T22:22:30.293066: step 3881, loss 0.0323968, acc 1\n",
      "2017-04-03T22:22:30.489104: step 3882, loss 0.0706633, acc 0.96875\n",
      "2017-04-03T22:22:30.690761: step 3883, loss 0.0232392, acc 1\n",
      "2017-04-03T22:22:30.889804: step 3884, loss 0.0992137, acc 0.984375\n",
      "2017-04-03T22:22:31.083855: step 3885, loss 0.0199748, acc 1\n",
      "2017-04-03T22:22:31.278573: step 3886, loss 0.0519522, acc 1\n",
      "2017-04-03T22:22:31.476610: step 3887, loss 0.156976, acc 0.9375\n",
      "2017-04-03T22:22:31.638307: step 3888, loss 0.043758, acc 1\n",
      "2017-04-03T22:22:31.832143: step 3889, loss 0.165954, acc 0.9375\n",
      "2017-04-03T22:22:32.025528: step 3890, loss 0.0299139, acc 1\n",
      "2017-04-03T22:22:32.218303: step 3891, loss 0.0555531, acc 0.96875\n",
      "2017-04-03T22:22:32.416057: step 3892, loss 0.0892326, acc 0.96875\n",
      "2017-04-03T22:22:32.608606: step 3893, loss 0.0852047, acc 0.984375\n",
      "2017-04-03T22:22:32.797752: step 3894, loss 0.027102, acc 1\n",
      "2017-04-03T22:22:32.991631: step 3895, loss 0.0612614, acc 0.984375\n",
      "2017-04-03T22:22:33.177951: step 3896, loss 0.0797676, acc 0.984375\n",
      "2017-04-03T22:22:33.370846: step 3897, loss 0.0245228, acc 1\n",
      "2017-04-03T22:22:33.558898: step 3898, loss 0.0493324, acc 1\n",
      "2017-04-03T22:22:33.750510: step 3899, loss 0.16805, acc 0.921875\n",
      "2017-04-03T22:22:33.941019: step 3900, loss 0.118565, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:22:34.595907: step 3900, loss 1.80745, acc 0.573107\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-3900\n",
      "\n",
      "2017-04-03T22:22:34.854774: step 3901, loss 0.0795354, acc 0.96875\n",
      "2017-04-03T22:22:35.044984: step 3902, loss 0.0247124, acc 1\n",
      "2017-04-03T22:22:35.235434: step 3903, loss 0.0167656, acc 1\n",
      "2017-04-03T22:22:35.425601: step 3904, loss 0.125445, acc 0.953125\n",
      "2017-04-03T22:22:35.617575: step 3905, loss 0.0344557, acc 1\n",
      "2017-04-03T22:22:35.811202: step 3906, loss 0.0401901, acc 0.984375\n",
      "2017-04-03T22:22:36.004079: step 3907, loss 0.153081, acc 0.9375\n",
      "2017-04-03T22:22:36.197275: step 3908, loss 0.0244351, acc 1\n",
      "2017-04-03T22:22:36.391081: step 3909, loss 0.118235, acc 0.9375\n",
      "2017-04-03T22:22:36.581251: step 3910, loss 0.0368664, acc 1\n",
      "2017-04-03T22:22:36.776473: step 3911, loss 0.0819499, acc 0.96875\n",
      "2017-04-03T22:22:36.967102: step 3912, loss 0.0337511, acc 0.984375\n",
      "2017-04-03T22:22:37.157876: step 3913, loss 0.125418, acc 0.953125\n",
      "2017-04-03T22:22:37.350231: step 3914, loss 0.171729, acc 0.9375\n",
      "2017-04-03T22:22:37.545716: step 3915, loss 0.0155437, acc 1\n",
      "2017-04-03T22:22:37.741934: step 3916, loss 0.0515216, acc 0.984375\n",
      "2017-04-03T22:22:37.933951: step 3917, loss 0.0807728, acc 0.96875\n",
      "2017-04-03T22:22:38.123711: step 3918, loss 0.0663533, acc 0.984375\n",
      "2017-04-03T22:22:38.316378: step 3919, loss 0.0284552, acc 0.984375\n",
      "2017-04-03T22:22:38.506499: step 3920, loss 0.0644284, acc 0.984375\n",
      "2017-04-03T22:22:38.706439: step 3921, loss 0.112561, acc 0.96875\n",
      "2017-04-03T22:22:38.900860: step 3922, loss 0.0979925, acc 0.96875\n",
      "2017-04-03T22:22:39.093603: step 3923, loss 0.239743, acc 0.90625\n",
      "2017-04-03T22:22:39.283580: step 3924, loss 0.0756085, acc 0.984375\n",
      "2017-04-03T22:22:39.473726: step 3925, loss 0.0499471, acc 0.984375\n",
      "2017-04-03T22:22:39.668363: step 3926, loss 0.0245276, acc 1\n",
      "2017-04-03T22:22:39.859037: step 3927, loss 0.0561388, acc 0.984375\n",
      "2017-04-03T22:22:40.050903: step 3928, loss 0.0334701, acc 0.984375\n",
      "2017-04-03T22:22:40.242669: step 3929, loss 0.106425, acc 0.9375\n",
      "2017-04-03T22:22:40.435930: step 3930, loss 0.0473856, acc 0.984375\n",
      "2017-04-03T22:22:40.628489: step 3931, loss 0.0667865, acc 0.984375\n",
      "2017-04-03T22:22:40.819360: step 3932, loss 0.0664508, acc 0.984375\n",
      "2017-04-03T22:22:41.012766: step 3933, loss 0.080163, acc 0.96875\n",
      "2017-04-03T22:22:41.204772: step 3934, loss 0.0113963, acc 1\n",
      "2017-04-03T22:22:41.399934: step 3935, loss 0.0139947, acc 1\n",
      "2017-04-03T22:22:41.592785: step 3936, loss 0.0330852, acc 0.984375\n",
      "2017-04-03T22:22:41.787607: step 3937, loss 0.0164163, acc 1\n",
      "2017-04-03T22:22:41.982382: step 3938, loss 0.0999737, acc 0.96875\n",
      "2017-04-03T22:22:42.178515: step 3939, loss 0.0277681, acc 0.984375\n",
      "2017-04-03T22:22:42.370217: step 3940, loss 0.0385867, acc 0.984375\n",
      "2017-04-03T22:22:42.559459: step 3941, loss 0.0935948, acc 0.96875\n",
      "2017-04-03T22:22:42.747042: step 3942, loss 0.0613891, acc 1\n",
      "2017-04-03T22:22:42.938072: step 3943, loss 0.0539348, acc 0.984375\n",
      "2017-04-03T22:22:43.133123: step 3944, loss 0.0534645, acc 0.984375\n",
      "2017-04-03T22:22:43.322973: step 3945, loss 0.0502527, acc 0.984375\n",
      "2017-04-03T22:22:43.513363: step 3946, loss 0.0839785, acc 0.984375\n",
      "2017-04-03T22:22:43.703019: step 3947, loss 0.0130867, acc 1\n",
      "2017-04-03T22:22:43.894213: step 3948, loss 0.0711723, acc 0.984375\n",
      "2017-04-03T22:22:44.088380: step 3949, loss 0.0427872, acc 1\n",
      "2017-04-03T22:22:44.278131: step 3950, loss 0.0733823, acc 0.984375\n",
      "2017-04-03T22:22:44.476094: step 3951, loss 0.0138169, acc 1\n",
      "2017-04-03T22:22:44.667697: step 3952, loss 0.0724521, acc 0.96875\n",
      "2017-04-03T22:22:44.860380: step 3953, loss 0.0920295, acc 0.953125\n",
      "2017-04-03T22:22:45.055778: step 3954, loss 0.100373, acc 0.984375\n",
      "2017-04-03T22:22:45.256802: step 3955, loss 0.0723765, acc 0.984375\n",
      "2017-04-03T22:22:45.447848: step 3956, loss 0.0545546, acc 0.984375\n",
      "2017-04-03T22:22:45.639638: step 3957, loss 0.0799272, acc 0.96875\n",
      "2017-04-03T22:22:45.828018: step 3958, loss 0.129929, acc 0.953125\n",
      "2017-04-03T22:22:46.021914: step 3959, loss 0.0215107, acc 1\n",
      "2017-04-03T22:22:46.215945: step 3960, loss 0.137248, acc 0.953125\n",
      "2017-04-03T22:22:46.408264: step 3961, loss 0.0497805, acc 0.96875\n",
      "2017-04-03T22:22:46.604793: step 3962, loss 0.0960462, acc 0.953125\n",
      "2017-04-03T22:22:46.798551: step 3963, loss 0.0227662, acc 1\n",
      "2017-04-03T22:22:46.990556: step 3964, loss 0.137106, acc 0.9375\n",
      "2017-04-03T22:22:47.183280: step 3965, loss 0.052249, acc 0.984375\n",
      "2017-04-03T22:22:47.377845: step 3966, loss 0.0844626, acc 0.984375\n",
      "2017-04-03T22:22:47.572672: step 3967, loss 0.14608, acc 0.953125\n",
      "2017-04-03T22:22:47.764062: step 3968, loss 0.0646606, acc 0.984375\n",
      "2017-04-03T22:22:47.958293: step 3969, loss 0.0123023, acc 1\n",
      "2017-04-03T22:22:48.170429: step 3970, loss 0.0364816, acc 1\n",
      "2017-04-03T22:22:48.363783: step 3971, loss 0.0856992, acc 0.96875\n",
      "2017-04-03T22:22:48.556064: step 3972, loss 0.0610412, acc 0.984375\n",
      "2017-04-03T22:22:48.748343: step 3973, loss 0.0509832, acc 1\n",
      "2017-04-03T22:22:48.940690: step 3974, loss 0.0210573, acc 1\n",
      "2017-04-03T22:22:49.136129: step 3975, loss 0.0469994, acc 0.984375\n",
      "2017-04-03T22:22:49.328852: step 3976, loss 0.0983552, acc 0.953125\n",
      "2017-04-03T22:22:49.525435: step 3977, loss 0.150092, acc 0.9375\n",
      "2017-04-03T22:22:49.716256: step 3978, loss 0.0775416, acc 0.984375\n",
      "2017-04-03T22:22:49.908389: step 3979, loss 0.0529099, acc 0.984375\n",
      "2017-04-03T22:22:50.105038: step 3980, loss 0.102989, acc 0.96875\n",
      "2017-04-03T22:22:50.292997: step 3981, loss 0.119524, acc 0.9375\n",
      "2017-04-03T22:22:50.484208: step 3982, loss 0.101687, acc 0.984375\n",
      "2017-04-03T22:22:50.677140: step 3983, loss 0.0809354, acc 1\n",
      "2017-04-03T22:22:50.871936: step 3984, loss 0.0412143, acc 1\n",
      "2017-04-03T22:22:51.064351: step 3985, loss 0.0585044, acc 0.984375\n",
      "2017-04-03T22:22:51.259673: step 3986, loss 0.0177845, acc 1\n",
      "2017-04-03T22:22:51.454263: step 3987, loss 0.0827225, acc 0.984375\n",
      "2017-04-03T22:22:51.644112: step 3988, loss 0.12574, acc 0.96875\n",
      "2017-04-03T22:22:51.842340: step 3989, loss 0.0665535, acc 0.984375\n",
      "2017-04-03T22:22:52.040993: step 3990, loss 0.0481128, acc 0.984375\n",
      "2017-04-03T22:22:52.236683: step 3991, loss 0.0553473, acc 0.984375\n",
      "2017-04-03T22:22:52.437911: step 3992, loss 0.0435163, acc 0.984375\n",
      "2017-04-03T22:22:52.632653: step 3993, loss 0.140148, acc 0.9375\n",
      "2017-04-03T22:22:52.828750: step 3994, loss 0.145901, acc 0.953125\n",
      "2017-04-03T22:22:53.029246: step 3995, loss 0.0529106, acc 0.984375\n",
      "2017-04-03T22:22:53.195724: step 3996, loss 0.031319, acc 1\n",
      "2017-04-03T22:22:53.396878: step 3997, loss 0.0578367, acc 0.984375\n",
      "2017-04-03T22:22:53.595385: step 3998, loss 0.0109983, acc 1\n",
      "2017-04-03T22:22:53.793578: step 3999, loss 0.0655067, acc 0.984375\n",
      "2017-04-03T22:22:53.994780: step 4000, loss 0.0363537, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:22:54.645023: step 4000, loss 1.81827, acc 0.565274\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4000\n",
      "\n",
      "2017-04-03T22:22:54.893376: step 4001, loss 0.0413768, acc 0.984375\n",
      "2017-04-03T22:22:55.087699: step 4002, loss 0.0551804, acc 0.984375\n",
      "2017-04-03T22:22:55.284565: step 4003, loss 0.083158, acc 0.984375\n",
      "2017-04-03T22:22:55.476417: step 4004, loss 0.0305592, acc 0.984375\n",
      "2017-04-03T22:22:55.669110: step 4005, loss 0.0559736, acc 0.984375\n",
      "2017-04-03T22:22:55.860138: step 4006, loss 0.0337731, acc 1\n",
      "2017-04-03T22:22:56.063547: step 4007, loss 0.131427, acc 0.953125\n",
      "2017-04-03T22:22:56.271023: step 4008, loss 0.0374891, acc 1\n",
      "2017-04-03T22:22:56.460740: step 4009, loss 0.0211292, acc 1\n",
      "2017-04-03T22:22:56.651377: step 4010, loss 0.0242635, acc 1\n",
      "2017-04-03T22:22:56.845023: step 4011, loss 0.187395, acc 0.96875\n",
      "2017-04-03T22:22:57.039538: step 4012, loss 0.0326938, acc 0.984375\n",
      "2017-04-03T22:22:57.230502: step 4013, loss 0.0506702, acc 0.984375\n",
      "2017-04-03T22:22:57.426058: step 4014, loss 0.0347102, acc 1\n",
      "2017-04-03T22:22:57.616318: step 4015, loss 0.0956577, acc 0.953125\n",
      "2017-04-03T22:22:57.808591: step 4016, loss 0.07597, acc 0.96875\n",
      "2017-04-03T22:22:58.001317: step 4017, loss 0.0565658, acc 0.984375\n",
      "2017-04-03T22:22:58.191243: step 4018, loss 0.0186303, acc 1\n",
      "2017-04-03T22:22:58.387875: step 4019, loss 0.13012, acc 0.96875\n",
      "2017-04-03T22:22:58.581998: step 4020, loss 0.102358, acc 0.953125\n",
      "2017-04-03T22:22:58.776300: step 4021, loss 0.04599, acc 0.984375\n",
      "2017-04-03T22:22:58.970064: step 4022, loss 0.122615, acc 0.953125\n",
      "2017-04-03T22:22:59.158238: step 4023, loss 0.0492885, acc 1\n",
      "2017-04-03T22:22:59.355226: step 4024, loss 0.134713, acc 0.953125\n",
      "2017-04-03T22:22:59.548608: step 4025, loss 0.120734, acc 0.953125\n",
      "2017-04-03T22:22:59.744055: step 4026, loss 0.0344045, acc 1\n",
      "2017-04-03T22:22:59.939963: step 4027, loss 0.0399703, acc 1\n",
      "2017-04-03T22:23:00.132333: step 4028, loss 0.149282, acc 0.921875\n",
      "2017-04-03T22:23:00.329794: step 4029, loss 0.0279342, acc 1\n",
      "2017-04-03T22:23:00.527560: step 4030, loss 0.112704, acc 0.9375\n",
      "2017-04-03T22:23:00.723694: step 4031, loss 0.0605413, acc 1\n",
      "2017-04-03T22:23:00.923531: step 4032, loss 0.024036, acc 1\n",
      "2017-04-03T22:23:01.120971: step 4033, loss 0.0253216, acc 1\n",
      "2017-04-03T22:23:01.317545: step 4034, loss 0.103869, acc 0.984375\n",
      "2017-04-03T22:23:01.515239: step 4035, loss 0.0622395, acc 0.984375\n",
      "2017-04-03T22:23:01.707772: step 4036, loss 0.0109085, acc 1\n",
      "2017-04-03T22:23:01.905205: step 4037, loss 0.0310042, acc 0.984375\n",
      "2017-04-03T22:23:02.099248: step 4038, loss 0.0392713, acc 0.984375\n",
      "2017-04-03T22:23:02.292324: step 4039, loss 0.109542, acc 0.953125\n",
      "2017-04-03T22:23:02.482315: step 4040, loss 0.0534841, acc 0.984375\n",
      "2017-04-03T22:23:02.689708: step 4041, loss 0.0462335, acc 0.984375\n",
      "2017-04-03T22:23:02.896043: step 4042, loss 0.022791, acc 1\n",
      "2017-04-03T22:23:03.087603: step 4043, loss 0.0800504, acc 0.96875\n",
      "2017-04-03T22:23:03.284154: step 4044, loss 0.0426219, acc 0.984375\n",
      "2017-04-03T22:23:03.476235: step 4045, loss 0.0526592, acc 0.984375\n",
      "2017-04-03T22:23:03.672228: step 4046, loss 0.072334, acc 0.984375\n",
      "2017-04-03T22:23:03.865269: step 4047, loss 0.0747191, acc 0.953125\n",
      "2017-04-03T22:23:04.058997: step 4048, loss 0.0614187, acc 0.984375\n",
      "2017-04-03T22:23:04.253226: step 4049, loss 0.0624793, acc 0.984375\n",
      "2017-04-03T22:23:04.447382: step 4050, loss 0.0683392, acc 0.96875\n",
      "2017-04-03T22:23:04.641102: step 4051, loss 0.0678873, acc 0.96875\n",
      "2017-04-03T22:23:04.837414: step 4052, loss 0.0458833, acc 0.984375\n",
      "2017-04-03T22:23:05.026162: step 4053, loss 0.0287229, acc 1\n",
      "2017-04-03T22:23:05.218088: step 4054, loss 0.0268915, acc 1\n",
      "2017-04-03T22:23:05.411272: step 4055, loss 0.0208216, acc 1\n",
      "2017-04-03T22:23:05.604867: step 4056, loss 0.0666785, acc 0.96875\n",
      "2017-04-03T22:23:05.800538: step 4057, loss 0.113514, acc 0.9375\n",
      "2017-04-03T22:23:05.997351: step 4058, loss 0.0744126, acc 0.96875\n",
      "2017-04-03T22:23:06.187144: step 4059, loss 0.119478, acc 0.953125\n",
      "2017-04-03T22:23:06.379075: step 4060, loss 0.0859551, acc 0.96875\n",
      "2017-04-03T22:23:06.572031: step 4061, loss 0.0389427, acc 1\n",
      "2017-04-03T22:23:06.767605: step 4062, loss 0.0577997, acc 0.96875\n",
      "2017-04-03T22:23:06.961188: step 4063, loss 0.0229678, acc 1\n",
      "2017-04-03T22:23:07.150414: step 4064, loss 0.0520112, acc 0.984375\n",
      "2017-04-03T22:23:07.340263: step 4065, loss 0.0298782, acc 1\n",
      "2017-04-03T22:23:07.534978: step 4066, loss 0.126224, acc 0.9375\n",
      "2017-04-03T22:23:07.728384: step 4067, loss 0.0417834, acc 1\n",
      "2017-04-03T22:23:07.918761: step 4068, loss 0.0591346, acc 0.984375\n",
      "2017-04-03T22:23:08.116646: step 4069, loss 0.0516865, acc 0.984375\n",
      "2017-04-03T22:23:08.315542: step 4070, loss 0.17011, acc 0.921875\n",
      "2017-04-03T22:23:08.508039: step 4071, loss 0.114384, acc 0.953125\n",
      "2017-04-03T22:23:08.699830: step 4072, loss 0.0180497, acc 1\n",
      "2017-04-03T22:23:08.892404: step 4073, loss 0.0331379, acc 1\n",
      "2017-04-03T22:23:09.086831: step 4074, loss 0.112337, acc 0.953125\n",
      "2017-04-03T22:23:09.276025: step 4075, loss 0.107007, acc 0.9375\n",
      "2017-04-03T22:23:09.466125: step 4076, loss 0.1803, acc 0.96875\n",
      "2017-04-03T22:23:09.653967: step 4077, loss 0.0900887, acc 0.96875\n",
      "2017-04-03T22:23:09.846154: step 4078, loss 0.0615582, acc 1\n",
      "2017-04-03T22:23:10.040391: step 4079, loss 0.105482, acc 0.953125\n",
      "2017-04-03T22:23:10.232759: step 4080, loss 0.0851268, acc 0.96875\n",
      "2017-04-03T22:23:10.421608: step 4081, loss 0.0383547, acc 1\n",
      "2017-04-03T22:23:10.611445: step 4082, loss 0.132394, acc 0.953125\n",
      "2017-04-03T22:23:10.805782: step 4083, loss 0.144867, acc 0.953125\n",
      "2017-04-03T22:23:10.995997: step 4084, loss 0.0741315, acc 1\n",
      "2017-04-03T22:23:11.187891: step 4085, loss 0.0496486, acc 0.96875\n",
      "2017-04-03T22:23:11.386035: step 4086, loss 0.193721, acc 0.953125\n",
      "2017-04-03T22:23:11.576360: step 4087, loss 0.101512, acc 0.96875\n",
      "2017-04-03T22:23:11.774463: step 4088, loss 0.0657458, acc 0.984375\n",
      "2017-04-03T22:23:11.969275: step 4089, loss 0.0821329, acc 0.953125\n",
      "2017-04-03T22:23:12.158879: step 4090, loss 0.0982982, acc 0.96875\n",
      "2017-04-03T22:23:12.348508: step 4091, loss 0.055537, acc 0.984375\n",
      "2017-04-03T22:23:12.545387: step 4092, loss 0.0291937, acc 1\n",
      "2017-04-03T22:23:12.739442: step 4093, loss 0.0664409, acc 0.984375\n",
      "2017-04-03T22:23:12.934389: step 4094, loss 0.0305485, acc 0.984375\n",
      "2017-04-03T22:23:13.127669: step 4095, loss 0.10525, acc 0.953125\n",
      "2017-04-03T22:23:13.322392: step 4096, loss 0.0498765, acc 1\n",
      "2017-04-03T22:23:13.517344: step 4097, loss 0.201594, acc 0.921875\n",
      "2017-04-03T22:23:13.706242: step 4098, loss 0.0184327, acc 1\n",
      "2017-04-03T22:23:13.902744: step 4099, loss 0.0694948, acc 0.984375\n",
      "2017-04-03T22:23:14.097202: step 4100, loss 0.0741352, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:23:14.758677: step 4100, loss 1.87884, acc 0.558747\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4100\n",
      "\n",
      "2017-04-03T22:23:15.023716: step 4101, loss 0.0882589, acc 0.96875\n",
      "2017-04-03T22:23:15.215533: step 4102, loss 0.0844651, acc 0.96875\n",
      "2017-04-03T22:23:15.407232: step 4103, loss 0.0306436, acc 1\n",
      "2017-04-03T22:23:15.566524: step 4104, loss 0.0851476, acc 0.961538\n",
      "2017-04-03T22:23:15.763591: step 4105, loss 0.011563, acc 1\n",
      "2017-04-03T22:23:15.955436: step 4106, loss 0.0418656, acc 1\n",
      "2017-04-03T22:23:16.152493: step 4107, loss 0.0521427, acc 0.984375\n",
      "2017-04-03T22:23:16.343263: step 4108, loss 0.0358588, acc 1\n",
      "2017-04-03T22:23:16.537155: step 4109, loss 0.0637605, acc 0.984375\n",
      "2017-04-03T22:23:16.727707: step 4110, loss 0.0743796, acc 0.984375\n",
      "2017-04-03T22:23:16.919587: step 4111, loss 0.0333413, acc 0.984375\n",
      "2017-04-03T22:23:17.112816: step 4112, loss 0.0172874, acc 1\n",
      "2017-04-03T22:23:17.309409: step 4113, loss 0.181557, acc 0.9375\n",
      "2017-04-03T22:23:17.510126: step 4114, loss 0.0287016, acc 0.984375\n",
      "2017-04-03T22:23:17.704805: step 4115, loss 0.0584033, acc 0.96875\n",
      "2017-04-03T22:23:17.904960: step 4116, loss 0.0428954, acc 0.984375\n",
      "2017-04-03T22:23:18.126791: step 4117, loss 0.00948836, acc 1\n",
      "2017-04-03T22:23:18.344692: step 4118, loss 0.0346295, acc 1\n",
      "2017-04-03T22:23:18.543945: step 4119, loss 0.0827084, acc 0.96875\n",
      "2017-04-03T22:23:18.745277: step 4120, loss 0.0131847, acc 1\n",
      "2017-04-03T22:23:18.942535: step 4121, loss 0.0673086, acc 0.984375\n",
      "2017-04-03T22:23:19.134376: step 4122, loss 0.0889159, acc 0.96875\n",
      "2017-04-03T22:23:19.328830: step 4123, loss 0.074747, acc 0.96875\n",
      "2017-04-03T22:23:19.519296: step 4124, loss 0.0789284, acc 0.9375\n",
      "2017-04-03T22:23:19.712591: step 4125, loss 0.0314437, acc 1\n",
      "2017-04-03T22:23:19.910817: step 4126, loss 0.0107088, acc 1\n",
      "2017-04-03T22:23:20.107814: step 4127, loss 0.106081, acc 0.96875\n",
      "2017-04-03T22:23:20.299309: step 4128, loss 0.050053, acc 0.984375\n",
      "2017-04-03T22:23:20.497533: step 4129, loss 0.0450097, acc 0.984375\n",
      "2017-04-03T22:23:20.723465: step 4130, loss 0.121657, acc 0.953125\n",
      "2017-04-03T22:23:20.925292: step 4131, loss 0.0693273, acc 0.984375\n",
      "2017-04-03T22:23:21.120765: step 4132, loss 0.146493, acc 0.953125\n",
      "2017-04-03T22:23:21.333086: step 4133, loss 0.0274529, acc 0.984375\n",
      "2017-04-03T22:23:21.537432: step 4134, loss 0.0428728, acc 1\n",
      "2017-04-03T22:23:21.740795: step 4135, loss 0.0274002, acc 1\n",
      "2017-04-03T22:23:21.937638: step 4136, loss 0.0235672, acc 1\n",
      "2017-04-03T22:23:22.130865: step 4137, loss 0.0622604, acc 0.984375\n",
      "2017-04-03T22:23:22.326438: step 4138, loss 0.0190731, acc 1\n",
      "2017-04-03T22:23:22.523859: step 4139, loss 0.16339, acc 0.9375\n",
      "2017-04-03T22:23:22.723385: step 4140, loss 0.0433609, acc 0.984375\n",
      "2017-04-03T22:23:22.917493: step 4141, loss 0.0981414, acc 0.96875\n",
      "2017-04-03T22:23:23.113977: step 4142, loss 0.219061, acc 0.921875\n",
      "2017-04-03T22:23:23.308750: step 4143, loss 0.0490683, acc 0.984375\n",
      "2017-04-03T22:23:23.504029: step 4144, loss 0.0541713, acc 1\n",
      "2017-04-03T22:23:23.696032: step 4145, loss 0.0380147, acc 1\n",
      "2017-04-03T22:23:23.885826: step 4146, loss 0.0754816, acc 0.984375\n",
      "2017-04-03T22:23:24.078486: step 4147, loss 0.0242321, acc 1\n",
      "2017-04-03T22:23:24.265709: step 4148, loss 0.120151, acc 0.96875\n",
      "2017-04-03T22:23:24.461094: step 4149, loss 0.0537404, acc 0.984375\n",
      "2017-04-03T22:23:24.655460: step 4150, loss 0.0549458, acc 0.984375\n",
      "2017-04-03T22:23:24.856181: step 4151, loss 0.0448756, acc 0.984375\n",
      "2017-04-03T22:23:25.049151: step 4152, loss 0.0385987, acc 0.984375\n",
      "2017-04-03T22:23:25.242698: step 4153, loss 0.0830868, acc 0.96875\n",
      "2017-04-03T22:23:25.436985: step 4154, loss 0.0434306, acc 1\n",
      "2017-04-03T22:23:25.632741: step 4155, loss 0.0332497, acc 1\n",
      "2017-04-03T22:23:25.823553: step 4156, loss 0.0246168, acc 1\n",
      "2017-04-03T22:23:26.015377: step 4157, loss 0.136839, acc 0.953125\n",
      "2017-04-03T22:23:26.205728: step 4158, loss 0.0197431, acc 1\n",
      "2017-04-03T22:23:26.394459: step 4159, loss 0.098319, acc 0.96875\n",
      "2017-04-03T22:23:26.586754: step 4160, loss 0.0182493, acc 1\n",
      "2017-04-03T22:23:26.778957: step 4161, loss 0.140481, acc 0.96875\n",
      "2017-04-03T22:23:26.974017: step 4162, loss 0.0391618, acc 0.984375\n",
      "2017-04-03T22:23:27.168169: step 4163, loss 0.101425, acc 0.953125\n",
      "2017-04-03T22:23:27.365038: step 4164, loss 0.0135681, acc 1\n",
      "2017-04-03T22:23:27.563345: step 4165, loss 0.0265356, acc 1\n",
      "2017-04-03T22:23:27.760009: step 4166, loss 0.0485572, acc 0.984375\n",
      "2017-04-03T22:23:27.954213: step 4167, loss 0.0763654, acc 0.984375\n",
      "2017-04-03T22:23:28.149774: step 4168, loss 0.0310417, acc 0.984375\n",
      "2017-04-03T22:23:28.342387: step 4169, loss 0.0616496, acc 0.984375\n",
      "2017-04-03T22:23:28.535605: step 4170, loss 0.0383705, acc 0.984375\n",
      "2017-04-03T22:23:28.738939: step 4171, loss 0.0572694, acc 0.984375\n",
      "2017-04-03T22:23:28.932736: step 4172, loss 0.0632285, acc 0.96875\n",
      "2017-04-03T22:23:29.125919: step 4173, loss 0.0835795, acc 0.953125\n",
      "2017-04-03T22:23:29.319976: step 4174, loss 0.0873245, acc 0.96875\n",
      "2017-04-03T22:23:29.509445: step 4175, loss 0.0200302, acc 0.984375\n",
      "2017-04-03T22:23:29.700574: step 4176, loss 0.0225515, acc 1\n",
      "2017-04-03T22:23:29.893264: step 4177, loss 0.0615815, acc 0.96875\n",
      "2017-04-03T22:23:30.083777: step 4178, loss 0.0875344, acc 0.96875\n",
      "2017-04-03T22:23:30.275924: step 4179, loss 0.101441, acc 0.96875\n",
      "2017-04-03T22:23:30.472845: step 4180, loss 0.0687994, acc 0.984375\n",
      "2017-04-03T22:23:30.666420: step 4181, loss 0.0310703, acc 1\n",
      "2017-04-03T22:23:30.864554: step 4182, loss 0.0320895, acc 1\n",
      "2017-04-03T22:23:31.055779: step 4183, loss 0.0300362, acc 1\n",
      "2017-04-03T22:23:31.251786: step 4184, loss 0.0503838, acc 0.984375\n",
      "2017-04-03T22:23:31.446065: step 4185, loss 0.11341, acc 0.953125\n",
      "2017-04-03T22:23:31.637775: step 4186, loss 0.0405985, acc 0.984375\n",
      "2017-04-03T22:23:31.833017: step 4187, loss 0.193944, acc 0.953125\n",
      "2017-04-03T22:23:32.024432: step 4188, loss 0.027209, acc 1\n",
      "2017-04-03T22:23:32.220860: step 4189, loss 0.0264173, acc 0.984375\n",
      "2017-04-03T22:23:32.412306: step 4190, loss 0.0376554, acc 0.984375\n",
      "2017-04-03T22:23:32.607154: step 4191, loss 0.0345276, acc 0.984375\n",
      "2017-04-03T22:23:32.820259: step 4192, loss 0.0587575, acc 1\n",
      "2017-04-03T22:23:33.012533: step 4193, loss 0.0376835, acc 1\n",
      "2017-04-03T22:23:33.204974: step 4194, loss 0.0154046, acc 1\n",
      "2017-04-03T22:23:33.399997: step 4195, loss 0.0758361, acc 0.96875\n",
      "2017-04-03T22:23:33.592615: step 4196, loss 0.12838, acc 0.953125\n",
      "2017-04-03T22:23:33.784835: step 4197, loss 0.0235566, acc 1\n",
      "2017-04-03T22:23:33.977264: step 4198, loss 0.0616548, acc 0.96875\n",
      "2017-04-03T22:23:34.168976: step 4199, loss 0.0566352, acc 0.984375\n",
      "2017-04-03T22:23:34.360158: step 4200, loss 0.0824424, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:23:35.013527: step 4200, loss 1.89854, acc 0.569191\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4200\n",
      "\n",
      "2017-04-03T22:23:35.265180: step 4201, loss 0.131808, acc 0.9375\n",
      "2017-04-03T22:23:35.453638: step 4202, loss 0.0808184, acc 0.96875\n",
      "2017-04-03T22:23:35.643091: step 4203, loss 0.0619503, acc 0.984375\n",
      "2017-04-03T22:23:35.834448: step 4204, loss 0.0858956, acc 0.984375\n",
      "2017-04-03T22:23:36.026962: step 4205, loss 0.0556567, acc 0.984375\n",
      "2017-04-03T22:23:36.218292: step 4206, loss 0.0740639, acc 0.984375\n",
      "2017-04-03T22:23:36.411396: step 4207, loss 0.0645355, acc 0.984375\n",
      "2017-04-03T22:23:36.607241: step 4208, loss 0.0505623, acc 0.984375\n",
      "2017-04-03T22:23:36.798799: step 4209, loss 0.019338, acc 1\n",
      "2017-04-03T22:23:36.994200: step 4210, loss 0.0552406, acc 0.984375\n",
      "2017-04-03T22:23:37.188999: step 4211, loss 0.0698609, acc 0.984375\n",
      "2017-04-03T22:23:37.353600: step 4212, loss 0.106272, acc 0.961538\n",
      "2017-04-03T22:23:37.554294: step 4213, loss 0.0338401, acc 1\n",
      "2017-04-03T22:23:37.744857: step 4214, loss 0.0824906, acc 0.953125\n",
      "2017-04-03T22:23:37.939775: step 4215, loss 0.0765511, acc 0.984375\n",
      "2017-04-03T22:23:38.129951: step 4216, loss 0.0469243, acc 0.984375\n",
      "2017-04-03T22:23:38.324367: step 4217, loss 0.0644459, acc 0.984375\n",
      "2017-04-03T22:23:38.519054: step 4218, loss 0.05746, acc 0.984375\n",
      "2017-04-03T22:23:38.716160: step 4219, loss 0.0385123, acc 0.984375\n",
      "2017-04-03T22:23:38.908717: step 4220, loss 0.0442031, acc 0.984375\n",
      "2017-04-03T22:23:39.103161: step 4221, loss 0.056878, acc 0.984375\n",
      "2017-04-03T22:23:39.295215: step 4222, loss 0.0150246, acc 1\n",
      "2017-04-03T22:23:39.490911: step 4223, loss 0.0592321, acc 0.96875\n",
      "2017-04-03T22:23:39.684964: step 4224, loss 0.0489626, acc 0.984375\n",
      "2017-04-03T22:23:39.881453: step 4225, loss 0.08125, acc 0.984375\n",
      "2017-04-03T22:23:40.072755: step 4226, loss 0.0558586, acc 1\n",
      "2017-04-03T22:23:40.263027: step 4227, loss 0.0914265, acc 0.984375\n",
      "2017-04-03T22:23:40.458184: step 4228, loss 0.0413655, acc 0.984375\n",
      "2017-04-03T22:23:40.656407: step 4229, loss 0.0998999, acc 0.96875\n",
      "2017-04-03T22:23:40.865291: step 4230, loss 0.0501234, acc 0.984375\n",
      "2017-04-03T22:23:41.059021: step 4231, loss 0.0189795, acc 1\n",
      "2017-04-03T22:23:41.253093: step 4232, loss 0.0867872, acc 0.96875\n",
      "2017-04-03T22:23:41.443766: step 4233, loss 0.0465832, acc 0.96875\n",
      "2017-04-03T22:23:41.637379: step 4234, loss 0.0474065, acc 1\n",
      "2017-04-03T22:23:41.837885: step 4235, loss 0.0206192, acc 1\n",
      "2017-04-03T22:23:42.036553: step 4236, loss 0.0278329, acc 1\n",
      "2017-04-03T22:23:42.228144: step 4237, loss 0.0794014, acc 0.984375\n",
      "2017-04-03T22:23:42.418881: step 4238, loss 0.055282, acc 0.984375\n",
      "2017-04-03T22:23:42.613349: step 4239, loss 0.0618661, acc 0.96875\n",
      "2017-04-03T22:23:42.805266: step 4240, loss 0.0181365, acc 1\n",
      "2017-04-03T22:23:42.999568: step 4241, loss 0.0175565, acc 1\n",
      "2017-04-03T22:23:43.190501: step 4242, loss 0.0180841, acc 1\n",
      "2017-04-03T22:23:43.381300: step 4243, loss 0.0600932, acc 1\n",
      "2017-04-03T22:23:43.574311: step 4244, loss 0.0145343, acc 1\n",
      "2017-04-03T22:23:43.767821: step 4245, loss 0.0377069, acc 1\n",
      "2017-04-03T22:23:43.957522: step 4246, loss 0.0422201, acc 0.984375\n",
      "2017-04-03T22:23:44.146697: step 4247, loss 0.0954474, acc 0.96875\n",
      "2017-04-03T22:23:44.337479: step 4248, loss 0.0837695, acc 0.96875\n",
      "2017-04-03T22:23:44.527169: step 4249, loss 0.0389454, acc 1\n",
      "2017-04-03T22:23:44.721284: step 4250, loss 0.0174887, acc 1\n",
      "2017-04-03T22:23:44.913919: step 4251, loss 0.0458702, acc 0.984375\n",
      "2017-04-03T22:23:45.106250: step 4252, loss 0.0525512, acc 0.984375\n",
      "2017-04-03T22:23:45.295198: step 4253, loss 0.0746367, acc 0.96875\n",
      "2017-04-03T22:23:45.489260: step 4254, loss 0.075048, acc 0.984375\n",
      "2017-04-03T22:23:45.681075: step 4255, loss 0.157932, acc 0.9375\n",
      "2017-04-03T22:23:45.879404: step 4256, loss 0.093248, acc 0.953125\n",
      "2017-04-03T22:23:46.076336: step 4257, loss 0.0493528, acc 0.984375\n",
      "2017-04-03T22:23:46.278917: step 4258, loss 0.0629998, acc 0.984375\n",
      "2017-04-03T22:23:46.475239: step 4259, loss 0.0162343, acc 1\n",
      "2017-04-03T22:23:46.666894: step 4260, loss 0.0575489, acc 0.984375\n",
      "2017-04-03T22:23:46.860367: step 4261, loss 0.0498059, acc 0.984375\n",
      "2017-04-03T22:23:47.058029: step 4262, loss 0.148036, acc 0.953125\n",
      "2017-04-03T22:23:47.252326: step 4263, loss 0.08555, acc 0.984375\n",
      "2017-04-03T22:23:47.445008: step 4264, loss 0.207351, acc 0.9375\n",
      "2017-04-03T22:23:47.637957: step 4265, loss 0.0171793, acc 1\n",
      "2017-04-03T22:23:47.832284: step 4266, loss 0.101557, acc 0.96875\n",
      "2017-04-03T22:23:48.027132: step 4267, loss 0.0199675, acc 1\n",
      "2017-04-03T22:23:48.219468: step 4268, loss 0.0779533, acc 0.96875\n",
      "2017-04-03T22:23:48.408913: step 4269, loss 0.102194, acc 0.953125\n",
      "2017-04-03T22:23:48.605049: step 4270, loss 0.0133158, acc 1\n",
      "2017-04-03T22:23:48.796890: step 4271, loss 0.0611834, acc 0.96875\n",
      "2017-04-03T22:23:48.990571: step 4272, loss 0.11721, acc 0.953125\n",
      "2017-04-03T22:23:49.181650: step 4273, loss 0.0229412, acc 1\n",
      "2017-04-03T22:23:49.371161: step 4274, loss 0.0868373, acc 0.953125\n",
      "2017-04-03T22:23:49.564177: step 4275, loss 0.0467227, acc 0.984375\n",
      "2017-04-03T22:23:49.754414: step 4276, loss 0.1042, acc 0.96875\n",
      "2017-04-03T22:23:49.944295: step 4277, loss 0.0365513, acc 1\n",
      "2017-04-03T22:23:50.150992: step 4278, loss 0.0608319, acc 0.984375\n",
      "2017-04-03T22:23:50.344326: step 4279, loss 0.0676147, acc 0.984375\n",
      "2017-04-03T22:23:50.538194: step 4280, loss 0.0226698, acc 1\n",
      "2017-04-03T22:23:50.728853: step 4281, loss 0.0311075, acc 1\n",
      "2017-04-03T22:23:50.923309: step 4282, loss 0.0526774, acc 0.984375\n",
      "2017-04-03T22:23:51.116301: step 4283, loss 0.0537972, acc 0.984375\n",
      "2017-04-03T22:23:51.314490: step 4284, loss 0.0894543, acc 0.96875\n",
      "2017-04-03T22:23:51.502460: step 4285, loss 0.0947413, acc 0.96875\n",
      "2017-04-03T22:23:51.695625: step 4286, loss 0.140499, acc 0.921875\n",
      "2017-04-03T22:23:51.893073: step 4287, loss 0.0781668, acc 0.96875\n",
      "2017-04-03T22:23:52.087432: step 4288, loss 0.0249246, acc 1\n",
      "2017-04-03T22:23:52.281234: step 4289, loss 0.0446621, acc 0.984375\n",
      "2017-04-03T22:23:52.475615: step 4290, loss 0.0190403, acc 1\n",
      "2017-04-03T22:23:52.672546: step 4291, loss 0.230142, acc 0.921875\n",
      "2017-04-03T22:23:52.866681: step 4292, loss 0.0398077, acc 0.984375\n",
      "2017-04-03T22:23:53.057369: step 4293, loss 0.0617922, acc 0.984375\n",
      "2017-04-03T22:23:53.251537: step 4294, loss 0.028657, acc 1\n",
      "2017-04-03T22:23:53.443190: step 4295, loss 0.14617, acc 0.921875\n",
      "2017-04-03T22:23:53.637664: step 4296, loss 0.0445988, acc 0.984375\n",
      "2017-04-03T22:23:53.832564: step 4297, loss 0.0568869, acc 0.984375\n",
      "2017-04-03T22:23:54.031369: step 4298, loss 0.107643, acc 0.953125\n",
      "2017-04-03T22:23:54.239114: step 4299, loss 0.0200662, acc 1\n",
      "2017-04-03T22:23:54.432615: step 4300, loss 0.0682297, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:23:55.081209: step 4300, loss 1.94629, acc 0.558747\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4300\n",
      "\n",
      "2017-04-03T22:23:55.333291: step 4301, loss 0.107627, acc 0.96875\n",
      "2017-04-03T22:23:55.524226: step 4302, loss 0.0248578, acc 1\n",
      "2017-04-03T22:23:55.713116: step 4303, loss 0.0508226, acc 0.984375\n",
      "2017-04-03T22:23:55.910583: step 4304, loss 0.0477297, acc 0.984375\n",
      "2017-04-03T22:23:56.101254: step 4305, loss 0.0486735, acc 1\n",
      "2017-04-03T22:23:56.303442: step 4306, loss 0.0705967, acc 0.984375\n",
      "2017-04-03T22:23:56.533702: step 4307, loss 0.0965761, acc 0.953125\n",
      "2017-04-03T22:23:56.731604: step 4308, loss 0.0592503, acc 0.984375\n",
      "2017-04-03T22:23:56.942627: step 4309, loss 0.121617, acc 0.953125\n",
      "2017-04-03T22:23:57.156332: step 4310, loss 0.13338, acc 0.953125\n",
      "2017-04-03T22:23:57.349491: step 4311, loss 0.0555403, acc 0.984375\n",
      "2017-04-03T22:23:57.544777: step 4312, loss 0.0596416, acc 0.984375\n",
      "2017-04-03T22:23:57.739552: step 4313, loss 0.05529, acc 0.984375\n",
      "2017-04-03T22:23:57.934084: step 4314, loss 0.061318, acc 0.984375\n",
      "2017-04-03T22:23:58.126000: step 4315, loss 0.0433774, acc 1\n",
      "2017-04-03T22:23:58.316731: step 4316, loss 0.0225898, acc 1\n",
      "2017-04-03T22:23:58.514225: step 4317, loss 0.119684, acc 0.953125\n",
      "2017-04-03T22:23:58.706418: step 4318, loss 0.0772824, acc 0.984375\n",
      "2017-04-03T22:23:58.901394: step 4319, loss 0.0195684, acc 1\n",
      "2017-04-03T22:23:59.065943: step 4320, loss 0.0519983, acc 0.980769\n",
      "2017-04-03T22:23:59.256594: step 4321, loss 0.0429661, acc 0.984375\n",
      "2017-04-03T22:23:59.451589: step 4322, loss 0.0671767, acc 0.984375\n",
      "2017-04-03T22:23:59.642932: step 4323, loss 0.0400812, acc 0.984375\n",
      "2017-04-03T22:23:59.834617: step 4324, loss 0.104534, acc 0.96875\n",
      "2017-04-03T22:24:00.027142: step 4325, loss 0.0678736, acc 0.96875\n",
      "2017-04-03T22:24:00.225173: step 4326, loss 0.0232833, acc 1\n",
      "2017-04-03T22:24:00.419849: step 4327, loss 0.0371893, acc 0.984375\n",
      "2017-04-03T22:24:00.618099: step 4328, loss 0.045963, acc 0.984375\n",
      "2017-04-03T22:24:00.810311: step 4329, loss 0.0124405, acc 1\n",
      "2017-04-03T22:24:01.007307: step 4330, loss 0.155388, acc 0.96875\n",
      "2017-04-03T22:24:01.206563: step 4331, loss 0.0400021, acc 0.984375\n",
      "2017-04-03T22:24:01.418263: step 4332, loss 0.0163928, acc 1\n",
      "2017-04-03T22:24:01.632555: step 4333, loss 0.0548825, acc 0.984375\n",
      "2017-04-03T22:24:01.858242: step 4334, loss 0.0550001, acc 0.984375\n",
      "2017-04-03T22:24:02.077396: step 4335, loss 0.0228557, acc 1\n",
      "2017-04-03T22:24:02.315754: step 4336, loss 0.116582, acc 0.953125\n",
      "2017-04-03T22:24:02.514752: step 4337, loss 0.0576837, acc 0.984375\n",
      "2017-04-03T22:24:02.719772: step 4338, loss 0.0389182, acc 0.984375\n",
      "2017-04-03T22:24:02.939837: step 4339, loss 0.0626139, acc 0.984375\n",
      "2017-04-03T22:24:03.161947: step 4340, loss 0.0524319, acc 0.984375\n",
      "2017-04-03T22:24:03.366602: step 4341, loss 0.0864643, acc 0.953125\n",
      "2017-04-03T22:24:03.563032: step 4342, loss 0.0221514, acc 0.984375\n",
      "2017-04-03T22:24:03.757008: step 4343, loss 0.0271399, acc 1\n",
      "2017-04-03T22:24:03.946778: step 4344, loss 0.0608806, acc 0.96875\n",
      "2017-04-03T22:24:04.140150: step 4345, loss 0.0422378, acc 0.984375\n",
      "2017-04-03T22:24:04.333328: step 4346, loss 0.0207208, acc 1\n",
      "2017-04-03T22:24:04.534224: step 4347, loss 0.0423303, acc 0.96875\n",
      "2017-04-03T22:24:04.726289: step 4348, loss 0.0522002, acc 0.984375\n",
      "2017-04-03T22:24:04.920737: step 4349, loss 0.191996, acc 0.953125\n",
      "2017-04-03T22:24:05.117595: step 4350, loss 0.013431, acc 1\n",
      "2017-04-03T22:24:05.310495: step 4351, loss 0.115453, acc 0.953125\n",
      "2017-04-03T22:24:05.502144: step 4352, loss 0.02821, acc 0.984375\n",
      "2017-04-03T22:24:05.716497: step 4353, loss 0.0502452, acc 0.984375\n",
      "2017-04-03T22:24:05.925538: step 4354, loss 0.0107765, acc 1\n",
      "2017-04-03T22:24:06.122102: step 4355, loss 0.0897306, acc 0.984375\n",
      "2017-04-03T22:24:06.321982: step 4356, loss 0.0987295, acc 0.96875\n",
      "2017-04-03T22:24:06.514543: step 4357, loss 0.0616693, acc 0.96875\n",
      "2017-04-03T22:24:06.710944: step 4358, loss 0.0155706, acc 1\n",
      "2017-04-03T22:24:06.901549: step 4359, loss 0.0728234, acc 0.953125\n",
      "2017-04-03T22:24:07.095733: step 4360, loss 0.053239, acc 0.96875\n",
      "2017-04-03T22:24:07.294726: step 4361, loss 0.0786974, acc 0.96875\n",
      "2017-04-03T22:24:07.490587: step 4362, loss 0.0449293, acc 0.984375\n",
      "2017-04-03T22:24:07.684669: step 4363, loss 0.0407465, acc 1\n",
      "2017-04-03T22:24:07.879234: step 4364, loss 0.0957485, acc 0.953125\n",
      "2017-04-03T22:24:08.091555: step 4365, loss 0.103864, acc 0.953125\n",
      "2017-04-03T22:24:08.305609: step 4366, loss 0.112687, acc 0.9375\n",
      "2017-04-03T22:24:08.526525: step 4367, loss 0.0441587, acc 0.984375\n",
      "2017-04-03T22:24:08.743395: step 4368, loss 0.137487, acc 0.9375\n",
      "2017-04-03T22:24:08.961679: step 4369, loss 0.0448126, acc 0.984375\n",
      "2017-04-03T22:24:09.181215: step 4370, loss 0.0925026, acc 0.96875\n",
      "2017-04-03T22:24:09.392190: step 4371, loss 0.111159, acc 0.96875\n",
      "2017-04-03T22:24:09.599852: step 4372, loss 0.0548092, acc 0.96875\n",
      "2017-04-03T22:24:09.809890: step 4373, loss 0.132307, acc 0.96875\n",
      "2017-04-03T22:24:10.023706: step 4374, loss 0.0600928, acc 1\n",
      "2017-04-03T22:24:10.224688: step 4375, loss 0.0421204, acc 0.984375\n",
      "2017-04-03T22:24:10.430312: step 4376, loss 0.0180504, acc 1\n",
      "2017-04-03T22:24:10.652093: step 4377, loss 0.0256506, acc 1\n",
      "2017-04-03T22:24:10.881595: step 4378, loss 0.0542838, acc 0.96875\n",
      "2017-04-03T22:24:11.089934: step 4379, loss 0.0166883, acc 1\n",
      "2017-04-03T22:24:11.309181: step 4380, loss 0.0423583, acc 1\n",
      "2017-04-03T22:24:11.519410: step 4381, loss 0.00973829, acc 1\n",
      "2017-04-03T22:24:11.733235: step 4382, loss 0.0446506, acc 0.984375\n",
      "2017-04-03T22:24:11.960090: step 4383, loss 0.0655193, acc 0.984375\n",
      "2017-04-03T22:24:12.166098: step 4384, loss 0.0369407, acc 0.984375\n",
      "2017-04-03T22:24:12.375491: step 4385, loss 0.0459343, acc 0.984375\n",
      "2017-04-03T22:24:12.571370: step 4386, loss 0.0622135, acc 0.984375\n",
      "2017-04-03T22:24:12.771711: step 4387, loss 0.0917525, acc 0.96875\n",
      "2017-04-03T22:24:12.973899: step 4388, loss 0.0425126, acc 0.984375\n",
      "2017-04-03T22:24:13.172122: step 4389, loss 0.118344, acc 0.96875\n",
      "2017-04-03T22:24:13.365926: step 4390, loss 0.116585, acc 0.953125\n",
      "2017-04-03T22:24:13.556561: step 4391, loss 0.0281419, acc 1\n",
      "2017-04-03T22:24:13.761745: step 4392, loss 0.0539322, acc 0.984375\n",
      "2017-04-03T22:24:13.968434: step 4393, loss 0.0341376, acc 0.984375\n",
      "2017-04-03T22:24:14.159090: step 4394, loss 0.0431081, acc 0.984375\n",
      "2017-04-03T22:24:14.354777: step 4395, loss 0.0513511, acc 0.984375\n",
      "2017-04-03T22:24:14.546673: step 4396, loss 0.0491039, acc 1\n",
      "2017-04-03T22:24:14.740409: step 4397, loss 0.103255, acc 0.96875\n",
      "2017-04-03T22:24:14.931132: step 4398, loss 0.0791116, acc 0.953125\n",
      "2017-04-03T22:24:15.124554: step 4399, loss 0.0248721, acc 1\n",
      "2017-04-03T22:24:15.317731: step 4400, loss 0.12316, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:24:15.996143: step 4400, loss 1.93185, acc 0.565274\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4400\n",
      "\n",
      "2017-04-03T22:24:16.272234: step 4401, loss 0.0127708, acc 1\n",
      "2017-04-03T22:24:16.474179: step 4402, loss 0.0166118, acc 1\n",
      "2017-04-03T22:24:16.691852: step 4403, loss 0.105505, acc 0.953125\n",
      "2017-04-03T22:24:16.893366: step 4404, loss 0.0335636, acc 1\n",
      "2017-04-03T22:24:17.109500: step 4405, loss 0.0862576, acc 0.984375\n",
      "2017-04-03T22:24:17.321837: step 4406, loss 0.0560686, acc 0.96875\n",
      "2017-04-03T22:24:17.532038: step 4407, loss 0.0345366, acc 1\n",
      "2017-04-03T22:24:17.733811: step 4408, loss 0.0265173, acc 1\n",
      "2017-04-03T22:24:17.935278: step 4409, loss 0.018663, acc 1\n",
      "2017-04-03T22:24:18.135838: step 4410, loss 0.154018, acc 0.953125\n",
      "2017-04-03T22:24:18.334216: step 4411, loss 0.0982627, acc 0.96875\n",
      "2017-04-03T22:24:18.532774: step 4412, loss 0.0266501, acc 0.984375\n",
      "2017-04-03T22:24:18.729871: step 4413, loss 0.0764982, acc 0.984375\n",
      "2017-04-03T22:24:18.947358: step 4414, loss 0.198675, acc 0.9375\n",
      "2017-04-03T22:24:19.153437: step 4415, loss 0.0426331, acc 1\n",
      "2017-04-03T22:24:19.362503: step 4416, loss 0.0181633, acc 1\n",
      "2017-04-03T22:24:19.573473: step 4417, loss 0.122961, acc 0.953125\n",
      "2017-04-03T22:24:19.782543: step 4418, loss 0.0605471, acc 0.96875\n",
      "2017-04-03T22:24:20.003026: step 4419, loss 0.0511619, acc 0.984375\n",
      "2017-04-03T22:24:20.209311: step 4420, loss 0.0517714, acc 1\n",
      "2017-04-03T22:24:20.407578: step 4421, loss 0.065023, acc 0.984375\n",
      "2017-04-03T22:24:20.619142: step 4422, loss 0.0637068, acc 0.984375\n",
      "2017-04-03T22:24:20.812931: step 4423, loss 0.0287529, acc 0.984375\n",
      "2017-04-03T22:24:21.009912: step 4424, loss 0.0100013, acc 1\n",
      "2017-04-03T22:24:21.207643: step 4425, loss 0.0384452, acc 1\n",
      "2017-04-03T22:24:21.409065: step 4426, loss 0.0619049, acc 1\n",
      "2017-04-03T22:24:21.603099: step 4427, loss 0.0616273, acc 0.984375\n",
      "2017-04-03T22:24:21.766293: step 4428, loss 0.0679347, acc 0.980769\n",
      "2017-04-03T22:24:21.964112: step 4429, loss 0.0162439, acc 1\n",
      "2017-04-03T22:24:22.159009: step 4430, loss 0.102034, acc 0.96875\n",
      "2017-04-03T22:24:22.350545: step 4431, loss 0.0723174, acc 0.984375\n",
      "2017-04-03T22:24:22.550108: step 4432, loss 0.00725558, acc 1\n",
      "2017-04-03T22:24:22.741737: step 4433, loss 0.0188674, acc 1\n",
      "2017-04-03T22:24:22.935821: step 4434, loss 0.0650072, acc 0.96875\n",
      "2017-04-03T22:24:23.129237: step 4435, loss 0.0226205, acc 1\n",
      "2017-04-03T22:24:23.325883: step 4436, loss 0.0159754, acc 1\n",
      "2017-04-03T22:24:23.517570: step 4437, loss 0.0465091, acc 0.984375\n",
      "2017-04-03T22:24:23.711569: step 4438, loss 0.0394712, acc 1\n",
      "2017-04-03T22:24:23.927605: step 4439, loss 0.0193662, acc 1\n",
      "2017-04-03T22:24:24.127908: step 4440, loss 0.0391943, acc 0.984375\n",
      "2017-04-03T22:24:24.324016: step 4441, loss 0.0223789, acc 1\n",
      "2017-04-03T22:24:24.518165: step 4442, loss 0.0687934, acc 0.984375\n",
      "2017-04-03T22:24:24.711790: step 4443, loss 0.0258895, acc 1\n",
      "2017-04-03T22:24:24.907089: step 4444, loss 0.0314893, acc 1\n",
      "2017-04-03T22:24:25.104057: step 4445, loss 0.0297949, acc 0.984375\n",
      "2017-04-03T22:24:25.302394: step 4446, loss 0.0239157, acc 1\n",
      "2017-04-03T22:24:25.498712: step 4447, loss 0.0665283, acc 0.984375\n",
      "2017-04-03T22:24:25.693985: step 4448, loss 0.0991699, acc 0.96875\n",
      "2017-04-03T22:24:25.889490: step 4449, loss 0.0697595, acc 0.96875\n",
      "2017-04-03T22:24:26.083984: step 4450, loss 0.135744, acc 0.953125\n",
      "2017-04-03T22:24:26.281208: step 4451, loss 0.0297849, acc 1\n",
      "2017-04-03T22:24:26.481996: step 4452, loss 0.0194938, acc 1\n",
      "2017-04-03T22:24:26.678349: step 4453, loss 0.077228, acc 0.96875\n",
      "2017-04-03T22:24:26.869811: step 4454, loss 0.136483, acc 0.921875\n",
      "2017-04-03T22:24:27.068563: step 4455, loss 0.125824, acc 0.953125\n",
      "2017-04-03T22:24:27.265480: step 4456, loss 0.0808472, acc 0.96875\n",
      "2017-04-03T22:24:27.462352: step 4457, loss 0.179693, acc 0.9375\n",
      "2017-04-03T22:24:27.666238: step 4458, loss 0.139037, acc 0.953125\n",
      "2017-04-03T22:24:27.871967: step 4459, loss 0.0959223, acc 0.984375\n",
      "2017-04-03T22:24:28.077331: step 4460, loss 0.0895988, acc 0.96875\n",
      "2017-04-03T22:24:28.280110: step 4461, loss 0.0284189, acc 1\n",
      "2017-04-03T22:24:28.477734: step 4462, loss 0.0729476, acc 0.984375\n",
      "2017-04-03T22:24:28.685659: step 4463, loss 0.0214147, acc 1\n",
      "2017-04-03T22:24:28.889931: step 4464, loss 0.0728953, acc 0.96875\n",
      "2017-04-03T22:24:29.100930: step 4465, loss 0.238698, acc 0.953125\n",
      "2017-04-03T22:24:29.305968: step 4466, loss 0.0279021, acc 0.984375\n",
      "2017-04-03T22:24:29.499293: step 4467, loss 0.0538589, acc 0.984375\n",
      "2017-04-03T22:24:29.692840: step 4468, loss 0.0393647, acc 0.984375\n",
      "2017-04-03T22:24:29.890905: step 4469, loss 0.0169063, acc 1\n",
      "2017-04-03T22:24:30.092058: step 4470, loss 0.0244433, acc 1\n",
      "2017-04-03T22:24:30.284122: step 4471, loss 0.0459077, acc 1\n",
      "2017-04-03T22:24:30.476655: step 4472, loss 0.0613773, acc 0.984375\n",
      "2017-04-03T22:24:30.668338: step 4473, loss 0.0645855, acc 0.96875\n",
      "2017-04-03T22:24:30.865886: step 4474, loss 0.0548064, acc 0.984375\n",
      "2017-04-03T22:24:31.062647: step 4475, loss 0.057983, acc 0.984375\n",
      "2017-04-03T22:24:31.264175: step 4476, loss 0.0134185, acc 1\n",
      "2017-04-03T22:24:31.461092: step 4477, loss 0.0366893, acc 1\n",
      "2017-04-03T22:24:31.663625: step 4478, loss 0.0463348, acc 0.984375\n",
      "2017-04-03T22:24:31.859574: step 4479, loss 0.135416, acc 0.96875\n",
      "2017-04-03T22:24:32.054109: step 4480, loss 0.0279612, acc 1\n",
      "2017-04-03T22:24:32.247197: step 4481, loss 0.0882836, acc 0.953125\n",
      "2017-04-03T22:24:32.442835: step 4482, loss 0.0610338, acc 0.984375\n",
      "2017-04-03T22:24:32.639855: step 4483, loss 0.0863352, acc 0.953125\n",
      "2017-04-03T22:24:32.837732: step 4484, loss 0.0908742, acc 0.984375\n",
      "2017-04-03T22:24:33.035110: step 4485, loss 0.00558833, acc 1\n",
      "2017-04-03T22:24:33.228362: step 4486, loss 0.191876, acc 0.96875\n",
      "2017-04-03T22:24:33.420987: step 4487, loss 0.0697307, acc 0.984375\n",
      "2017-04-03T22:24:33.620414: step 4488, loss 0.0322579, acc 1\n",
      "2017-04-03T22:24:33.809044: step 4489, loss 0.0679783, acc 0.984375\n",
      "2017-04-03T22:24:34.002864: step 4490, loss 0.0410212, acc 0.984375\n",
      "2017-04-03T22:24:34.198685: step 4491, loss 0.0216036, acc 1\n",
      "2017-04-03T22:24:34.395514: step 4492, loss 0.0463917, acc 0.96875\n",
      "2017-04-03T22:24:34.593763: step 4493, loss 0.0322108, acc 1\n",
      "2017-04-03T22:24:34.789673: step 4494, loss 0.0480286, acc 1\n",
      "2017-04-03T22:24:34.995233: step 4495, loss 0.0788787, acc 0.96875\n",
      "2017-04-03T22:24:35.191897: step 4496, loss 0.0939013, acc 0.984375\n",
      "2017-04-03T22:24:35.392899: step 4497, loss 0.141829, acc 0.953125\n",
      "2017-04-03T22:24:35.586395: step 4498, loss 0.0815025, acc 0.984375\n",
      "2017-04-03T22:24:35.783466: step 4499, loss 0.0662428, acc 0.984375\n",
      "2017-04-03T22:24:35.986787: step 4500, loss 0.0362319, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:24:36.651352: step 4500, loss 1.9599, acc 0.558747\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4500\n",
      "\n",
      "2017-04-03T22:24:36.914007: step 4501, loss 0.10175, acc 0.953125\n",
      "2017-04-03T22:24:37.104108: step 4502, loss 0.0452322, acc 0.984375\n",
      "2017-04-03T22:24:37.307426: step 4503, loss 0.0475525, acc 0.984375\n",
      "2017-04-03T22:24:37.506640: step 4504, loss 0.0195552, acc 1\n",
      "2017-04-03T22:24:37.708827: step 4505, loss 0.095159, acc 0.96875\n",
      "2017-04-03T22:24:37.903349: step 4506, loss 0.0635834, acc 0.96875\n",
      "2017-04-03T22:24:38.101583: step 4507, loss 0.125593, acc 0.96875\n",
      "2017-04-03T22:24:38.296583: step 4508, loss 0.0134445, acc 1\n",
      "2017-04-03T22:24:38.493055: step 4509, loss 0.252134, acc 0.984375\n",
      "2017-04-03T22:24:38.690509: step 4510, loss 0.0580914, acc 0.984375\n",
      "2017-04-03T22:24:38.883370: step 4511, loss 0.0726248, acc 0.96875\n",
      "2017-04-03T22:24:39.074796: step 4512, loss 0.0818112, acc 0.96875\n",
      "2017-04-03T22:24:39.270946: step 4513, loss 0.14611, acc 0.921875\n",
      "2017-04-03T22:24:39.462413: step 4514, loss 0.06517, acc 0.984375\n",
      "2017-04-03T22:24:39.659728: step 4515, loss 0.143968, acc 0.953125\n",
      "2017-04-03T22:24:39.851869: step 4516, loss 0.0291316, acc 1\n",
      "2017-04-03T22:24:40.046168: step 4517, loss 0.0411271, acc 1\n",
      "2017-04-03T22:24:40.238264: step 4518, loss 0.0208326, acc 1\n",
      "2017-04-03T22:24:40.429268: step 4519, loss 0.0321467, acc 0.984375\n",
      "2017-04-03T22:24:40.619146: step 4520, loss 0.0286746, acc 0.984375\n",
      "2017-04-03T22:24:40.810700: step 4521, loss 0.102417, acc 0.953125\n",
      "2017-04-03T22:24:41.006502: step 4522, loss 0.0812853, acc 0.984375\n",
      "2017-04-03T22:24:41.198958: step 4523, loss 0.0590522, acc 0.984375\n",
      "2017-04-03T22:24:41.391426: step 4524, loss 0.0953612, acc 0.953125\n",
      "2017-04-03T22:24:41.585487: step 4525, loss 0.0821984, acc 0.96875\n",
      "2017-04-03T22:24:41.780520: step 4526, loss 0.0725931, acc 0.984375\n",
      "2017-04-03T22:24:41.976999: step 4527, loss 0.0687807, acc 0.984375\n",
      "2017-04-03T22:24:42.171387: step 4528, loss 0.0599632, acc 0.96875\n",
      "2017-04-03T22:24:42.366449: step 4529, loss 0.0366206, acc 1\n",
      "2017-04-03T22:24:42.556561: step 4530, loss 0.0676853, acc 0.984375\n",
      "2017-04-03T22:24:42.754648: step 4531, loss 0.115781, acc 0.96875\n",
      "2017-04-03T22:24:42.952475: step 4532, loss 0.0189456, acc 1\n",
      "2017-04-03T22:24:43.148427: step 4533, loss 0.0975952, acc 0.96875\n",
      "2017-04-03T22:24:43.342459: step 4534, loss 0.181391, acc 0.984375\n",
      "2017-04-03T22:24:43.534956: step 4535, loss 0.0710354, acc 0.96875\n",
      "2017-04-03T22:24:43.697795: step 4536, loss 0.0546518, acc 0.980769\n",
      "2017-04-03T22:24:43.892119: step 4537, loss 0.0286082, acc 1\n",
      "2017-04-03T22:24:44.088386: step 4538, loss 0.0447518, acc 0.984375\n",
      "2017-04-03T22:24:44.281098: step 4539, loss 0.0271802, acc 1\n",
      "2017-04-03T22:24:44.480807: step 4540, loss 0.0600638, acc 0.984375\n",
      "2017-04-03T22:24:44.678501: step 4541, loss 0.0576927, acc 0.984375\n",
      "2017-04-03T22:24:44.872553: step 4542, loss 0.128495, acc 0.96875\n",
      "2017-04-03T22:24:45.068786: step 4543, loss 0.089626, acc 0.96875\n",
      "2017-04-03T22:24:45.263602: step 4544, loss 0.167669, acc 0.9375\n",
      "2017-04-03T22:24:45.456744: step 4545, loss 0.0137636, acc 1\n",
      "2017-04-03T22:24:45.650541: step 4546, loss 0.0292258, acc 0.984375\n",
      "2017-04-03T22:24:45.841257: step 4547, loss 0.0496089, acc 1\n",
      "2017-04-03T22:24:46.037598: step 4548, loss 0.0529623, acc 1\n",
      "2017-04-03T22:24:46.247640: step 4549, loss 0.039396, acc 0.984375\n",
      "2017-04-03T22:24:46.439398: step 4550, loss 0.161372, acc 0.953125\n",
      "2017-04-03T22:24:46.632438: step 4551, loss 0.0291198, acc 1\n",
      "2017-04-03T22:24:46.827365: step 4552, loss 0.0219494, acc 1\n",
      "2017-04-03T22:24:47.020839: step 4553, loss 0.0577511, acc 0.984375\n",
      "2017-04-03T22:24:47.213818: step 4554, loss 0.0428582, acc 0.984375\n",
      "2017-04-03T22:24:47.413024: step 4555, loss 0.0700064, acc 0.984375\n",
      "2017-04-03T22:24:47.605462: step 4556, loss 0.0111106, acc 1\n",
      "2017-04-03T22:24:47.800506: step 4557, loss 0.0459368, acc 0.984375\n",
      "2017-04-03T22:24:47.994855: step 4558, loss 0.0242847, acc 1\n",
      "2017-04-03T22:24:48.192612: step 4559, loss 0.0857719, acc 0.984375\n",
      "2017-04-03T22:24:48.384877: step 4560, loss 0.0445791, acc 0.984375\n",
      "2017-04-03T22:24:48.576635: step 4561, loss 0.0426884, acc 0.984375\n",
      "2017-04-03T22:24:48.768952: step 4562, loss 0.0452061, acc 0.984375\n",
      "2017-04-03T22:24:48.961000: step 4563, loss 0.06825, acc 0.96875\n",
      "2017-04-03T22:24:49.159948: step 4564, loss 0.0148052, acc 1\n",
      "2017-04-03T22:24:49.366633: step 4565, loss 0.106669, acc 0.953125\n",
      "2017-04-03T22:24:49.565081: step 4566, loss 0.0675711, acc 0.96875\n",
      "2017-04-03T22:24:49.761030: step 4567, loss 0.0114581, acc 1\n",
      "2017-04-03T22:24:49.961955: step 4568, loss 0.0931489, acc 0.953125\n",
      "2017-04-03T22:24:50.158905: step 4569, loss 0.0847661, acc 0.953125\n",
      "2017-04-03T22:24:50.368489: step 4570, loss 0.0266515, acc 0.984375\n",
      "2017-04-03T22:24:50.562570: step 4571, loss 0.0831598, acc 0.96875\n",
      "2017-04-03T22:24:50.755878: step 4572, loss 0.0787278, acc 0.96875\n",
      "2017-04-03T22:24:50.949447: step 4573, loss 0.023094, acc 1\n",
      "2017-04-03T22:24:51.140582: step 4574, loss 0.0581798, acc 1\n",
      "2017-04-03T22:24:51.333612: step 4575, loss 0.0374509, acc 0.984375\n",
      "2017-04-03T22:24:51.530378: step 4576, loss 0.0222915, acc 1\n",
      "2017-04-03T22:24:51.723824: step 4577, loss 0.0187396, acc 1\n",
      "2017-04-03T22:24:51.917435: step 4578, loss 0.0266469, acc 1\n",
      "2017-04-03T22:24:52.108539: step 4579, loss 0.0926118, acc 0.96875\n",
      "2017-04-03T22:24:52.305389: step 4580, loss 0.051443, acc 1\n",
      "2017-04-03T22:24:52.509764: step 4581, loss 0.0913766, acc 0.96875\n",
      "2017-04-03T22:24:52.709460: step 4582, loss 0.0179526, acc 1\n",
      "2017-04-03T22:24:52.906899: step 4583, loss 0.0662919, acc 0.984375\n",
      "2017-04-03T22:24:53.101886: step 4584, loss 0.0581457, acc 0.984375\n",
      "2017-04-03T22:24:53.296358: step 4585, loss 0.0674483, acc 0.96875\n",
      "2017-04-03T22:24:53.497487: step 4586, loss 0.0146735, acc 1\n",
      "2017-04-03T22:24:53.705997: step 4587, loss 0.144918, acc 0.953125\n",
      "2017-04-03T22:24:53.909489: step 4588, loss 0.063802, acc 0.96875\n",
      "2017-04-03T22:24:54.103051: step 4589, loss 0.12816, acc 0.9375\n",
      "2017-04-03T22:24:54.316190: step 4590, loss 0.11293, acc 0.96875\n",
      "2017-04-03T22:24:54.523679: step 4591, loss 0.037759, acc 1\n",
      "2017-04-03T22:24:54.737538: step 4592, loss 0.0950425, acc 0.96875\n",
      "2017-04-03T22:24:54.945180: step 4593, loss 0.0231872, acc 1\n",
      "2017-04-03T22:24:55.140832: step 4594, loss 0.0201761, acc 1\n",
      "2017-04-03T22:24:55.340102: step 4595, loss 0.0151453, acc 1\n",
      "2017-04-03T22:24:55.528486: step 4596, loss 0.0450939, acc 0.984375\n",
      "2017-04-03T22:24:55.719820: step 4597, loss 0.0232162, acc 1\n",
      "2017-04-03T22:24:55.912248: step 4598, loss 0.095065, acc 0.96875\n",
      "2017-04-03T22:24:56.105817: step 4599, loss 0.0458363, acc 0.984375\n",
      "2017-04-03T22:24:56.295485: step 4600, loss 0.0427176, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:24:56.981078: step 4600, loss 1.93845, acc 0.570496\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4600\n",
      "\n",
      "2017-04-03T22:24:57.243268: step 4601, loss 0.0163279, acc 1\n",
      "2017-04-03T22:24:57.434985: step 4602, loss 0.0669894, acc 0.96875\n",
      "2017-04-03T22:24:57.633215: step 4603, loss 0.0276171, acc 1\n",
      "2017-04-03T22:24:57.826140: step 4604, loss 0.0387198, acc 0.984375\n",
      "2017-04-03T22:24:58.023757: step 4605, loss 0.0626288, acc 0.984375\n",
      "2017-04-03T22:24:58.217791: step 4606, loss 0.0200693, acc 1\n",
      "2017-04-03T22:24:58.412501: step 4607, loss 0.0354308, acc 1\n",
      "2017-04-03T22:24:58.625739: step 4608, loss 0.0343848, acc 1\n",
      "2017-04-03T22:24:58.824455: step 4609, loss 0.0550526, acc 1\n",
      "2017-04-03T22:24:59.020531: step 4610, loss 0.129371, acc 0.96875\n",
      "2017-04-03T22:24:59.211893: step 4611, loss 0.041526, acc 1\n",
      "2017-04-03T22:24:59.401553: step 4612, loss 0.0190574, acc 1\n",
      "2017-04-03T22:24:59.599233: step 4613, loss 0.0729682, acc 0.96875\n",
      "2017-04-03T22:24:59.793634: step 4614, loss 0.108801, acc 0.9375\n",
      "2017-04-03T22:24:59.991361: step 4615, loss 0.0839574, acc 0.984375\n",
      "2017-04-03T22:25:00.184864: step 4616, loss 0.0783534, acc 0.96875\n",
      "2017-04-03T22:25:00.380700: step 4617, loss 0.0310062, acc 1\n",
      "2017-04-03T22:25:00.575196: step 4618, loss 0.068755, acc 0.96875\n",
      "2017-04-03T22:25:00.766896: step 4619, loss 0.113028, acc 0.953125\n",
      "2017-04-03T22:25:00.960385: step 4620, loss 0.0710502, acc 0.96875\n",
      "2017-04-03T22:25:01.154715: step 4621, loss 0.0736931, acc 0.984375\n",
      "2017-04-03T22:25:01.345683: step 4622, loss 0.0545829, acc 0.984375\n",
      "2017-04-03T22:25:01.538774: step 4623, loss 0.0611099, acc 0.96875\n",
      "2017-04-03T22:25:01.737223: step 4624, loss 0.0279229, acc 1\n",
      "2017-04-03T22:25:01.949188: step 4625, loss 0.0584981, acc 0.984375\n",
      "2017-04-03T22:25:02.164664: step 4626, loss 0.0587109, acc 0.96875\n",
      "2017-04-03T22:25:02.392795: step 4627, loss 0.0969507, acc 0.953125\n",
      "2017-04-03T22:25:02.616248: step 4628, loss 0.038085, acc 0.984375\n",
      "2017-04-03T22:25:02.844167: step 4629, loss 0.071399, acc 0.984375\n",
      "2017-04-03T22:25:03.059911: step 4630, loss 0.0724927, acc 0.984375\n",
      "2017-04-03T22:25:03.265931: step 4631, loss 0.0779997, acc 0.96875\n",
      "2017-04-03T22:25:03.469242: step 4632, loss 0.0529356, acc 1\n",
      "2017-04-03T22:25:03.664324: step 4633, loss 0.0423723, acc 0.984375\n",
      "2017-04-03T22:25:03.866732: step 4634, loss 0.0117373, acc 1\n",
      "2017-04-03T22:25:04.059287: step 4635, loss 0.0285959, acc 1\n",
      "2017-04-03T22:25:04.260841: step 4636, loss 0.0560354, acc 0.96875\n",
      "2017-04-03T22:25:04.462018: step 4637, loss 0.0293661, acc 0.984375\n",
      "2017-04-03T22:25:04.662654: step 4638, loss 0.0451418, acc 0.984375\n",
      "2017-04-03T22:25:04.870589: step 4639, loss 0.148728, acc 0.953125\n",
      "2017-04-03T22:25:05.059882: step 4640, loss 0.0209848, acc 1\n",
      "2017-04-03T22:25:05.253458: step 4641, loss 0.0740749, acc 0.96875\n",
      "2017-04-03T22:25:05.445486: step 4642, loss 0.0697473, acc 0.984375\n",
      "2017-04-03T22:25:05.645223: step 4643, loss 0.0416565, acc 0.984375\n",
      "2017-04-03T22:25:05.808703: step 4644, loss 0.120212, acc 0.961538\n",
      "2017-04-03T22:25:05.996079: step 4645, loss 0.175564, acc 0.9375\n",
      "2017-04-03T22:25:06.187422: step 4646, loss 0.0269205, acc 0.984375\n",
      "2017-04-03T22:25:06.383411: step 4647, loss 0.014892, acc 1\n",
      "2017-04-03T22:25:06.580669: step 4648, loss 0.0681685, acc 0.984375\n",
      "2017-04-03T22:25:06.773114: step 4649, loss 0.0433731, acc 1\n",
      "2017-04-03T22:25:06.964663: step 4650, loss 0.0902639, acc 0.953125\n",
      "2017-04-03T22:25:07.155081: step 4651, loss 0.0762533, acc 0.984375\n",
      "2017-04-03T22:25:07.346318: step 4652, loss 0.0107366, acc 1\n",
      "2017-04-03T22:25:07.559719: step 4653, loss 0.066382, acc 0.96875\n",
      "2017-04-03T22:25:07.768577: step 4654, loss 0.0445745, acc 0.984375\n",
      "2017-04-03T22:25:07.963441: step 4655, loss 0.0187431, acc 1\n",
      "2017-04-03T22:25:08.155134: step 4656, loss 0.127432, acc 0.953125\n",
      "2017-04-03T22:25:08.345828: step 4657, loss 0.0445371, acc 0.984375\n",
      "2017-04-03T22:25:08.537710: step 4658, loss 0.168106, acc 0.9375\n",
      "2017-04-03T22:25:08.731305: step 4659, loss 0.0595109, acc 0.984375\n",
      "2017-04-03T22:25:08.925066: step 4660, loss 0.0351413, acc 0.984375\n",
      "2017-04-03T22:25:09.121432: step 4661, loss 0.113014, acc 0.953125\n",
      "2017-04-03T22:25:09.319583: step 4662, loss 0.0397617, acc 1\n",
      "2017-04-03T22:25:09.512861: step 4663, loss 0.0150957, acc 1\n",
      "2017-04-03T22:25:09.708836: step 4664, loss 0.132624, acc 0.953125\n",
      "2017-04-03T22:25:09.901087: step 4665, loss 0.0359759, acc 1\n",
      "2017-04-03T22:25:10.093524: step 4666, loss 0.0656301, acc 0.984375\n",
      "2017-04-03T22:25:10.284537: step 4667, loss 0.0839497, acc 0.96875\n",
      "2017-04-03T22:25:10.481262: step 4668, loss 0.0113908, acc 1\n",
      "2017-04-03T22:25:10.686162: step 4669, loss 0.0796145, acc 0.96875\n",
      "2017-04-03T22:25:10.891959: step 4670, loss 0.0128306, acc 1\n",
      "2017-04-03T22:25:11.092278: step 4671, loss 0.0460311, acc 0.984375\n",
      "2017-04-03T22:25:11.291173: step 4672, loss 0.0513827, acc 0.96875\n",
      "2017-04-03T22:25:11.487513: step 4673, loss 0.06159, acc 0.984375\n",
      "2017-04-03T22:25:11.686448: step 4674, loss 0.0843849, acc 0.96875\n",
      "2017-04-03T22:25:11.881444: step 4675, loss 0.0557843, acc 0.984375\n",
      "2017-04-03T22:25:12.080136: step 4676, loss 0.119282, acc 0.953125\n",
      "2017-04-03T22:25:12.280182: step 4677, loss 0.0224202, acc 1\n",
      "2017-04-03T22:25:12.471912: step 4678, loss 0.0250581, acc 1\n",
      "2017-04-03T22:25:12.662886: step 4679, loss 0.0906829, acc 0.96875\n",
      "2017-04-03T22:25:12.857769: step 4680, loss 0.0841205, acc 0.96875\n",
      "2017-04-03T22:25:13.056262: step 4681, loss 0.133034, acc 0.953125\n",
      "2017-04-03T22:25:13.247231: step 4682, loss 0.0464978, acc 1\n",
      "2017-04-03T22:25:13.441204: step 4683, loss 0.033048, acc 0.984375\n",
      "2017-04-03T22:25:13.642496: step 4684, loss 0.092073, acc 0.96875\n",
      "2017-04-03T22:25:13.834424: step 4685, loss 0.0417925, acc 0.984375\n",
      "2017-04-03T22:25:14.028464: step 4686, loss 0.040983, acc 1\n",
      "2017-04-03T22:25:14.219643: step 4687, loss 0.0780302, acc 0.984375\n",
      "2017-04-03T22:25:14.413108: step 4688, loss 0.0657595, acc 0.984375\n",
      "2017-04-03T22:25:14.606825: step 4689, loss 0.0604919, acc 1\n",
      "2017-04-03T22:25:14.801244: step 4690, loss 0.0529468, acc 0.96875\n",
      "2017-04-03T22:25:15.009495: step 4691, loss 0.0421942, acc 1\n",
      "2017-04-03T22:25:15.214150: step 4692, loss 0.0567573, acc 0.96875\n",
      "2017-04-03T22:25:15.419808: step 4693, loss 0.0502852, acc 0.984375\n",
      "2017-04-03T22:25:15.622021: step 4694, loss 0.00616667, acc 1\n",
      "2017-04-03T22:25:15.815202: step 4695, loss 0.0405968, acc 1\n",
      "2017-04-03T22:25:16.009978: step 4696, loss 0.0422386, acc 1\n",
      "2017-04-03T22:25:16.203620: step 4697, loss 0.0483441, acc 0.984375\n",
      "2017-04-03T22:25:16.392586: step 4698, loss 0.0274576, acc 1\n",
      "2017-04-03T22:25:16.591881: step 4699, loss 0.0167741, acc 1\n",
      "2017-04-03T22:25:16.802895: step 4700, loss 0.0544562, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:25:17.489162: step 4700, loss 1.98257, acc 0.565274\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4700\n",
      "\n",
      "2017-04-03T22:25:17.751298: step 4701, loss 0.0705612, acc 0.96875\n",
      "2017-04-03T22:25:17.942626: step 4702, loss 0.0748438, acc 0.96875\n",
      "2017-04-03T22:25:18.133371: step 4703, loss 0.0227707, acc 1\n",
      "2017-04-03T22:25:18.326462: step 4704, loss 0.0614022, acc 0.96875\n",
      "2017-04-03T22:25:18.515789: step 4705, loss 0.150219, acc 0.96875\n",
      "2017-04-03T22:25:18.718997: step 4706, loss 0.0618191, acc 0.984375\n",
      "2017-04-03T22:25:18.923923: step 4707, loss 0.031639, acc 0.984375\n",
      "2017-04-03T22:25:19.116827: step 4708, loss 0.0893434, acc 0.96875\n",
      "2017-04-03T22:25:19.313176: step 4709, loss 0.047669, acc 0.984375\n",
      "2017-04-03T22:25:19.502912: step 4710, loss 0.0963024, acc 0.96875\n",
      "2017-04-03T22:25:19.700481: step 4711, loss 0.0534915, acc 0.984375\n",
      "2017-04-03T22:25:19.904546: step 4712, loss 0.0266568, acc 1\n",
      "2017-04-03T22:25:20.109964: step 4713, loss 0.0744637, acc 0.96875\n",
      "2017-04-03T22:25:20.303431: step 4714, loss 0.0462759, acc 1\n",
      "2017-04-03T22:25:20.495748: step 4715, loss 0.0492792, acc 0.984375\n",
      "2017-04-03T22:25:20.689186: step 4716, loss 0.105424, acc 0.953125\n",
      "2017-04-03T22:25:20.879117: step 4717, loss 0.0588264, acc 0.984375\n",
      "2017-04-03T22:25:21.068403: step 4718, loss 0.119266, acc 0.953125\n",
      "2017-04-03T22:25:21.262951: step 4719, loss 0.0263201, acc 1\n",
      "2017-04-03T22:25:21.458150: step 4720, loss 0.118456, acc 0.96875\n",
      "2017-04-03T22:25:21.652337: step 4721, loss 0.027467, acc 1\n",
      "2017-04-03T22:25:21.848232: step 4722, loss 0.0203701, acc 1\n",
      "2017-04-03T22:25:22.040538: step 4723, loss 0.0774505, acc 0.96875\n",
      "2017-04-03T22:25:22.230907: step 4724, loss 0.0512297, acc 0.984375\n",
      "2017-04-03T22:25:22.421940: step 4725, loss 0.0606785, acc 0.984375\n",
      "2017-04-03T22:25:22.617163: step 4726, loss 0.0820689, acc 0.953125\n",
      "2017-04-03T22:25:22.810809: step 4727, loss 0.026851, acc 1\n",
      "2017-04-03T22:25:23.003351: step 4728, loss 0.038449, acc 0.984375\n",
      "2017-04-03T22:25:23.198848: step 4729, loss 0.111416, acc 0.953125\n",
      "2017-04-03T22:25:23.394739: step 4730, loss 0.0603645, acc 0.96875\n",
      "2017-04-03T22:25:23.588954: step 4731, loss 0.0500764, acc 0.984375\n",
      "2017-04-03T22:25:23.779519: step 4732, loss 0.0322687, acc 1\n",
      "2017-04-03T22:25:23.969969: step 4733, loss 0.0196284, acc 1\n",
      "2017-04-03T22:25:24.164529: step 4734, loss 0.0284614, acc 1\n",
      "2017-04-03T22:25:24.357188: step 4735, loss 0.112107, acc 0.953125\n",
      "2017-04-03T22:25:24.549488: step 4736, loss 0.107274, acc 0.953125\n",
      "2017-04-03T22:25:24.742221: step 4737, loss 0.0276782, acc 1\n",
      "2017-04-03T22:25:24.939349: step 4738, loss 0.0835265, acc 0.984375\n",
      "2017-04-03T22:25:25.132802: step 4739, loss 0.0679782, acc 0.96875\n",
      "2017-04-03T22:25:25.321444: step 4740, loss 0.106412, acc 0.96875\n",
      "2017-04-03T22:25:25.520546: step 4741, loss 0.0133612, acc 1\n",
      "2017-04-03T22:25:25.710638: step 4742, loss 0.046575, acc 0.984375\n",
      "2017-04-03T22:25:25.903742: step 4743, loss 0.0163908, acc 1\n",
      "2017-04-03T22:25:26.094073: step 4744, loss 0.0250135, acc 1\n",
      "2017-04-03T22:25:26.288887: step 4745, loss 0.0580526, acc 0.984375\n",
      "2017-04-03T22:25:26.480480: step 4746, loss 0.0604663, acc 0.984375\n",
      "2017-04-03T22:25:26.671082: step 4747, loss 0.0492785, acc 0.984375\n",
      "2017-04-03T22:25:26.866440: step 4748, loss 0.106462, acc 0.96875\n",
      "2017-04-03T22:25:27.055330: step 4749, loss 0.046704, acc 0.984375\n",
      "2017-04-03T22:25:27.250232: step 4750, loss 0.0503098, acc 0.984375\n",
      "2017-04-03T22:25:27.447115: step 4751, loss 0.0222444, acc 1\n",
      "2017-04-03T22:25:27.611316: step 4752, loss 0.0981258, acc 0.961538\n",
      "2017-04-03T22:25:27.809400: step 4753, loss 0.0938095, acc 0.984375\n",
      "2017-04-03T22:25:28.003013: step 4754, loss 0.0548847, acc 0.984375\n",
      "2017-04-03T22:25:28.196305: step 4755, loss 0.0707591, acc 0.96875\n",
      "2017-04-03T22:25:28.388185: step 4756, loss 0.0125095, acc 1\n",
      "2017-04-03T22:25:28.584090: step 4757, loss 0.0386585, acc 1\n",
      "2017-04-03T22:25:28.788322: step 4758, loss 0.032598, acc 1\n",
      "2017-04-03T22:25:28.990647: step 4759, loss 0.0913753, acc 0.96875\n",
      "2017-04-03T22:25:29.194255: step 4760, loss 0.0656202, acc 0.96875\n",
      "2017-04-03T22:25:29.395142: step 4761, loss 0.106296, acc 0.953125\n",
      "2017-04-03T22:25:29.590927: step 4762, loss 0.0145108, acc 1\n",
      "2017-04-03T22:25:29.788278: step 4763, loss 0.0401884, acc 1\n",
      "2017-04-03T22:25:29.979557: step 4764, loss 0.0149904, acc 1\n",
      "2017-04-03T22:25:30.169023: step 4765, loss 0.0484652, acc 0.96875\n",
      "2017-04-03T22:25:30.359234: step 4766, loss 0.0364858, acc 0.984375\n",
      "2017-04-03T22:25:30.550616: step 4767, loss 0.0700486, acc 0.984375\n",
      "2017-04-03T22:25:30.769853: step 4768, loss 0.0835564, acc 0.9375\n",
      "2017-04-03T22:25:30.961948: step 4769, loss 0.0997856, acc 0.96875\n",
      "2017-04-03T22:25:31.155186: step 4770, loss 0.022666, acc 0.984375\n",
      "2017-04-03T22:25:31.349740: step 4771, loss 0.0524285, acc 0.984375\n",
      "2017-04-03T22:25:31.540932: step 4772, loss 0.0219965, acc 1\n",
      "2017-04-03T22:25:31.734485: step 4773, loss 0.0772941, acc 0.96875\n",
      "2017-04-03T22:25:31.927036: step 4774, loss 0.0117376, acc 1\n",
      "2017-04-03T22:25:32.120075: step 4775, loss 0.0410881, acc 1\n",
      "2017-04-03T22:25:32.314809: step 4776, loss 0.060263, acc 0.96875\n",
      "2017-04-03T22:25:32.504209: step 4777, loss 0.163607, acc 0.96875\n",
      "2017-04-03T22:25:32.695855: step 4778, loss 0.0138672, acc 1\n",
      "2017-04-03T22:25:32.887968: step 4779, loss 0.0760401, acc 0.953125\n",
      "2017-04-03T22:25:33.090702: step 4780, loss 0.0359629, acc 0.984375\n",
      "2017-04-03T22:25:33.293008: step 4781, loss 0.0965369, acc 0.96875\n",
      "2017-04-03T22:25:33.484104: step 4782, loss 0.117924, acc 0.96875\n",
      "2017-04-03T22:25:33.675336: step 4783, loss 0.0688205, acc 0.984375\n",
      "2017-04-03T22:25:33.871113: step 4784, loss 0.0553314, acc 0.984375\n",
      "2017-04-03T22:25:34.065040: step 4785, loss 0.0647031, acc 0.984375\n",
      "2017-04-03T22:25:34.255846: step 4786, loss 0.0715995, acc 0.96875\n",
      "2017-04-03T22:25:34.451297: step 4787, loss 0.060557, acc 0.96875\n",
      "2017-04-03T22:25:34.645058: step 4788, loss 0.0136304, acc 1\n",
      "2017-04-03T22:25:34.845677: step 4789, loss 0.019929, acc 1\n",
      "2017-04-03T22:25:35.042187: step 4790, loss 0.0540277, acc 0.96875\n",
      "2017-04-03T22:25:35.236302: step 4791, loss 0.0633767, acc 0.984375\n",
      "2017-04-03T22:25:35.433515: step 4792, loss 0.0553156, acc 0.96875\n",
      "2017-04-03T22:25:35.629151: step 4793, loss 0.146718, acc 0.953125\n",
      "2017-04-03T22:25:35.825470: step 4794, loss 0.0596849, acc 0.96875\n",
      "2017-04-03T22:25:36.030617: step 4795, loss 0.0971794, acc 0.953125\n",
      "2017-04-03T22:25:36.233292: step 4796, loss 0.0760449, acc 0.984375\n",
      "2017-04-03T22:25:36.424241: step 4797, loss 0.0685823, acc 0.96875\n",
      "2017-04-03T22:25:36.620594: step 4798, loss 0.0239711, acc 1\n",
      "2017-04-03T22:25:36.813148: step 4799, loss 0.0508764, acc 0.96875\n",
      "2017-04-03T22:25:37.008457: step 4800, loss 0.0969131, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:25:37.668723: step 4800, loss 2.02523, acc 0.558747\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4800\n",
      "\n",
      "2017-04-03T22:25:37.924969: step 4801, loss 0.0231862, acc 1\n",
      "2017-04-03T22:25:38.118239: step 4802, loss 0.112023, acc 0.953125\n",
      "2017-04-03T22:25:38.315565: step 4803, loss 0.0694485, acc 0.984375\n",
      "2017-04-03T22:25:38.506872: step 4804, loss 0.00824665, acc 1\n",
      "2017-04-03T22:25:38.700240: step 4805, loss 0.048829, acc 1\n",
      "2017-04-03T22:25:38.893994: step 4806, loss 0.0331623, acc 1\n",
      "2017-04-03T22:25:39.089066: step 4807, loss 0.0491694, acc 0.984375\n",
      "2017-04-03T22:25:39.276623: step 4808, loss 0.157273, acc 0.9375\n",
      "2017-04-03T22:25:39.505517: step 4809, loss 0.0965317, acc 0.953125\n",
      "2017-04-03T22:25:39.719996: step 4810, loss 0.0247902, acc 1\n",
      "2017-04-03T22:25:39.914326: step 4811, loss 0.0639915, acc 0.984375\n",
      "2017-04-03T22:25:40.105311: step 4812, loss 0.0253948, acc 1\n",
      "2017-04-03T22:25:40.298452: step 4813, loss 0.081082, acc 0.984375\n",
      "2017-04-03T22:25:40.501419: step 4814, loss 0.0206013, acc 1\n",
      "2017-04-03T22:25:40.699102: step 4815, loss 0.0363046, acc 1\n",
      "2017-04-03T22:25:40.899604: step 4816, loss 0.0600778, acc 1\n",
      "2017-04-03T22:25:41.094509: step 4817, loss 0.0452617, acc 0.984375\n",
      "2017-04-03T22:25:41.291484: step 4818, loss 0.00716801, acc 1\n",
      "2017-04-03T22:25:41.495189: step 4819, loss 0.091587, acc 0.953125\n",
      "2017-04-03T22:25:41.711445: step 4820, loss 0.0168732, acc 1\n",
      "2017-04-03T22:25:41.904734: step 4821, loss 0.0450582, acc 1\n",
      "2017-04-03T22:25:42.100523: step 4822, loss 0.0570634, acc 0.984375\n",
      "2017-04-03T22:25:42.290604: step 4823, loss 0.107599, acc 0.953125\n",
      "2017-04-03T22:25:42.483775: step 4824, loss 0.00429806, acc 1\n",
      "2017-04-03T22:25:42.679881: step 4825, loss 0.0233104, acc 0.984375\n",
      "2017-04-03T22:25:42.877655: step 4826, loss 0.0267157, acc 1\n",
      "2017-04-03T22:25:43.072022: step 4827, loss 0.0314993, acc 0.984375\n",
      "2017-04-03T22:25:43.265573: step 4828, loss 0.0524713, acc 0.96875\n",
      "2017-04-03T22:25:43.459812: step 4829, loss 0.0696039, acc 0.96875\n",
      "2017-04-03T22:25:43.661430: step 4830, loss 0.0355113, acc 0.984375\n",
      "2017-04-03T22:25:43.858552: step 4831, loss 0.0569855, acc 1\n",
      "2017-04-03T22:25:44.055344: step 4832, loss 0.159862, acc 0.9375\n",
      "2017-04-03T22:25:44.254116: step 4833, loss 0.0160159, acc 1\n",
      "2017-04-03T22:25:44.464531: step 4834, loss 0.0506449, acc 0.984375\n",
      "2017-04-03T22:25:44.681165: step 4835, loss 0.0414137, acc 0.984375\n",
      "2017-04-03T22:25:44.887135: step 4836, loss 0.0151487, acc 1\n",
      "2017-04-03T22:25:45.097947: step 4837, loss 0.0315865, acc 1\n",
      "2017-04-03T22:25:45.318610: step 4838, loss 0.0808462, acc 0.96875\n",
      "2017-04-03T22:25:45.514157: step 4839, loss 0.0423045, acc 0.984375\n",
      "2017-04-03T22:25:45.724822: step 4840, loss 0.0305652, acc 0.984375\n",
      "2017-04-03T22:25:45.929030: step 4841, loss 0.170383, acc 0.9375\n",
      "2017-04-03T22:25:46.128302: step 4842, loss 0.0775355, acc 0.96875\n",
      "2017-04-03T22:25:46.326792: step 4843, loss 0.101041, acc 0.96875\n",
      "2017-04-03T22:25:46.516143: step 4844, loss 0.0505802, acc 0.984375\n",
      "2017-04-03T22:25:46.715407: step 4845, loss 0.0762233, acc 0.96875\n",
      "2017-04-03T22:25:46.911309: step 4846, loss 0.0177958, acc 1\n",
      "2017-04-03T22:25:47.105781: step 4847, loss 0.0105484, acc 1\n",
      "2017-04-03T22:25:47.306375: step 4848, loss 0.0814451, acc 0.984375\n",
      "2017-04-03T22:25:47.516683: step 4849, loss 0.0633433, acc 0.984375\n",
      "2017-04-03T22:25:47.715818: step 4850, loss 0.251221, acc 0.984375\n",
      "2017-04-03T22:25:47.916438: step 4851, loss 0.0179803, acc 1\n",
      "2017-04-03T22:25:48.115178: step 4852, loss 0.0165628, acc 1\n",
      "2017-04-03T22:25:48.311750: step 4853, loss 0.105299, acc 0.96875\n",
      "2017-04-03T22:25:48.522383: step 4854, loss 0.0318794, acc 1\n",
      "2017-04-03T22:25:48.724493: step 4855, loss 0.0120638, acc 1\n",
      "2017-04-03T22:25:48.927251: step 4856, loss 0.0314471, acc 1\n",
      "2017-04-03T22:25:49.126887: step 4857, loss 0.0393596, acc 0.984375\n",
      "2017-04-03T22:25:49.323978: step 4858, loss 0.114834, acc 0.953125\n",
      "2017-04-03T22:25:49.522098: step 4859, loss 0.10808, acc 0.984375\n",
      "2017-04-03T22:25:49.699885: step 4860, loss 0.0487503, acc 0.980769\n",
      "2017-04-03T22:25:49.917539: step 4861, loss 0.0206139, acc 1\n",
      "2017-04-03T22:25:50.142068: step 4862, loss 0.0476097, acc 0.984375\n",
      "2017-04-03T22:25:50.346879: step 4863, loss 0.0771177, acc 0.953125\n",
      "2017-04-03T22:25:50.569425: step 4864, loss 0.00646924, acc 1\n",
      "2017-04-03T22:25:50.772854: step 4865, loss 0.106397, acc 0.953125\n",
      "2017-04-03T22:25:50.965925: step 4866, loss 0.0895803, acc 0.984375\n",
      "2017-04-03T22:25:51.162526: step 4867, loss 0.0259208, acc 0.984375\n",
      "2017-04-03T22:25:51.368832: step 4868, loss 0.106643, acc 0.953125\n",
      "2017-04-03T22:25:51.573978: step 4869, loss 0.0107549, acc 1\n",
      "2017-04-03T22:25:51.788883: step 4870, loss 0.0239271, acc 1\n",
      "2017-04-03T22:25:51.994015: step 4871, loss 0.00844926, acc 1\n",
      "2017-04-03T22:25:52.191527: step 4872, loss 0.0227287, acc 1\n",
      "2017-04-03T22:25:52.393551: step 4873, loss 0.057372, acc 0.984375\n",
      "2017-04-03T22:25:52.614373: step 4874, loss 0.0475464, acc 0.96875\n",
      "2017-04-03T22:25:52.825390: step 4875, loss 0.0519785, acc 0.96875\n",
      "2017-04-03T22:25:53.025958: step 4876, loss 0.0708239, acc 0.96875\n",
      "2017-04-03T22:25:53.237525: step 4877, loss 0.0316786, acc 0.984375\n",
      "2017-04-03T22:25:53.456037: step 4878, loss 0.0418817, acc 0.984375\n",
      "2017-04-03T22:25:53.656621: step 4879, loss 0.0378481, acc 0.984375\n",
      "2017-04-03T22:25:53.870909: step 4880, loss 0.0316768, acc 1\n",
      "2017-04-03T22:25:54.073673: step 4881, loss 0.0517831, acc 0.984375\n",
      "2017-04-03T22:25:54.291520: step 4882, loss 0.0494912, acc 1\n",
      "2017-04-03T22:25:54.495264: step 4883, loss 0.0955517, acc 0.984375\n",
      "2017-04-03T22:25:54.706625: step 4884, loss 0.0140192, acc 1\n",
      "2017-04-03T22:25:54.914418: step 4885, loss 0.11666, acc 0.984375\n",
      "2017-04-03T22:25:55.105406: step 4886, loss 0.0354386, acc 0.984375\n",
      "2017-04-03T22:25:55.300081: step 4887, loss 0.0154662, acc 1\n",
      "2017-04-03T22:25:55.495472: step 4888, loss 0.109286, acc 0.9375\n",
      "2017-04-03T22:25:55.696274: step 4889, loss 0.0233611, acc 1\n",
      "2017-04-03T22:25:55.892963: step 4890, loss 0.0471466, acc 0.984375\n",
      "2017-04-03T22:25:56.086119: step 4891, loss 0.0642253, acc 0.96875\n",
      "2017-04-03T22:25:56.278222: step 4892, loss 0.0111098, acc 1\n",
      "2017-04-03T22:25:56.474023: step 4893, loss 0.0436967, acc 1\n",
      "2017-04-03T22:25:56.668177: step 4894, loss 0.0753802, acc 0.96875\n",
      "2017-04-03T22:25:56.859465: step 4895, loss 0.0463488, acc 0.984375\n",
      "2017-04-03T22:25:57.054638: step 4896, loss 0.10852, acc 0.984375\n",
      "2017-04-03T22:25:57.247326: step 4897, loss 0.0909238, acc 0.96875\n",
      "2017-04-03T22:25:57.438556: step 4898, loss 0.0192961, acc 1\n",
      "2017-04-03T22:25:57.633867: step 4899, loss 0.0395264, acc 0.984375\n",
      "2017-04-03T22:25:57.826233: step 4900, loss 0.0405473, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:25:58.492070: step 4900, loss 2.00492, acc 0.560052\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-4900\n",
      "\n",
      "2017-04-03T22:25:58.763028: step 4901, loss 0.0354821, acc 0.984375\n",
      "2017-04-03T22:25:58.961005: step 4902, loss 0.0408495, acc 1\n",
      "2017-04-03T22:25:59.161996: step 4903, loss 0.0141581, acc 1\n",
      "2017-04-03T22:25:59.356629: step 4904, loss 0.0125037, acc 1\n",
      "2017-04-03T22:25:59.559169: step 4905, loss 0.0271158, acc 1\n",
      "2017-04-03T22:25:59.761733: step 4906, loss 0.0486858, acc 0.984375\n",
      "2017-04-03T22:25:59.967176: step 4907, loss 0.0423339, acc 0.984375\n",
      "2017-04-03T22:26:00.177796: step 4908, loss 0.12116, acc 0.96875\n",
      "2017-04-03T22:26:00.386411: step 4909, loss 0.0160688, acc 1\n",
      "2017-04-03T22:26:00.598303: step 4910, loss 0.0406937, acc 0.984375\n",
      "2017-04-03T22:26:00.815541: step 4911, loss 0.0201375, acc 1\n",
      "2017-04-03T22:26:01.020984: step 4912, loss 0.0211949, acc 1\n",
      "2017-04-03T22:26:01.237161: step 4913, loss 0.0830845, acc 0.984375\n",
      "2017-04-03T22:26:01.458779: step 4914, loss 0.101324, acc 0.953125\n",
      "2017-04-03T22:26:01.661381: step 4915, loss 0.0527838, acc 0.984375\n",
      "2017-04-03T22:26:01.860298: step 4916, loss 0.0796822, acc 0.984375\n",
      "2017-04-03T22:26:02.059489: step 4917, loss 0.0111843, acc 1\n",
      "2017-04-03T22:26:02.254958: step 4918, loss 0.0282607, acc 1\n",
      "2017-04-03T22:26:02.450022: step 4919, loss 0.0174356, acc 1\n",
      "2017-04-03T22:26:02.654741: step 4920, loss 0.0116631, acc 1\n",
      "2017-04-03T22:26:02.868031: step 4921, loss 0.0579415, acc 1\n",
      "2017-04-03T22:26:03.078232: step 4922, loss 0.0376102, acc 1\n",
      "2017-04-03T22:26:03.281707: step 4923, loss 0.0461638, acc 1\n",
      "2017-04-03T22:26:03.502157: step 4924, loss 0.0578754, acc 0.96875\n",
      "2017-04-03T22:26:03.695790: step 4925, loss 0.0374572, acc 0.984375\n",
      "2017-04-03T22:26:03.907207: step 4926, loss 0.0960456, acc 0.953125\n",
      "2017-04-03T22:26:04.098238: step 4927, loss 0.0545196, acc 0.96875\n",
      "2017-04-03T22:26:04.292537: step 4928, loss 0.0794579, acc 0.96875\n",
      "2017-04-03T22:26:04.486072: step 4929, loss 0.0571554, acc 0.96875\n",
      "2017-04-03T22:26:04.677897: step 4930, loss 0.066755, acc 0.984375\n",
      "2017-04-03T22:26:04.875303: step 4931, loss 0.0196904, acc 1\n",
      "2017-04-03T22:26:05.081512: step 4932, loss 0.0581683, acc 0.984375\n",
      "2017-04-03T22:26:05.276713: step 4933, loss 0.0185189, acc 1\n",
      "2017-04-03T22:26:05.476598: step 4934, loss 0.305486, acc 0.875\n",
      "2017-04-03T22:26:05.677956: step 4935, loss 0.132072, acc 0.953125\n",
      "2017-04-03T22:26:05.873365: step 4936, loss 0.1107, acc 0.96875\n",
      "2017-04-03T22:26:06.070285: step 4937, loss 0.0992464, acc 0.96875\n",
      "2017-04-03T22:26:06.267444: step 4938, loss 0.113423, acc 0.953125\n",
      "2017-04-03T22:26:06.465622: step 4939, loss 0.0623204, acc 0.96875\n",
      "2017-04-03T22:26:06.661896: step 4940, loss 0.0250141, acc 1\n",
      "2017-04-03T22:26:06.858556: step 4941, loss 0.0999789, acc 0.96875\n",
      "2017-04-03T22:26:07.051673: step 4942, loss 0.0273894, acc 1\n",
      "2017-04-03T22:26:07.248869: step 4943, loss 0.105261, acc 0.96875\n",
      "2017-04-03T22:26:07.448282: step 4944, loss 0.0600624, acc 0.96875\n",
      "2017-04-03T22:26:07.647305: step 4945, loss 0.0377806, acc 1\n",
      "2017-04-03T22:26:07.858773: step 4946, loss 0.0611638, acc 1\n",
      "2017-04-03T22:26:08.063549: step 4947, loss 0.0236354, acc 1\n",
      "2017-04-03T22:26:08.259043: step 4948, loss 0.187142, acc 0.96875\n",
      "2017-04-03T22:26:08.452399: step 4949, loss 0.0512371, acc 0.984375\n",
      "2017-04-03T22:26:08.647994: step 4950, loss 0.0947618, acc 0.96875\n",
      "2017-04-03T22:26:08.846619: step 4951, loss 0.0913987, acc 0.96875\n",
      "2017-04-03T22:26:09.041574: step 4952, loss 0.0471129, acc 0.984375\n",
      "2017-04-03T22:26:09.236526: step 4953, loss 0.0772442, acc 0.96875\n",
      "2017-04-03T22:26:09.435125: step 4954, loss 0.017486, acc 1\n",
      "2017-04-03T22:26:09.638273: step 4955, loss 0.0408944, acc 1\n",
      "2017-04-03T22:26:09.832016: step 4956, loss 0.0479117, acc 0.984375\n",
      "2017-04-03T22:26:10.031052: step 4957, loss 0.0373816, acc 0.984375\n",
      "2017-04-03T22:26:10.222394: step 4958, loss 0.0557439, acc 0.984375\n",
      "2017-04-03T22:26:10.413407: step 4959, loss 0.164943, acc 0.953125\n",
      "2017-04-03T22:26:10.605959: step 4960, loss 0.100705, acc 0.96875\n",
      "2017-04-03T22:26:10.799364: step 4961, loss 0.0824263, acc 0.984375\n",
      "2017-04-03T22:26:10.993954: step 4962, loss 0.0389587, acc 0.984375\n",
      "2017-04-03T22:26:11.186212: step 4963, loss 0.00959096, acc 1\n",
      "2017-04-03T22:26:11.382020: step 4964, loss 0.0798024, acc 0.984375\n",
      "2017-04-03T22:26:11.580435: step 4965, loss 0.0445031, acc 0.984375\n",
      "2017-04-03T22:26:11.773305: step 4966, loss 0.0246184, acc 1\n",
      "2017-04-03T22:26:11.968961: step 4967, loss 0.118222, acc 0.953125\n",
      "2017-04-03T22:26:12.147214: step 4968, loss 0.0699762, acc 0.980769\n",
      "2017-04-03T22:26:12.363139: step 4969, loss 0.0193318, acc 1\n",
      "2017-04-03T22:26:12.558307: step 4970, loss 0.0864764, acc 0.984375\n",
      "2017-04-03T22:26:12.755021: step 4971, loss 0.119707, acc 0.9375\n",
      "2017-04-03T22:26:12.951389: step 4972, loss 0.100949, acc 0.96875\n",
      "2017-04-03T22:26:13.145808: step 4973, loss 0.0185776, acc 1\n",
      "2017-04-03T22:26:13.339829: step 4974, loss 0.0423918, acc 0.984375\n",
      "2017-04-03T22:26:13.536447: step 4975, loss 0.0172663, acc 1\n",
      "2017-04-03T22:26:13.732957: step 4976, loss 0.0843562, acc 0.96875\n",
      "2017-04-03T22:26:13.927184: step 4977, loss 0.0461251, acc 0.984375\n",
      "2017-04-03T22:26:14.118553: step 4978, loss 0.0387894, acc 0.984375\n",
      "2017-04-03T22:26:14.315807: step 4979, loss 0.0378928, acc 0.984375\n",
      "2017-04-03T22:26:14.508366: step 4980, loss 0.0610367, acc 0.984375\n",
      "2017-04-03T22:26:14.708519: step 4981, loss 0.0567784, acc 0.96875\n",
      "2017-04-03T22:26:14.904890: step 4982, loss 0.0281923, acc 1\n",
      "2017-04-03T22:26:15.113970: step 4983, loss 0.0212208, acc 1\n",
      "2017-04-03T22:26:15.324149: step 4984, loss 0.0174388, acc 1\n",
      "2017-04-03T22:26:15.520702: step 4985, loss 0.090956, acc 0.984375\n",
      "2017-04-03T22:26:15.738396: step 4986, loss 0.0210547, acc 1\n",
      "2017-04-03T22:26:15.949650: step 4987, loss 0.133314, acc 0.953125\n",
      "2017-04-03T22:26:16.156030: step 4988, loss 0.0144875, acc 1\n",
      "2017-04-03T22:26:16.360001: step 4989, loss 0.124108, acc 0.953125\n",
      "2017-04-03T22:26:16.567302: step 4990, loss 0.0652415, acc 0.984375\n",
      "2017-04-03T22:26:16.763632: step 4991, loss 0.0322562, acc 0.984375\n",
      "2017-04-03T22:26:16.964675: step 4992, loss 0.0781051, acc 0.96875\n",
      "2017-04-03T22:26:17.162299: step 4993, loss 0.0148949, acc 1\n",
      "2017-04-03T22:26:17.369047: step 4994, loss 0.038418, acc 0.984375\n",
      "2017-04-03T22:26:17.568729: step 4995, loss 0.0439328, acc 0.984375\n",
      "2017-04-03T22:26:17.794674: step 4996, loss 0.0320979, acc 1\n",
      "2017-04-03T22:26:18.009085: step 4997, loss 0.0498283, acc 0.96875\n",
      "2017-04-03T22:26:18.217439: step 4998, loss 0.025556, acc 0.984375\n",
      "2017-04-03T22:26:18.413652: step 4999, loss 0.0201983, acc 1\n",
      "2017-04-03T22:26:18.613600: step 5000, loss 0.0618903, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:26:19.268007: step 5000, loss 2.03582, acc 0.570496\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5000\n",
      "\n",
      "2017-04-03T22:26:19.561700: step 5001, loss 0.0113371, acc 1\n",
      "2017-04-03T22:26:19.756065: step 5002, loss 0.0705141, acc 0.984375\n",
      "2017-04-03T22:26:19.951753: step 5003, loss 0.0732244, acc 0.96875\n",
      "2017-04-03T22:26:20.142094: step 5004, loss 0.0360682, acc 0.984375\n",
      "2017-04-03T22:26:20.334506: step 5005, loss 0.0446342, acc 0.984375\n",
      "2017-04-03T22:26:20.526699: step 5006, loss 0.0470118, acc 0.984375\n",
      "2017-04-03T22:26:20.721593: step 5007, loss 0.0137925, acc 1\n",
      "2017-04-03T22:26:20.919627: step 5008, loss 0.0702624, acc 0.96875\n",
      "2017-04-03T22:26:21.112820: step 5009, loss 0.017927, acc 1\n",
      "2017-04-03T22:26:21.304242: step 5010, loss 0.0399561, acc 0.984375\n",
      "2017-04-03T22:26:21.499500: step 5011, loss 0.0182789, acc 1\n",
      "2017-04-03T22:26:21.692581: step 5012, loss 0.128047, acc 0.9375\n",
      "2017-04-03T22:26:21.890297: step 5013, loss 0.0418739, acc 0.984375\n",
      "2017-04-03T22:26:22.086342: step 5014, loss 0.0396309, acc 0.96875\n",
      "2017-04-03T22:26:22.278614: step 5015, loss 0.0167113, acc 1\n",
      "2017-04-03T22:26:22.471422: step 5016, loss 0.0689274, acc 0.984375\n",
      "2017-04-03T22:26:22.664741: step 5017, loss 0.0699407, acc 0.96875\n",
      "2017-04-03T22:26:22.857170: step 5018, loss 0.0375455, acc 0.984375\n",
      "2017-04-03T22:26:23.048088: step 5019, loss 0.0487039, acc 0.984375\n",
      "2017-04-03T22:26:23.242445: step 5020, loss 0.0169816, acc 1\n",
      "2017-04-03T22:26:23.435326: step 5021, loss 0.0650378, acc 0.96875\n",
      "2017-04-03T22:26:23.626959: step 5022, loss 0.0435144, acc 0.984375\n",
      "2017-04-03T22:26:23.827296: step 5023, loss 0.0787228, acc 0.96875\n",
      "2017-04-03T22:26:24.023814: step 5024, loss 0.128762, acc 0.96875\n",
      "2017-04-03T22:26:24.222307: step 5025, loss 0.095736, acc 0.953125\n",
      "2017-04-03T22:26:24.421881: step 5026, loss 0.0859671, acc 0.953125\n",
      "2017-04-03T22:26:24.618878: step 5027, loss 0.0216242, acc 1\n",
      "2017-04-03T22:26:24.815274: step 5028, loss 0.0691363, acc 0.984375\n",
      "2017-04-03T22:26:25.016586: step 5029, loss 0.0557764, acc 0.984375\n",
      "2017-04-03T22:26:25.218748: step 5030, loss 0.00801482, acc 1\n",
      "2017-04-03T22:26:25.419726: step 5031, loss 0.0612206, acc 0.96875\n",
      "2017-04-03T22:26:25.630927: step 5032, loss 0.0797213, acc 0.96875\n",
      "2017-04-03T22:26:25.836501: step 5033, loss 0.0307529, acc 1\n",
      "2017-04-03T22:26:26.035070: step 5034, loss 0.00913213, acc 1\n",
      "2017-04-03T22:26:26.228125: step 5035, loss 0.0143459, acc 1\n",
      "2017-04-03T22:26:26.420720: step 5036, loss 0.0351964, acc 1\n",
      "2017-04-03T22:26:26.614048: step 5037, loss 0.0920463, acc 0.953125\n",
      "2017-04-03T22:26:26.806949: step 5038, loss 0.0574186, acc 0.984375\n",
      "2017-04-03T22:26:26.998830: step 5039, loss 0.0415425, acc 0.984375\n",
      "2017-04-03T22:26:27.189900: step 5040, loss 0.223593, acc 0.96875\n",
      "2017-04-03T22:26:27.380795: step 5041, loss 0.050033, acc 0.984375\n",
      "2017-04-03T22:26:27.576625: step 5042, loss 0.0834012, acc 0.96875\n",
      "2017-04-03T22:26:27.770142: step 5043, loss 0.167918, acc 0.96875\n",
      "2017-04-03T22:26:27.961241: step 5044, loss 0.0254982, acc 1\n",
      "2017-04-03T22:26:28.158727: step 5045, loss 0.0386519, acc 1\n",
      "2017-04-03T22:26:28.352542: step 5046, loss 0.0190673, acc 1\n",
      "2017-04-03T22:26:28.545079: step 5047, loss 0.0425013, acc 0.984375\n",
      "2017-04-03T22:26:28.740352: step 5048, loss 0.072211, acc 0.984375\n",
      "2017-04-03T22:26:28.935227: step 5049, loss 0.0497779, acc 0.984375\n",
      "2017-04-03T22:26:29.132739: step 5050, loss 0.0432625, acc 0.984375\n",
      "2017-04-03T22:26:29.324490: step 5051, loss 0.075609, acc 0.96875\n",
      "2017-04-03T22:26:29.516983: step 5052, loss 0.0147326, acc 1\n",
      "2017-04-03T22:26:29.713037: step 5053, loss 0.122784, acc 0.96875\n",
      "2017-04-03T22:26:29.907642: step 5054, loss 0.0907657, acc 0.96875\n",
      "2017-04-03T22:26:30.100615: step 5055, loss 0.0708535, acc 0.96875\n",
      "2017-04-03T22:26:30.291984: step 5056, loss 0.0463737, acc 0.984375\n",
      "2017-04-03T22:26:30.485082: step 5057, loss 0.0942023, acc 0.984375\n",
      "2017-04-03T22:26:30.681249: step 5058, loss 0.0218154, acc 0.984375\n",
      "2017-04-03T22:26:30.870205: step 5059, loss 0.0411459, acc 0.984375\n",
      "2017-04-03T22:26:31.060562: step 5060, loss 0.0346113, acc 1\n",
      "2017-04-03T22:26:31.252613: step 5061, loss 0.0211865, acc 1\n",
      "2017-04-03T22:26:31.446784: step 5062, loss 0.0529115, acc 0.984375\n",
      "2017-04-03T22:26:31.643178: step 5063, loss 0.092811, acc 0.953125\n",
      "2017-04-03T22:26:31.853438: step 5064, loss 0.0975033, acc 0.96875\n",
      "2017-04-03T22:26:32.054996: step 5065, loss 0.101285, acc 0.953125\n",
      "2017-04-03T22:26:32.252552: step 5066, loss 0.113311, acc 0.953125\n",
      "2017-04-03T22:26:32.448638: step 5067, loss 0.0595737, acc 0.984375\n",
      "2017-04-03T22:26:32.648295: step 5068, loss 0.0945906, acc 0.953125\n",
      "2017-04-03T22:26:32.857989: step 5069, loss 0.118192, acc 0.9375\n",
      "2017-04-03T22:26:33.066332: step 5070, loss 0.122183, acc 0.953125\n",
      "2017-04-03T22:26:33.258468: step 5071, loss 0.10997, acc 0.984375\n",
      "2017-04-03T22:26:33.448236: step 5072, loss 0.0980612, acc 0.96875\n",
      "2017-04-03T22:26:33.636666: step 5073, loss 0.0396388, acc 0.984375\n",
      "2017-04-03T22:26:33.827256: step 5074, loss 0.0218115, acc 1\n",
      "2017-04-03T22:26:34.021988: step 5075, loss 0.0337561, acc 0.984375\n",
      "2017-04-03T22:26:34.188418: step 5076, loss 0.0209687, acc 1\n",
      "2017-04-03T22:26:34.376306: step 5077, loss 0.0832471, acc 0.96875\n",
      "2017-04-03T22:26:34.569674: step 5078, loss 0.0903071, acc 0.984375\n",
      "2017-04-03T22:26:34.759918: step 5079, loss 0.00692, acc 1\n",
      "2017-04-03T22:26:34.954001: step 5080, loss 0.085339, acc 0.953125\n",
      "2017-04-03T22:26:35.146603: step 5081, loss 0.0191973, acc 1\n",
      "2017-04-03T22:26:35.340768: step 5082, loss 0.0782495, acc 0.96875\n",
      "2017-04-03T22:26:35.531839: step 5083, loss 0.0114239, acc 1\n",
      "2017-04-03T22:26:35.725177: step 5084, loss 0.0303053, acc 0.984375\n",
      "2017-04-03T22:26:35.917182: step 5085, loss 0.0735956, acc 0.984375\n",
      "2017-04-03T22:26:36.114897: step 5086, loss 0.0229975, acc 1\n",
      "2017-04-03T22:26:36.307506: step 5087, loss 0.0289017, acc 0.984375\n",
      "2017-04-03T22:26:36.505306: step 5088, loss 0.063558, acc 0.96875\n",
      "2017-04-03T22:26:36.712269: step 5089, loss 0.0276543, acc 1\n",
      "2017-04-03T22:26:36.919537: step 5090, loss 0.0728989, acc 0.96875\n",
      "2017-04-03T22:26:37.112824: step 5091, loss 0.00971257, acc 1\n",
      "2017-04-03T22:26:37.311322: step 5092, loss 0.0159016, acc 1\n",
      "2017-04-03T22:26:37.503170: step 5093, loss 0.0222512, acc 1\n",
      "2017-04-03T22:26:37.704299: step 5094, loss 0.113997, acc 0.953125\n",
      "2017-04-03T22:26:37.903263: step 5095, loss 0.0714871, acc 0.96875\n",
      "2017-04-03T22:26:38.108541: step 5096, loss 0.0228573, acc 1\n",
      "2017-04-03T22:26:38.299684: step 5097, loss 0.00733657, acc 1\n",
      "2017-04-03T22:26:38.497466: step 5098, loss 0.0481405, acc 0.96875\n",
      "2017-04-03T22:26:38.694782: step 5099, loss 0.0316128, acc 1\n",
      "2017-04-03T22:26:38.888628: step 5100, loss 0.0336214, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:26:39.545220: step 5100, loss 2.05854, acc 0.56658\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5100\n",
      "\n",
      "2017-04-03T22:26:39.793504: step 5101, loss 0.0695493, acc 0.96875\n",
      "2017-04-03T22:26:39.987725: step 5102, loss 0.070572, acc 0.984375\n",
      "2017-04-03T22:26:40.179480: step 5103, loss 0.084922, acc 0.96875\n",
      "2017-04-03T22:26:40.373155: step 5104, loss 0.0710395, acc 0.953125\n",
      "2017-04-03T22:26:40.564276: step 5105, loss 0.0367368, acc 0.984375\n",
      "2017-04-03T22:26:40.763003: step 5106, loss 0.0526065, acc 0.96875\n",
      "2017-04-03T22:26:40.957052: step 5107, loss 0.0869597, acc 0.984375\n",
      "2017-04-03T22:26:41.148426: step 5108, loss 0.0301891, acc 1\n",
      "2017-04-03T22:26:41.341831: step 5109, loss 0.0507678, acc 0.96875\n",
      "2017-04-03T22:26:41.535087: step 5110, loss 0.0447554, acc 0.984375\n",
      "2017-04-03T22:26:41.735760: step 5111, loss 0.0135036, acc 1\n",
      "2017-04-03T22:26:41.926357: step 5112, loss 0.12941, acc 0.9375\n",
      "2017-04-03T22:26:42.122152: step 5113, loss 0.0213494, acc 1\n",
      "2017-04-03T22:26:42.316087: step 5114, loss 0.0185576, acc 1\n",
      "2017-04-03T22:26:42.508788: step 5115, loss 0.0191938, acc 1\n",
      "2017-04-03T22:26:42.699622: step 5116, loss 0.0358726, acc 0.984375\n",
      "2017-04-03T22:26:42.891495: step 5117, loss 0.10318, acc 0.953125\n",
      "2017-04-03T22:26:43.080619: step 5118, loss 0.11699, acc 0.953125\n",
      "2017-04-03T22:26:43.278426: step 5119, loss 0.0561748, acc 0.96875\n",
      "2017-04-03T22:26:43.470800: step 5120, loss 0.118318, acc 0.953125\n",
      "2017-04-03T22:26:43.660749: step 5121, loss 0.0112354, acc 1\n",
      "2017-04-03T22:26:43.849572: step 5122, loss 0.0386995, acc 1\n",
      "2017-04-03T22:26:44.042152: step 5123, loss 0.0193558, acc 1\n",
      "2017-04-03T22:26:44.234725: step 5124, loss 0.0597826, acc 0.984375\n",
      "2017-04-03T22:26:44.426409: step 5125, loss 0.0460723, acc 0.984375\n",
      "2017-04-03T22:26:44.629390: step 5126, loss 0.0362228, acc 0.984375\n",
      "2017-04-03T22:26:44.834262: step 5127, loss 0.0640808, acc 0.984375\n",
      "2017-04-03T22:26:45.040349: step 5128, loss 0.0315522, acc 0.984375\n",
      "2017-04-03T22:26:45.240667: step 5129, loss 0.145359, acc 0.953125\n",
      "2017-04-03T22:26:45.430500: step 5130, loss 0.0537339, acc 0.984375\n",
      "2017-04-03T22:26:45.624644: step 5131, loss 0.0916342, acc 0.96875\n",
      "2017-04-03T22:26:45.821581: step 5132, loss 0.099067, acc 0.96875\n",
      "2017-04-03T22:26:46.020801: step 5133, loss 0.0762211, acc 0.984375\n",
      "2017-04-03T22:26:46.236523: step 5134, loss 0.0225901, acc 1\n",
      "2017-04-03T22:26:46.430605: step 5135, loss 0.0200549, acc 1\n",
      "2017-04-03T22:26:46.621005: step 5136, loss 0.0555307, acc 0.96875\n",
      "2017-04-03T22:26:46.816058: step 5137, loss 0.0100019, acc 1\n",
      "2017-04-03T22:26:47.016419: step 5138, loss 0.109099, acc 0.953125\n",
      "2017-04-03T22:26:47.221302: step 5139, loss 0.0971215, acc 0.96875\n",
      "2017-04-03T22:26:47.421754: step 5140, loss 0.0242261, acc 0.984375\n",
      "2017-04-03T22:26:47.617841: step 5141, loss 0.0355966, acc 0.984375\n",
      "2017-04-03T22:26:47.818632: step 5142, loss 0.0265243, acc 0.984375\n",
      "2017-04-03T22:26:48.026932: step 5143, loss 0.0825911, acc 0.96875\n",
      "2017-04-03T22:26:48.226173: step 5144, loss 0.0246103, acc 1\n",
      "2017-04-03T22:26:48.420167: step 5145, loss 0.0250724, acc 1\n",
      "2017-04-03T22:26:48.613420: step 5146, loss 0.0357938, acc 0.984375\n",
      "2017-04-03T22:26:48.811192: step 5147, loss 0.0953482, acc 0.96875\n",
      "2017-04-03T22:26:49.008779: step 5148, loss 0.0271197, acc 1\n",
      "2017-04-03T22:26:49.197753: step 5149, loss 0.109426, acc 0.96875\n",
      "2017-04-03T22:26:49.394250: step 5150, loss 0.0415769, acc 1\n",
      "2017-04-03T22:26:49.592427: step 5151, loss 0.0171182, acc 1\n",
      "2017-04-03T22:26:49.784886: step 5152, loss 0.0405235, acc 0.984375\n",
      "2017-04-03T22:26:49.977785: step 5153, loss 0.121446, acc 0.953125\n",
      "2017-04-03T22:26:50.168643: step 5154, loss 0.0191374, acc 1\n",
      "2017-04-03T22:26:50.363053: step 5155, loss 0.0417352, acc 1\n",
      "2017-04-03T22:26:50.554844: step 5156, loss 0.0492897, acc 0.984375\n",
      "2017-04-03T22:26:50.751888: step 5157, loss 0.106301, acc 0.9375\n",
      "2017-04-03T22:26:50.944952: step 5158, loss 0.0176847, acc 1\n",
      "2017-04-03T22:26:51.138739: step 5159, loss 0.0506489, acc 0.984375\n",
      "2017-04-03T22:26:51.331976: step 5160, loss 0.0409461, acc 0.984375\n",
      "2017-04-03T22:26:51.525750: step 5161, loss 0.0202426, acc 1\n",
      "2017-04-03T22:26:51.720801: step 5162, loss 0.0609935, acc 0.984375\n",
      "2017-04-03T22:26:51.916355: step 5163, loss 0.0202036, acc 1\n",
      "2017-04-03T22:26:52.138513: step 5164, loss 0.028336, acc 1\n",
      "2017-04-03T22:26:52.344531: step 5165, loss 0.155629, acc 0.9375\n",
      "2017-04-03T22:26:52.541408: step 5166, loss 0.0111989, acc 1\n",
      "2017-04-03T22:26:52.731209: step 5167, loss 0.0675395, acc 0.984375\n",
      "2017-04-03T22:26:52.928361: step 5168, loss 0.0397892, acc 1\n",
      "2017-04-03T22:26:53.118653: step 5169, loss 0.00852276, acc 1\n",
      "2017-04-03T22:26:53.314329: step 5170, loss 0.0688345, acc 0.96875\n",
      "2017-04-03T22:26:53.514210: step 5171, loss 0.0819256, acc 0.984375\n",
      "2017-04-03T22:26:53.708689: step 5172, loss 0.225632, acc 0.921875\n",
      "2017-04-03T22:26:53.901001: step 5173, loss 0.0414553, acc 0.984375\n",
      "2017-04-03T22:26:54.093801: step 5174, loss 0.043293, acc 0.984375\n",
      "2017-04-03T22:26:54.294312: step 5175, loss 0.038859, acc 0.984375\n",
      "2017-04-03T22:26:54.486743: step 5176, loss 0.128622, acc 0.953125\n",
      "2017-04-03T22:26:54.682604: step 5177, loss 0.0778039, acc 0.984375\n",
      "2017-04-03T22:26:54.877123: step 5178, loss 0.147635, acc 0.9375\n",
      "2017-04-03T22:26:55.074922: step 5179, loss 0.0262575, acc 1\n",
      "2017-04-03T22:26:55.273655: step 5180, loss 0.0790649, acc 0.984375\n",
      "2017-04-03T22:26:55.463147: step 5181, loss 0.00397364, acc 1\n",
      "2017-04-03T22:26:55.655622: step 5182, loss 0.0655441, acc 0.984375\n",
      "2017-04-03T22:26:55.852657: step 5183, loss 0.0247686, acc 1\n",
      "2017-04-03T22:26:56.021004: step 5184, loss 0.0649049, acc 0.980769\n",
      "2017-04-03T22:26:56.220040: step 5185, loss 0.0127644, acc 1\n",
      "2017-04-03T22:26:56.417641: step 5186, loss 0.0785143, acc 0.96875\n",
      "2017-04-03T22:26:56.615286: step 5187, loss 0.0208967, acc 0.984375\n",
      "2017-04-03T22:26:56.805009: step 5188, loss 0.0165087, acc 1\n",
      "2017-04-03T22:26:57.001540: step 5189, loss 0.0178505, acc 0.984375\n",
      "2017-04-03T22:26:57.206580: step 5190, loss 0.0827009, acc 0.984375\n",
      "2017-04-03T22:26:57.419917: step 5191, loss 0.0139863, acc 1\n",
      "2017-04-03T22:26:57.608031: step 5192, loss 0.0616561, acc 0.96875\n",
      "2017-04-03T22:26:57.798304: step 5193, loss 0.109466, acc 0.953125\n",
      "2017-04-03T22:26:57.994143: step 5194, loss 0.0081375, acc 1\n",
      "2017-04-03T22:26:58.188638: step 5195, loss 0.0195775, acc 1\n",
      "2017-04-03T22:26:58.382813: step 5196, loss 0.0392586, acc 1\n",
      "2017-04-03T22:26:58.576586: step 5197, loss 0.0263992, acc 0.984375\n",
      "2017-04-03T22:26:58.770587: step 5198, loss 0.0347691, acc 1\n",
      "2017-04-03T22:26:58.966653: step 5199, loss 0.0704405, acc 1\n",
      "2017-04-03T22:26:59.160773: step 5200, loss 0.0883816, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:26:59.821784: step 5200, loss 2.0562, acc 0.56658\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5200\n",
      "\n",
      "2017-04-03T22:27:00.090797: step 5201, loss 0.0141803, acc 1\n",
      "2017-04-03T22:27:00.293530: step 5202, loss 0.0438571, acc 0.984375\n",
      "2017-04-03T22:27:00.484911: step 5203, loss 0.0179433, acc 1\n",
      "2017-04-03T22:27:00.678798: step 5204, loss 0.0346833, acc 0.984375\n",
      "2017-04-03T22:27:00.871804: step 5205, loss 0.0142414, acc 1\n",
      "2017-04-03T22:27:01.065528: step 5206, loss 0.0180388, acc 0.984375\n",
      "2017-04-03T22:27:01.260390: step 5207, loss 0.0612586, acc 0.96875\n",
      "2017-04-03T22:27:01.450875: step 5208, loss 0.139342, acc 0.953125\n",
      "2017-04-03T22:27:01.643397: step 5209, loss 0.0837035, acc 0.96875\n",
      "2017-04-03T22:27:01.838398: step 5210, loss 0.0312835, acc 1\n",
      "2017-04-03T22:27:02.032853: step 5211, loss 0.072113, acc 0.96875\n",
      "2017-04-03T22:27:02.224192: step 5212, loss 0.0897819, acc 0.96875\n",
      "2017-04-03T22:27:02.420572: step 5213, loss 0.499771, acc 0.9375\n",
      "2017-04-03T22:27:02.619751: step 5214, loss 0.0387885, acc 0.984375\n",
      "2017-04-03T22:27:02.815611: step 5215, loss 0.0283933, acc 0.984375\n",
      "2017-04-03T22:27:03.017838: step 5216, loss 0.0430395, acc 1\n",
      "2017-04-03T22:27:03.210157: step 5217, loss 0.0624448, acc 0.984375\n",
      "2017-04-03T22:27:03.400599: step 5218, loss 0.0766508, acc 0.953125\n",
      "2017-04-03T22:27:03.593255: step 5219, loss 0.0335093, acc 0.984375\n",
      "2017-04-03T22:27:03.786025: step 5220, loss 0.0331995, acc 1\n",
      "2017-04-03T22:27:03.981381: step 5221, loss 0.0292177, acc 1\n",
      "2017-04-03T22:27:04.174731: step 5222, loss 0.068029, acc 0.984375\n",
      "2017-04-03T22:27:04.371627: step 5223, loss 0.0202299, acc 1\n",
      "2017-04-03T22:27:04.564149: step 5224, loss 0.0124897, acc 1\n",
      "2017-04-03T22:27:04.756933: step 5225, loss 0.0247378, acc 0.984375\n",
      "2017-04-03T22:27:04.950140: step 5226, loss 0.192768, acc 0.921875\n",
      "2017-04-03T22:27:05.141987: step 5227, loss 0.155201, acc 0.9375\n",
      "2017-04-03T22:27:05.335630: step 5228, loss 0.0648298, acc 0.953125\n",
      "2017-04-03T22:27:05.525247: step 5229, loss 0.016868, acc 1\n",
      "2017-04-03T22:27:05.720399: step 5230, loss 0.0694754, acc 0.984375\n",
      "2017-04-03T22:27:05.911702: step 5231, loss 0.0819713, acc 0.953125\n",
      "2017-04-03T22:27:06.106321: step 5232, loss 0.10631, acc 0.96875\n",
      "2017-04-03T22:27:06.301102: step 5233, loss 0.0161635, acc 1\n",
      "2017-04-03T22:27:06.496256: step 5234, loss 0.0770981, acc 0.96875\n",
      "2017-04-03T22:27:06.688066: step 5235, loss 0.0354573, acc 0.984375\n",
      "2017-04-03T22:27:06.877737: step 5236, loss 0.112298, acc 0.96875\n",
      "2017-04-03T22:27:07.067142: step 5237, loss 0.138821, acc 0.9375\n",
      "2017-04-03T22:27:07.261608: step 5238, loss 0.0157722, acc 1\n",
      "2017-04-03T22:27:07.452495: step 5239, loss 0.0177386, acc 1\n",
      "2017-04-03T22:27:07.644122: step 5240, loss 0.00874757, acc 1\n",
      "2017-04-03T22:27:07.838866: step 5241, loss 0.0246111, acc 1\n",
      "2017-04-03T22:27:08.035850: step 5242, loss 0.0615443, acc 0.96875\n",
      "2017-04-03T22:27:08.228403: step 5243, loss 0.108228, acc 0.953125\n",
      "2017-04-03T22:27:08.421160: step 5244, loss 0.0232709, acc 1\n",
      "2017-04-03T22:27:08.616460: step 5245, loss 0.077876, acc 0.984375\n",
      "2017-04-03T22:27:08.813146: step 5246, loss 0.00882798, acc 1\n",
      "2017-04-03T22:27:09.010878: step 5247, loss 0.0235304, acc 0.984375\n",
      "2017-04-03T22:27:09.221059: step 5248, loss 0.0193366, acc 1\n",
      "2017-04-03T22:27:09.420013: step 5249, loss 0.0839432, acc 0.984375\n",
      "2017-04-03T22:27:09.619783: step 5250, loss 0.0687185, acc 0.984375\n",
      "2017-04-03T22:27:09.811991: step 5251, loss 0.0710329, acc 0.96875\n",
      "2017-04-03T22:27:10.003204: step 5252, loss 0.0570046, acc 0.96875\n",
      "2017-04-03T22:27:10.206225: step 5253, loss 0.106662, acc 0.953125\n",
      "2017-04-03T22:27:10.412537: step 5254, loss 0.0139656, acc 1\n",
      "2017-04-03T22:27:10.616394: step 5255, loss 0.0267973, acc 0.984375\n",
      "2017-04-03T22:27:10.818920: step 5256, loss 0.0764098, acc 0.96875\n",
      "2017-04-03T22:27:11.029243: step 5257, loss 0.0741108, acc 0.96875\n",
      "2017-04-03T22:27:11.232031: step 5258, loss 0.0364813, acc 0.984375\n",
      "2017-04-03T22:27:11.439780: step 5259, loss 0.0504013, acc 0.96875\n",
      "2017-04-03T22:27:11.645845: step 5260, loss 0.0154332, acc 1\n",
      "2017-04-03T22:27:11.854115: step 5261, loss 0.07987, acc 0.96875\n",
      "2017-04-03T22:27:12.058736: step 5262, loss 0.0125772, acc 1\n",
      "2017-04-03T22:27:12.253251: step 5263, loss 0.1348, acc 0.9375\n",
      "2017-04-03T22:27:12.449334: step 5264, loss 0.0428491, acc 1\n",
      "2017-04-03T22:27:12.649530: step 5265, loss 0.0197019, acc 1\n",
      "2017-04-03T22:27:12.844372: step 5266, loss 0.0178064, acc 1\n",
      "2017-04-03T22:27:13.050536: step 5267, loss 0.0099236, acc 1\n",
      "2017-04-03T22:27:13.244526: step 5268, loss 0.135619, acc 0.96875\n",
      "2017-04-03T22:27:13.443293: step 5269, loss 0.0326541, acc 1\n",
      "2017-04-03T22:27:13.640242: step 5270, loss 0.0995842, acc 0.953125\n",
      "2017-04-03T22:27:13.852998: step 5271, loss 0.0925251, acc 0.953125\n",
      "2017-04-03T22:27:14.045470: step 5272, loss 0.104962, acc 0.96875\n",
      "2017-04-03T22:27:14.240792: step 5273, loss 0.0850029, acc 0.96875\n",
      "2017-04-03T22:27:14.445874: step 5274, loss 0.0547563, acc 1\n",
      "2017-04-03T22:27:14.648365: step 5275, loss 0.0958046, acc 0.96875\n",
      "2017-04-03T22:27:14.849909: step 5276, loss 0.0157481, acc 1\n",
      "2017-04-03T22:27:15.041817: step 5277, loss 0.113407, acc 0.96875\n",
      "2017-04-03T22:27:15.231565: step 5278, loss 0.0441317, acc 1\n",
      "2017-04-03T22:27:15.426469: step 5279, loss 0.0275038, acc 0.984375\n",
      "2017-04-03T22:27:15.623368: step 5280, loss 0.0924269, acc 0.953125\n",
      "2017-04-03T22:27:15.817155: step 5281, loss 0.054491, acc 0.984375\n",
      "2017-04-03T22:27:16.015539: step 5282, loss 0.0723238, acc 0.96875\n",
      "2017-04-03T22:27:16.228871: step 5283, loss 0.0890966, acc 0.96875\n",
      "2017-04-03T22:27:16.419138: step 5284, loss 0.0170885, acc 1\n",
      "2017-04-03T22:27:16.612410: step 5285, loss 0.0734347, acc 0.96875\n",
      "2017-04-03T22:27:16.832786: step 5286, loss 0.085105, acc 0.96875\n",
      "2017-04-03T22:27:17.048511: step 5287, loss 0.0615122, acc 0.96875\n",
      "2017-04-03T22:27:17.242429: step 5288, loss 0.067213, acc 0.96875\n",
      "2017-04-03T22:27:17.435963: step 5289, loss 0.0271757, acc 1\n",
      "2017-04-03T22:27:17.630665: step 5290, loss 0.108702, acc 0.96875\n",
      "2017-04-03T22:27:17.832293: step 5291, loss 0.0887045, acc 0.953125\n",
      "2017-04-03T22:27:17.992326: step 5292, loss 0.177934, acc 0.923077\n",
      "2017-04-03T22:27:18.193405: step 5293, loss 0.0188874, acc 1\n",
      "2017-04-03T22:27:18.385219: step 5294, loss 0.0452663, acc 0.984375\n",
      "2017-04-03T22:27:18.580624: step 5295, loss 0.117206, acc 0.984375\n",
      "2017-04-03T22:27:18.771935: step 5296, loss 0.0506486, acc 1\n",
      "2017-04-03T22:27:18.963442: step 5297, loss 0.0882367, acc 0.96875\n",
      "2017-04-03T22:27:19.156356: step 5298, loss 0.0676548, acc 0.96875\n",
      "2017-04-03T22:27:19.347920: step 5299, loss 0.02302, acc 1\n",
      "2017-04-03T22:27:19.543522: step 5300, loss 0.0822883, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:27:20.208502: step 5300, loss 2.10101, acc 0.55483\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5300\n",
      "\n",
      "2017-04-03T22:27:20.476869: step 5301, loss 0.053129, acc 0.984375\n",
      "2017-04-03T22:27:20.670406: step 5302, loss 0.0974016, acc 0.953125\n",
      "2017-04-03T22:27:20.879859: step 5303, loss 0.0399913, acc 0.984375\n",
      "2017-04-03T22:27:21.077185: step 5304, loss 0.0297566, acc 0.984375\n",
      "2017-04-03T22:27:21.275907: step 5305, loss 0.0363289, acc 0.984375\n",
      "2017-04-03T22:27:21.469406: step 5306, loss 0.0785076, acc 0.96875\n",
      "2017-04-03T22:27:21.670532: step 5307, loss 0.0106262, acc 1\n",
      "2017-04-03T22:27:21.869955: step 5308, loss 0.0601997, acc 0.984375\n",
      "2017-04-03T22:27:22.075379: step 5309, loss 0.0156411, acc 1\n",
      "2017-04-03T22:27:22.272651: step 5310, loss 0.0321745, acc 0.984375\n",
      "2017-04-03T22:27:22.467751: step 5311, loss 0.00714468, acc 1\n",
      "2017-04-03T22:27:22.683548: step 5312, loss 0.0406521, acc 0.984375\n",
      "2017-04-03T22:27:22.886233: step 5313, loss 0.0820697, acc 0.984375\n",
      "2017-04-03T22:27:23.097998: step 5314, loss 0.0361261, acc 0.984375\n",
      "2017-04-03T22:27:23.308388: step 5315, loss 0.113262, acc 0.953125\n",
      "2017-04-03T22:27:23.509623: step 5316, loss 0.0501508, acc 0.984375\n",
      "2017-04-03T22:27:23.708034: step 5317, loss 0.0317227, acc 1\n",
      "2017-04-03T22:27:23.910842: step 5318, loss 0.0404321, acc 0.984375\n",
      "2017-04-03T22:27:24.119877: step 5319, loss 0.107614, acc 0.953125\n",
      "2017-04-03T22:27:24.323258: step 5320, loss 0.0362365, acc 0.984375\n",
      "2017-04-03T22:27:24.539174: step 5321, loss 0.109579, acc 0.953125\n",
      "2017-04-03T22:27:24.748830: step 5322, loss 0.0367122, acc 0.984375\n",
      "2017-04-03T22:27:24.955103: step 5323, loss 0.0938891, acc 0.96875\n",
      "2017-04-03T22:27:25.154105: step 5324, loss 0.0158622, acc 1\n",
      "2017-04-03T22:27:25.346261: step 5325, loss 0.0518991, acc 0.984375\n",
      "2017-04-03T22:27:25.543510: step 5326, loss 0.0574614, acc 0.984375\n",
      "2017-04-03T22:27:25.754758: step 5327, loss 0.0359972, acc 0.984375\n",
      "2017-04-03T22:27:25.957771: step 5328, loss 0.0525017, acc 0.96875\n",
      "2017-04-03T22:27:26.156637: step 5329, loss 0.0127162, acc 1\n",
      "2017-04-03T22:27:26.352178: step 5330, loss 0.0565516, acc 0.96875\n",
      "2017-04-03T22:27:26.560665: step 5331, loss 0.17108, acc 0.96875\n",
      "2017-04-03T22:27:26.762792: step 5332, loss 0.0595297, acc 0.96875\n",
      "2017-04-03T22:27:26.962763: step 5333, loss 0.0479874, acc 0.984375\n",
      "2017-04-03T22:27:27.159717: step 5334, loss 0.0316408, acc 0.984375\n",
      "2017-04-03T22:27:27.357734: step 5335, loss 0.054944, acc 0.96875\n",
      "2017-04-03T22:27:27.569472: step 5336, loss 0.0918223, acc 0.9375\n",
      "2017-04-03T22:27:27.791089: step 5337, loss 0.0805026, acc 0.96875\n",
      "2017-04-03T22:27:28.009606: step 5338, loss 0.0768613, acc 0.984375\n",
      "2017-04-03T22:27:28.231988: step 5339, loss 0.0369129, acc 0.984375\n",
      "2017-04-03T22:27:28.448706: step 5340, loss 0.0417143, acc 0.984375\n",
      "2017-04-03T22:27:28.662004: step 5341, loss 0.0502647, acc 0.96875\n",
      "2017-04-03T22:27:28.875546: step 5342, loss 0.0555851, acc 0.984375\n",
      "2017-04-03T22:27:29.092360: step 5343, loss 0.0514926, acc 0.984375\n",
      "2017-04-03T22:27:29.296505: step 5344, loss 0.122305, acc 0.953125\n",
      "2017-04-03T22:27:29.506350: step 5345, loss 0.0365422, acc 0.984375\n",
      "2017-04-03T22:27:29.704567: step 5346, loss 0.0888881, acc 0.984375\n",
      "2017-04-03T22:27:29.909002: step 5347, loss 0.0503344, acc 0.984375\n",
      "2017-04-03T22:27:30.101267: step 5348, loss 0.0517554, acc 0.96875\n",
      "2017-04-03T22:27:30.291734: step 5349, loss 0.0667857, acc 0.96875\n",
      "2017-04-03T22:27:30.483720: step 5350, loss 0.0415871, acc 1\n",
      "2017-04-03T22:27:30.689022: step 5351, loss 0.0265079, acc 1\n",
      "2017-04-03T22:27:30.884237: step 5352, loss 0.0719562, acc 0.96875\n",
      "2017-04-03T22:27:31.089190: step 5353, loss 0.055655, acc 0.96875\n",
      "2017-04-03T22:27:31.286628: step 5354, loss 0.0567908, acc 0.984375\n",
      "2017-04-03T22:27:31.481135: step 5355, loss 0.0524132, acc 0.984375\n",
      "2017-04-03T22:27:31.675077: step 5356, loss 0.11028, acc 0.96875\n",
      "2017-04-03T22:27:31.874239: step 5357, loss 0.0305356, acc 0.984375\n",
      "2017-04-03T22:27:32.066021: step 5358, loss 0.0455903, acc 0.984375\n",
      "2017-04-03T22:27:32.261043: step 5359, loss 0.079371, acc 0.96875\n",
      "2017-04-03T22:27:32.456500: step 5360, loss 0.0952683, acc 0.96875\n",
      "2017-04-03T22:27:32.668205: step 5361, loss 0.0806046, acc 0.96875\n",
      "2017-04-03T22:27:32.859641: step 5362, loss 0.0103911, acc 1\n",
      "2017-04-03T22:27:33.051319: step 5363, loss 0.0292906, acc 1\n",
      "2017-04-03T22:27:33.243888: step 5364, loss 0.0616009, acc 0.984375\n",
      "2017-04-03T22:27:33.438948: step 5365, loss 0.00547419, acc 1\n",
      "2017-04-03T22:27:33.632424: step 5366, loss 0.0244443, acc 1\n",
      "2017-04-03T22:27:33.826027: step 5367, loss 0.0572574, acc 0.96875\n",
      "2017-04-03T22:27:34.021445: step 5368, loss 0.0521437, acc 0.984375\n",
      "2017-04-03T22:27:34.216673: step 5369, loss 0.0655096, acc 0.96875\n",
      "2017-04-03T22:27:34.411414: step 5370, loss 0.0775864, acc 0.984375\n",
      "2017-04-03T22:27:34.605000: step 5371, loss 0.103863, acc 0.96875\n",
      "2017-04-03T22:27:34.800234: step 5372, loss 0.0136973, acc 1\n",
      "2017-04-03T22:27:34.991788: step 5373, loss 0.00371011, acc 1\n",
      "2017-04-03T22:27:35.181423: step 5374, loss 0.0736283, acc 0.96875\n",
      "2017-04-03T22:27:35.377783: step 5375, loss 0.0815431, acc 0.96875\n",
      "2017-04-03T22:27:35.572957: step 5376, loss 0.0466388, acc 0.984375\n",
      "2017-04-03T22:27:35.766098: step 5377, loss 0.116269, acc 0.984375\n",
      "2017-04-03T22:27:35.959618: step 5378, loss 0.072319, acc 0.96875\n",
      "2017-04-03T22:27:36.153215: step 5379, loss 0.0318863, acc 0.984375\n",
      "2017-04-03T22:27:36.344864: step 5380, loss 0.0619028, acc 0.984375\n",
      "2017-04-03T22:27:36.537343: step 5381, loss 0.039933, acc 0.984375\n",
      "2017-04-03T22:27:36.728187: step 5382, loss 0.135046, acc 0.96875\n",
      "2017-04-03T22:27:36.921236: step 5383, loss 0.0368142, acc 0.984375\n",
      "2017-04-03T22:27:37.113413: step 5384, loss 0.0665202, acc 0.984375\n",
      "2017-04-03T22:27:37.303334: step 5385, loss 0.0664291, acc 0.984375\n",
      "2017-04-03T22:27:37.493025: step 5386, loss 0.0562747, acc 0.984375\n",
      "2017-04-03T22:27:37.684579: step 5387, loss 0.0744377, acc 0.96875\n",
      "2017-04-03T22:27:37.880089: step 5388, loss 0.00694218, acc 1\n",
      "2017-04-03T22:27:38.070807: step 5389, loss 0.0191439, acc 1\n",
      "2017-04-03T22:27:38.264297: step 5390, loss 0.0645193, acc 1\n",
      "2017-04-03T22:27:38.467800: step 5391, loss 0.0488509, acc 0.984375\n",
      "2017-04-03T22:27:38.674151: step 5392, loss 0.0309219, acc 1\n",
      "2017-04-03T22:27:38.864603: step 5393, loss 0.0687059, acc 0.984375\n",
      "2017-04-03T22:27:39.054387: step 5394, loss 0.0363732, acc 0.984375\n",
      "2017-04-03T22:27:39.247497: step 5395, loss 0.0601984, acc 0.96875\n",
      "2017-04-03T22:27:39.441845: step 5396, loss 0.0401614, acc 1\n",
      "2017-04-03T22:27:39.632370: step 5397, loss 0.131595, acc 0.953125\n",
      "2017-04-03T22:27:39.833766: step 5398, loss 0.0452265, acc 0.984375\n",
      "2017-04-03T22:27:40.031672: step 5399, loss 0.0432053, acc 1\n",
      "2017-04-03T22:27:40.193938: step 5400, loss 0.049103, acc 0.980769\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:27:40.857439: step 5400, loss 2.10654, acc 0.573107\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5400\n",
      "\n",
      "2017-04-03T22:27:41.119817: step 5401, loss 0.0403882, acc 0.984375\n",
      "2017-04-03T22:27:41.311352: step 5402, loss 0.0607194, acc 0.984375\n",
      "2017-04-03T22:27:41.505040: step 5403, loss 0.00495554, acc 1\n",
      "2017-04-03T22:27:41.694127: step 5404, loss 0.0160944, acc 1\n",
      "2017-04-03T22:27:41.891335: step 5405, loss 0.0447538, acc 1\n",
      "2017-04-03T22:27:42.088289: step 5406, loss 0.0908914, acc 0.96875\n",
      "2017-04-03T22:27:42.280401: step 5407, loss 0.00846832, acc 1\n",
      "2017-04-03T22:27:42.472431: step 5408, loss 0.0288057, acc 0.984375\n",
      "2017-04-03T22:27:42.664002: step 5409, loss 0.0688552, acc 0.96875\n",
      "2017-04-03T22:27:42.856392: step 5410, loss 0.0675436, acc 0.96875\n",
      "2017-04-03T22:27:43.051859: step 5411, loss 0.145705, acc 0.9375\n",
      "2017-04-03T22:27:43.245371: step 5412, loss 0.0370661, acc 0.984375\n",
      "2017-04-03T22:27:43.436473: step 5413, loss 0.0262932, acc 1\n",
      "2017-04-03T22:27:43.627177: step 5414, loss 0.0496593, acc 0.984375\n",
      "2017-04-03T22:27:43.819952: step 5415, loss 0.0530845, acc 0.984375\n",
      "2017-04-03T22:27:44.021622: step 5416, loss 0.0478919, acc 1\n",
      "2017-04-03T22:27:44.234221: step 5417, loss 0.0103484, acc 1\n",
      "2017-04-03T22:27:44.424418: step 5418, loss 0.0520769, acc 0.984375\n",
      "2017-04-03T22:27:44.619688: step 5419, loss 0.0149877, acc 1\n",
      "2017-04-03T22:27:44.813971: step 5420, loss 0.072286, acc 0.953125\n",
      "2017-04-03T22:27:45.004411: step 5421, loss 0.0299067, acc 0.984375\n",
      "2017-04-03T22:27:45.196104: step 5422, loss 0.0757027, acc 0.953125\n",
      "2017-04-03T22:27:45.387317: step 5423, loss 0.022664, acc 1\n",
      "2017-04-03T22:27:45.584134: step 5424, loss 0.0050339, acc 1\n",
      "2017-04-03T22:27:45.778409: step 5425, loss 0.077346, acc 0.96875\n",
      "2017-04-03T22:27:45.967926: step 5426, loss 0.00740946, acc 1\n",
      "2017-04-03T22:27:46.157990: step 5427, loss 0.0227229, acc 1\n",
      "2017-04-03T22:27:46.348293: step 5428, loss 0.0953267, acc 0.96875\n",
      "2017-04-03T22:27:46.544305: step 5429, loss 0.0224699, acc 0.984375\n",
      "2017-04-03T22:27:46.738731: step 5430, loss 0.0199934, acc 1\n",
      "2017-04-03T22:27:46.934369: step 5431, loss 0.0504882, acc 0.984375\n",
      "2017-04-03T22:27:47.126496: step 5432, loss 0.0557106, acc 0.984375\n",
      "2017-04-03T22:27:47.317635: step 5433, loss 0.0598033, acc 1\n",
      "2017-04-03T22:27:47.508893: step 5434, loss 0.17796, acc 0.96875\n",
      "2017-04-03T22:27:47.705440: step 5435, loss 0.0233448, acc 1\n",
      "2017-04-03T22:27:47.902049: step 5436, loss 0.0994895, acc 0.953125\n",
      "2017-04-03T22:27:48.096695: step 5437, loss 0.0467164, acc 0.984375\n",
      "2017-04-03T22:27:48.286086: step 5438, loss 0.0208702, acc 1\n",
      "2017-04-03T22:27:48.477934: step 5439, loss 0.0317046, acc 1\n",
      "2017-04-03T22:27:48.677602: step 5440, loss 0.0383561, acc 0.984375\n",
      "2017-04-03T22:27:48.868565: step 5441, loss 0.0462558, acc 0.984375\n",
      "2017-04-03T22:27:49.058359: step 5442, loss 0.0196851, acc 1\n",
      "2017-04-03T22:27:49.250412: step 5443, loss 0.076546, acc 0.96875\n",
      "2017-04-03T22:27:49.443707: step 5444, loss 0.0446355, acc 0.984375\n",
      "2017-04-03T22:27:49.635742: step 5445, loss 0.0299774, acc 0.984375\n",
      "2017-04-03T22:27:49.824701: step 5446, loss 0.0915634, acc 0.984375\n",
      "2017-04-03T22:27:50.024688: step 5447, loss 0.0547165, acc 0.96875\n",
      "2017-04-03T22:27:50.231660: step 5448, loss 0.0450931, acc 0.984375\n",
      "2017-04-03T22:27:50.423978: step 5449, loss 0.0128751, acc 1\n",
      "2017-04-03T22:27:50.621951: step 5450, loss 0.055301, acc 0.984375\n",
      "2017-04-03T22:27:50.811642: step 5451, loss 0.0295706, acc 0.984375\n",
      "2017-04-03T22:27:51.007763: step 5452, loss 0.0125954, acc 1\n",
      "2017-04-03T22:27:51.200559: step 5453, loss 0.0559824, acc 0.96875\n",
      "2017-04-03T22:27:51.396535: step 5454, loss 0.104485, acc 0.953125\n",
      "2017-04-03T22:27:51.588379: step 5455, loss 0.122981, acc 0.9375\n",
      "2017-04-03T22:27:51.779143: step 5456, loss 0.0134768, acc 1\n",
      "2017-04-03T22:27:51.972447: step 5457, loss 0.0775893, acc 0.96875\n",
      "2017-04-03T22:27:52.165753: step 5458, loss 0.11034, acc 0.984375\n",
      "2017-04-03T22:27:52.356690: step 5459, loss 0.0394993, acc 0.984375\n",
      "2017-04-03T22:27:52.550908: step 5460, loss 0.038999, acc 1\n",
      "2017-04-03T22:27:52.747750: step 5461, loss 0.0396214, acc 0.984375\n",
      "2017-04-03T22:27:52.938597: step 5462, loss 0.0644862, acc 0.96875\n",
      "2017-04-03T22:27:53.129383: step 5463, loss 0.0348804, acc 1\n",
      "2017-04-03T22:27:53.323360: step 5464, loss 0.0587493, acc 0.984375\n",
      "2017-04-03T22:27:53.516739: step 5465, loss 0.0255446, acc 0.984375\n",
      "2017-04-03T22:27:53.706782: step 5466, loss 0.0873543, acc 0.984375\n",
      "2017-04-03T22:27:53.901391: step 5467, loss 0.0385458, acc 0.984375\n",
      "2017-04-03T22:27:54.095017: step 5468, loss 0.013717, acc 1\n",
      "2017-04-03T22:27:54.284165: step 5469, loss 0.0114337, acc 1\n",
      "2017-04-03T22:27:54.479741: step 5470, loss 0.0124508, acc 1\n",
      "2017-04-03T22:27:54.670739: step 5471, loss 0.0199251, acc 1\n",
      "2017-04-03T22:27:54.865927: step 5472, loss 0.0105778, acc 1\n",
      "2017-04-03T22:27:55.056593: step 5473, loss 0.00405606, acc 1\n",
      "2017-04-03T22:27:55.250112: step 5474, loss 0.0460718, acc 0.984375\n",
      "2017-04-03T22:27:55.442226: step 5475, loss 0.0106023, acc 1\n",
      "2017-04-03T22:27:55.633786: step 5476, loss 0.0434152, acc 0.96875\n",
      "2017-04-03T22:27:55.828194: step 5477, loss 0.0586032, acc 0.984375\n",
      "2017-04-03T22:27:56.023815: step 5478, loss 0.123215, acc 0.96875\n",
      "2017-04-03T22:27:56.221224: step 5479, loss 0.032215, acc 0.984375\n",
      "2017-04-03T22:27:56.414232: step 5480, loss 0.0651192, acc 0.96875\n",
      "2017-04-03T22:27:56.603575: step 5481, loss 0.0792529, acc 0.96875\n",
      "2017-04-03T22:27:56.803343: step 5482, loss 0.0131874, acc 1\n",
      "2017-04-03T22:27:56.997550: step 5483, loss 0.0134152, acc 1\n",
      "2017-04-03T22:27:57.187554: step 5484, loss 0.112764, acc 0.953125\n",
      "2017-04-03T22:27:57.383220: step 5485, loss 0.100309, acc 0.96875\n",
      "2017-04-03T22:27:57.573724: step 5486, loss 0.0775052, acc 0.953125\n",
      "2017-04-03T22:27:57.764278: step 5487, loss 0.059861, acc 0.984375\n",
      "2017-04-03T22:27:57.958673: step 5488, loss 0.0767307, acc 0.96875\n",
      "2017-04-03T22:27:58.153649: step 5489, loss 0.011033, acc 1\n",
      "2017-04-03T22:27:58.351745: step 5490, loss 0.0435297, acc 0.984375\n",
      "2017-04-03T22:27:58.548437: step 5491, loss 0.1183, acc 0.96875\n",
      "2017-04-03T22:27:58.749827: step 5492, loss 0.0528159, acc 1\n",
      "2017-04-03T22:27:58.941285: step 5493, loss 0.0642942, acc 0.96875\n",
      "2017-04-03T22:27:59.131625: step 5494, loss 0.066942, acc 0.984375\n",
      "2017-04-03T22:27:59.323411: step 5495, loss 0.00692775, acc 1\n",
      "2017-04-03T22:27:59.516287: step 5496, loss 0.0680454, acc 0.96875\n",
      "2017-04-03T22:27:59.706997: step 5497, loss 0.0459232, acc 0.984375\n",
      "2017-04-03T22:27:59.902116: step 5498, loss 0.0636791, acc 0.96875\n",
      "2017-04-03T22:28:00.097319: step 5499, loss 0.0814159, acc 0.96875\n",
      "2017-04-03T22:28:00.288849: step 5500, loss 0.0323644, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:28:00.936284: step 5500, loss 2.14911, acc 0.567885\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5500\n",
      "\n",
      "2017-04-03T22:28:01.189000: step 5501, loss 0.0860978, acc 0.96875\n",
      "2017-04-03T22:28:01.384285: step 5502, loss 0.108482, acc 0.984375\n",
      "2017-04-03T22:28:01.575412: step 5503, loss 0.060577, acc 0.96875\n",
      "2017-04-03T22:28:01.768219: step 5504, loss 0.0643853, acc 0.96875\n",
      "2017-04-03T22:28:01.966834: step 5505, loss 0.0442766, acc 0.984375\n",
      "2017-04-03T22:28:02.179738: step 5506, loss 0.0558268, acc 0.96875\n",
      "2017-04-03T22:28:02.368818: step 5507, loss 0.0732678, acc 0.96875\n",
      "2017-04-03T22:28:02.535805: step 5508, loss 0.0242451, acc 1\n",
      "2017-04-03T22:28:02.729870: step 5509, loss 0.0288012, acc 0.984375\n",
      "2017-04-03T22:28:02.919973: step 5510, loss 0.0210272, acc 1\n",
      "2017-04-03T22:28:03.113912: step 5511, loss 0.0702338, acc 0.96875\n",
      "2017-04-03T22:28:03.310317: step 5512, loss 0.0215417, acc 1\n",
      "2017-04-03T22:28:03.507819: step 5513, loss 0.0131152, acc 1\n",
      "2017-04-03T22:28:03.703252: step 5514, loss 0.0772603, acc 0.984375\n",
      "2017-04-03T22:28:03.900713: step 5515, loss 0.0372645, acc 0.984375\n",
      "2017-04-03T22:28:04.090701: step 5516, loss 0.0906698, acc 0.96875\n",
      "2017-04-03T22:28:04.287800: step 5517, loss 0.104371, acc 0.96875\n",
      "2017-04-03T22:28:04.480628: step 5518, loss 0.0345049, acc 0.984375\n",
      "2017-04-03T22:28:04.676066: step 5519, loss 0.0248175, acc 1\n",
      "2017-04-03T22:28:04.871666: step 5520, loss 0.0520283, acc 0.984375\n",
      "2017-04-03T22:28:05.066505: step 5521, loss 0.0891697, acc 0.984375\n",
      "2017-04-03T22:28:05.264132: step 5522, loss 0.0296441, acc 0.984375\n",
      "2017-04-03T22:28:05.458007: step 5523, loss 0.0650498, acc 0.984375\n",
      "2017-04-03T22:28:05.645132: step 5524, loss 0.105761, acc 0.953125\n",
      "2017-04-03T22:28:05.835816: step 5525, loss 0.0398392, acc 0.984375\n",
      "2017-04-03T22:28:06.031209: step 5526, loss 0.0501532, acc 0.984375\n",
      "2017-04-03T22:28:06.228748: step 5527, loss 0.018761, acc 1\n",
      "2017-04-03T22:28:06.421278: step 5528, loss 0.0699488, acc 0.96875\n",
      "2017-04-03T22:28:06.614437: step 5529, loss 0.0813697, acc 0.984375\n",
      "2017-04-03T22:28:06.805292: step 5530, loss 0.0256048, acc 1\n",
      "2017-04-03T22:28:06.998480: step 5531, loss 0.039815, acc 0.984375\n",
      "2017-04-03T22:28:07.193707: step 5532, loss 0.0510032, acc 0.984375\n",
      "2017-04-03T22:28:07.387084: step 5533, loss 0.0652591, acc 0.984375\n",
      "2017-04-03T22:28:07.583044: step 5534, loss 0.0678609, acc 0.96875\n",
      "2017-04-03T22:28:07.771863: step 5535, loss 0.0650841, acc 0.96875\n",
      "2017-04-03T22:28:07.966076: step 5536, loss 0.0357339, acc 1\n",
      "2017-04-03T22:28:08.155574: step 5537, loss 0.0111136, acc 1\n",
      "2017-04-03T22:28:08.347159: step 5538, loss 0.0894054, acc 0.984375\n",
      "2017-04-03T22:28:08.539363: step 5539, loss 0.0664959, acc 0.984375\n",
      "2017-04-03T22:28:08.734396: step 5540, loss 0.0583328, acc 1\n",
      "2017-04-03T22:28:08.929787: step 5541, loss 0.041205, acc 0.984375\n",
      "2017-04-03T22:28:09.124660: step 5542, loss 0.0133001, acc 1\n",
      "2017-04-03T22:28:09.319302: step 5543, loss 0.0562445, acc 0.984375\n",
      "2017-04-03T22:28:09.514938: step 5544, loss 0.107862, acc 0.953125\n",
      "2017-04-03T22:28:09.706125: step 5545, loss 0.0304478, acc 1\n",
      "2017-04-03T22:28:09.899571: step 5546, loss 0.0119915, acc 1\n",
      "2017-04-03T22:28:10.087752: step 5547, loss 0.0677402, acc 0.96875\n",
      "2017-04-03T22:28:10.283808: step 5548, loss 0.0150258, acc 1\n",
      "2017-04-03T22:28:10.475154: step 5549, loss 0.031596, acc 0.984375\n",
      "2017-04-03T22:28:10.665678: step 5550, loss 0.0396875, acc 0.984375\n",
      "2017-04-03T22:28:10.858922: step 5551, loss 0.0671327, acc 0.96875\n",
      "2017-04-03T22:28:11.055515: step 5552, loss 0.0269225, acc 0.984375\n",
      "2017-04-03T22:28:11.248855: step 5553, loss 0.046449, acc 0.984375\n",
      "2017-04-03T22:28:11.444116: step 5554, loss 0.0508617, acc 0.984375\n",
      "2017-04-03T22:28:11.636495: step 5555, loss 0.0713614, acc 0.953125\n",
      "2017-04-03T22:28:11.829365: step 5556, loss 0.0186524, acc 1\n",
      "2017-04-03T22:28:12.021098: step 5557, loss 0.0916588, acc 0.984375\n",
      "2017-04-03T22:28:12.213130: step 5558, loss 0.0392592, acc 0.984375\n",
      "2017-04-03T22:28:12.402462: step 5559, loss 0.0274118, acc 0.984375\n",
      "2017-04-03T22:28:12.597533: step 5560, loss 0.0605356, acc 0.984375\n",
      "2017-04-03T22:28:12.786965: step 5561, loss 0.07725, acc 0.96875\n",
      "2017-04-03T22:28:12.984262: step 5562, loss 0.033045, acc 0.96875\n",
      "2017-04-03T22:28:13.175590: step 5563, loss 0.0878528, acc 0.96875\n",
      "2017-04-03T22:28:13.370960: step 5564, loss 0.116148, acc 0.9375\n",
      "2017-04-03T22:28:13.566423: step 5565, loss 0.0414028, acc 0.984375\n",
      "2017-04-03T22:28:13.757820: step 5566, loss 0.0914259, acc 0.96875\n",
      "2017-04-03T22:28:13.948582: step 5567, loss 0.023111, acc 1\n",
      "2017-04-03T22:28:14.144649: step 5568, loss 0.0422743, acc 0.984375\n",
      "2017-04-03T22:28:14.339601: step 5569, loss 0.0674974, acc 0.96875\n",
      "2017-04-03T22:28:14.536697: step 5570, loss 0.057852, acc 0.96875\n",
      "2017-04-03T22:28:14.732157: step 5571, loss 0.0461969, acc 1\n",
      "2017-04-03T22:28:14.924039: step 5572, loss 0.0266454, acc 0.984375\n",
      "2017-04-03T22:28:15.115194: step 5573, loss 0.0259029, acc 1\n",
      "2017-04-03T22:28:15.307297: step 5574, loss 0.0178163, acc 1\n",
      "2017-04-03T22:28:15.502303: step 5575, loss 0.043841, acc 0.984375\n",
      "2017-04-03T22:28:15.700893: step 5576, loss 0.0187566, acc 1\n",
      "2017-04-03T22:28:15.892127: step 5577, loss 0.00758769, acc 1\n",
      "2017-04-03T22:28:16.086296: step 5578, loss 0.0235205, acc 1\n",
      "2017-04-03T22:28:16.282238: step 5579, loss 0.0095693, acc 1\n",
      "2017-04-03T22:28:16.476727: step 5580, loss 0.0288571, acc 0.984375\n",
      "2017-04-03T22:28:16.670562: step 5581, loss 0.0825629, acc 0.984375\n",
      "2017-04-03T22:28:16.859447: step 5582, loss 0.0301734, acc 0.984375\n",
      "2017-04-03T22:28:17.047358: step 5583, loss 0.0700588, acc 0.984375\n",
      "2017-04-03T22:28:17.238500: step 5584, loss 0.0401552, acc 0.984375\n",
      "2017-04-03T22:28:17.434023: step 5585, loss 0.0122589, acc 1\n",
      "2017-04-03T22:28:17.624845: step 5586, loss 0.0569956, acc 0.96875\n",
      "2017-04-03T22:28:17.817105: step 5587, loss 0.0498616, acc 0.984375\n",
      "2017-04-03T22:28:18.021095: step 5588, loss 0.0454006, acc 0.984375\n",
      "2017-04-03T22:28:18.230617: step 5589, loss 0.104812, acc 0.953125\n",
      "2017-04-03T22:28:18.428358: step 5590, loss 0.0118984, acc 1\n",
      "2017-04-03T22:28:18.620621: step 5591, loss 0.0518423, acc 0.984375\n",
      "2017-04-03T22:28:18.811873: step 5592, loss 0.0516321, acc 0.984375\n",
      "2017-04-03T22:28:19.001512: step 5593, loss 0.0700034, acc 0.984375\n",
      "2017-04-03T22:28:19.195109: step 5594, loss 0.0963638, acc 0.96875\n",
      "2017-04-03T22:28:19.391961: step 5595, loss 0.0156263, acc 1\n",
      "2017-04-03T22:28:19.579957: step 5596, loss 0.0507717, acc 0.984375\n",
      "2017-04-03T22:28:19.773715: step 5597, loss 0.0592134, acc 1\n",
      "2017-04-03T22:28:19.963301: step 5598, loss 0.0210848, acc 0.984375\n",
      "2017-04-03T22:28:20.156819: step 5599, loss 0.0556647, acc 0.984375\n",
      "2017-04-03T22:28:20.347486: step 5600, loss 0.055008, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:28:20.999667: step 5600, loss 2.12655, acc 0.571802\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5600\n",
      "\n",
      "2017-04-03T22:28:21.256318: step 5601, loss 0.0906775, acc 0.953125\n",
      "2017-04-03T22:28:21.449778: step 5602, loss 0.0543986, acc 0.984375\n",
      "2017-04-03T22:28:21.643258: step 5603, loss 0.0977804, acc 0.96875\n",
      "2017-04-03T22:28:21.839531: step 5604, loss 0.103971, acc 0.953125\n",
      "2017-04-03T22:28:22.031411: step 5605, loss 0.0313231, acc 0.984375\n",
      "2017-04-03T22:28:22.230071: step 5606, loss 0.0610883, acc 0.984375\n",
      "2017-04-03T22:28:22.422187: step 5607, loss 0.0586141, acc 0.96875\n",
      "2017-04-03T22:28:22.618439: step 5608, loss 0.00792331, acc 1\n",
      "2017-04-03T22:28:22.807544: step 5609, loss 0.0534865, acc 0.984375\n",
      "2017-04-03T22:28:22.996276: step 5610, loss 0.0855121, acc 0.984375\n",
      "2017-04-03T22:28:23.191371: step 5611, loss 0.053383, acc 0.984375\n",
      "2017-04-03T22:28:23.382856: step 5612, loss 0.0863201, acc 0.96875\n",
      "2017-04-03T22:28:23.572940: step 5613, loss 0.0235528, acc 1\n",
      "2017-04-03T22:28:23.763105: step 5614, loss 0.0448571, acc 0.984375\n",
      "2017-04-03T22:28:23.954648: step 5615, loss 0.0336315, acc 0.984375\n",
      "2017-04-03T22:28:24.115921: step 5616, loss 0.0110645, acc 1\n",
      "2017-04-03T22:28:24.306723: step 5617, loss 0.0384092, acc 0.984375\n",
      "2017-04-03T22:28:24.497785: step 5618, loss 0.0180674, acc 1\n",
      "2017-04-03T22:28:24.689170: step 5619, loss 0.0364576, acc 1\n",
      "2017-04-03T22:28:24.879957: step 5620, loss 0.0490487, acc 0.984375\n",
      "2017-04-03T22:28:25.072381: step 5621, loss 0.0615874, acc 0.984375\n",
      "2017-04-03T22:28:25.267790: step 5622, loss 0.123566, acc 0.953125\n",
      "2017-04-03T22:28:25.461371: step 5623, loss 0.0934153, acc 0.96875\n",
      "2017-04-03T22:28:25.653374: step 5624, loss 0.110221, acc 0.953125\n",
      "2017-04-03T22:28:25.847062: step 5625, loss 0.0492531, acc 0.984375\n",
      "2017-04-03T22:28:26.038873: step 5626, loss 0.0231135, acc 0.984375\n",
      "2017-04-03T22:28:26.232713: step 5627, loss 0.0811904, acc 0.984375\n",
      "2017-04-03T22:28:26.428510: step 5628, loss 0.0899334, acc 0.96875\n",
      "2017-04-03T22:28:26.617476: step 5629, loss 0.0905119, acc 0.953125\n",
      "2017-04-03T22:28:26.807096: step 5630, loss 0.0389632, acc 0.984375\n",
      "2017-04-03T22:28:27.001620: step 5631, loss 0.0651433, acc 0.96875\n",
      "2017-04-03T22:28:27.191916: step 5632, loss 0.135616, acc 0.953125\n",
      "2017-04-03T22:28:27.383834: step 5633, loss 0.0803972, acc 0.96875\n",
      "2017-04-03T22:28:27.572632: step 5634, loss 0.0416698, acc 0.984375\n",
      "2017-04-03T22:28:27.766668: step 5635, loss 0.0458836, acc 0.984375\n",
      "2017-04-03T22:28:27.963232: step 5636, loss 0.0397553, acc 0.984375\n",
      "2017-04-03T22:28:28.158207: step 5637, loss 0.0960761, acc 0.953125\n",
      "2017-04-03T22:28:28.352309: step 5638, loss 0.026738, acc 1\n",
      "2017-04-03T22:28:28.543672: step 5639, loss 0.0326124, acc 0.984375\n",
      "2017-04-03T22:28:28.740419: step 5640, loss 0.0468802, acc 0.984375\n",
      "2017-04-03T22:28:28.934378: step 5641, loss 0.0135966, acc 1\n",
      "2017-04-03T22:28:29.123897: step 5642, loss 0.065303, acc 0.96875\n",
      "2017-04-03T22:28:29.319339: step 5643, loss 0.0653114, acc 0.984375\n",
      "2017-04-03T22:28:29.511911: step 5644, loss 0.0409758, acc 0.984375\n",
      "2017-04-03T22:28:29.702469: step 5645, loss 0.0104643, acc 1\n",
      "2017-04-03T22:28:29.893320: step 5646, loss 0.0481753, acc 0.984375\n",
      "2017-04-03T22:28:30.085287: step 5647, loss 0.0777891, acc 0.96875\n",
      "2017-04-03T22:28:30.275001: step 5648, loss 0.062244, acc 0.984375\n",
      "2017-04-03T22:28:30.470145: step 5649, loss 0.0168485, acc 1\n",
      "2017-04-03T22:28:30.661394: step 5650, loss 0.0526207, acc 0.984375\n",
      "2017-04-03T22:28:30.851656: step 5651, loss 0.0513612, acc 0.984375\n",
      "2017-04-03T22:28:31.041403: step 5652, loss 0.0113697, acc 1\n",
      "2017-04-03T22:28:31.233958: step 5653, loss 0.0893918, acc 0.96875\n",
      "2017-04-03T22:28:31.426907: step 5654, loss 0.0104927, acc 1\n",
      "2017-04-03T22:28:31.628759: step 5655, loss 0.0268644, acc 0.984375\n",
      "2017-04-03T22:28:31.827943: step 5656, loss 0.0311799, acc 0.984375\n",
      "2017-04-03T22:28:32.024652: step 5657, loss 0.0850694, acc 0.953125\n",
      "2017-04-03T22:28:32.228434: step 5658, loss 0.0520049, acc 0.96875\n",
      "2017-04-03T22:28:32.427412: step 5659, loss 0.0939678, acc 0.953125\n",
      "2017-04-03T22:28:32.625641: step 5660, loss 0.0465605, acc 0.984375\n",
      "2017-04-03T22:28:32.837594: step 5661, loss 0.024665, acc 1\n",
      "2017-04-03T22:28:33.027654: step 5662, loss 0.0162668, acc 1\n",
      "2017-04-03T22:28:33.221009: step 5663, loss 0.0617679, acc 0.984375\n",
      "2017-04-03T22:28:33.414017: step 5664, loss 0.0349174, acc 0.984375\n",
      "2017-04-03T22:28:33.610829: step 5665, loss 0.0341549, acc 1\n",
      "2017-04-03T22:28:33.802137: step 5666, loss 0.0658706, acc 0.984375\n",
      "2017-04-03T22:28:33.995465: step 5667, loss 0.033172, acc 0.984375\n",
      "2017-04-03T22:28:34.184875: step 5668, loss 0.0421656, acc 0.984375\n",
      "2017-04-03T22:28:34.380452: step 5669, loss 0.0722759, acc 0.984375\n",
      "2017-04-03T22:28:34.571207: step 5670, loss 0.0863973, acc 0.984375\n",
      "2017-04-03T22:28:34.764851: step 5671, loss 0.0333833, acc 1\n",
      "2017-04-03T22:28:34.953879: step 5672, loss 0.0157741, acc 1\n",
      "2017-04-03T22:28:35.152544: step 5673, loss 0.19947, acc 0.9375\n",
      "2017-04-03T22:28:35.345686: step 5674, loss 0.0454708, acc 0.984375\n",
      "2017-04-03T22:28:35.536861: step 5675, loss 0.0610352, acc 0.96875\n",
      "2017-04-03T22:28:35.729502: step 5676, loss 0.0341569, acc 0.984375\n",
      "2017-04-03T22:28:35.922460: step 5677, loss 0.0586404, acc 0.96875\n",
      "2017-04-03T22:28:36.114429: step 5678, loss 0.0932844, acc 0.953125\n",
      "2017-04-03T22:28:36.308056: step 5679, loss 0.0964493, acc 0.953125\n",
      "2017-04-03T22:28:36.498992: step 5680, loss 0.0475917, acc 1\n",
      "2017-04-03T22:28:36.693679: step 5681, loss 0.0422868, acc 1\n",
      "2017-04-03T22:28:36.888496: step 5682, loss 0.0743054, acc 0.96875\n",
      "2017-04-03T22:28:37.081511: step 5683, loss 0.0372936, acc 1\n",
      "2017-04-03T22:28:37.274740: step 5684, loss 0.00601927, acc 1\n",
      "2017-04-03T22:28:37.468595: step 5685, loss 0.0123289, acc 1\n",
      "2017-04-03T22:28:37.658113: step 5686, loss 0.176897, acc 0.921875\n",
      "2017-04-03T22:28:37.851155: step 5687, loss 0.0300293, acc 1\n",
      "2017-04-03T22:28:38.043533: step 5688, loss 0.0390092, acc 0.984375\n",
      "2017-04-03T22:28:38.237482: step 5689, loss 0.0489173, acc 0.984375\n",
      "2017-04-03T22:28:38.429523: step 5690, loss 0.00729525, acc 1\n",
      "2017-04-03T22:28:38.622212: step 5691, loss 0.0430625, acc 0.984375\n",
      "2017-04-03T22:28:38.816559: step 5692, loss 0.0667312, acc 1\n",
      "2017-04-03T22:28:39.009829: step 5693, loss 0.0210173, acc 1\n",
      "2017-04-03T22:28:39.211833: step 5694, loss 0.0312418, acc 0.984375\n",
      "2017-04-03T22:28:39.412318: step 5695, loss 0.0433675, acc 0.984375\n",
      "2017-04-03T22:28:39.612293: step 5696, loss 0.0351865, acc 1\n",
      "2017-04-03T22:28:39.808688: step 5697, loss 0.0288952, acc 1\n",
      "2017-04-03T22:28:40.010687: step 5698, loss 0.0313245, acc 1\n",
      "2017-04-03T22:28:40.210843: step 5699, loss 0.0342461, acc 0.984375\n",
      "2017-04-03T22:28:40.405321: step 5700, loss 0.0273931, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:28:41.057902: step 5700, loss 2.16837, acc 0.571802\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5700\n",
      "\n",
      "2017-04-03T22:28:41.306729: step 5701, loss 0.0292979, acc 0.984375\n",
      "2017-04-03T22:28:41.497296: step 5702, loss 0.0440553, acc 0.984375\n",
      "2017-04-03T22:28:41.691761: step 5703, loss 0.120563, acc 0.984375\n",
      "2017-04-03T22:28:41.882552: step 5704, loss 0.141456, acc 0.921875\n",
      "2017-04-03T22:28:42.081836: step 5705, loss 0.0643518, acc 0.984375\n",
      "2017-04-03T22:28:42.275166: step 5706, loss 0.0112104, acc 1\n",
      "2017-04-03T22:28:42.469621: step 5707, loss 0.0663664, acc 0.984375\n",
      "2017-04-03T22:28:42.660901: step 5708, loss 0.0158438, acc 1\n",
      "2017-04-03T22:28:42.855583: step 5709, loss 0.0619285, acc 0.984375\n",
      "2017-04-03T22:28:43.050626: step 5710, loss 0.0428248, acc 0.984375\n",
      "2017-04-03T22:28:43.242871: step 5711, loss 0.0258356, acc 1\n",
      "2017-04-03T22:28:43.436579: step 5712, loss 0.009797, acc 1\n",
      "2017-04-03T22:28:43.630627: step 5713, loss 0.0494207, acc 0.984375\n",
      "2017-04-03T22:28:43.825025: step 5714, loss 0.0369296, acc 1\n",
      "2017-04-03T22:28:44.022022: step 5715, loss 0.00398299, acc 1\n",
      "2017-04-03T22:28:44.235506: step 5716, loss 0.00879631, acc 1\n",
      "2017-04-03T22:28:44.454899: step 5717, loss 0.0149154, acc 1\n",
      "2017-04-03T22:28:44.659768: step 5718, loss 0.0517977, acc 0.984375\n",
      "2017-04-03T22:28:44.865500: step 5719, loss 0.0698913, acc 0.96875\n",
      "2017-04-03T22:28:45.065766: step 5720, loss 0.0451974, acc 0.984375\n",
      "2017-04-03T22:28:45.270108: step 5721, loss 0.0586216, acc 0.984375\n",
      "2017-04-03T22:28:45.458212: step 5722, loss 0.00184574, acc 1\n",
      "2017-04-03T22:28:45.651134: step 5723, loss 0.0328747, acc 0.984375\n",
      "2017-04-03T22:28:45.811132: step 5724, loss 0.0670719, acc 0.980769\n",
      "2017-04-03T22:28:46.005612: step 5725, loss 0.0785865, acc 0.96875\n",
      "2017-04-03T22:28:46.211700: step 5726, loss 0.0504398, acc 0.984375\n",
      "2017-04-03T22:28:46.409467: step 5727, loss 0.00704454, acc 1\n",
      "2017-04-03T22:28:46.611976: step 5728, loss 0.0797079, acc 0.984375\n",
      "2017-04-03T22:28:46.808811: step 5729, loss 0.0508694, acc 0.96875\n",
      "2017-04-03T22:28:47.003087: step 5730, loss 0.0279117, acc 0.984375\n",
      "2017-04-03T22:28:47.195137: step 5731, loss 0.0133606, acc 1\n",
      "2017-04-03T22:28:47.385131: step 5732, loss 0.0257217, acc 1\n",
      "2017-04-03T22:28:47.578019: step 5733, loss 0.011771, acc 1\n",
      "2017-04-03T22:28:47.774040: step 5734, loss 0.0130961, acc 1\n",
      "2017-04-03T22:28:47.964714: step 5735, loss 0.0430781, acc 0.984375\n",
      "2017-04-03T22:28:48.158856: step 5736, loss 0.0126905, acc 1\n",
      "2017-04-03T22:28:48.350659: step 5737, loss 0.0207802, acc 0.984375\n",
      "2017-04-03T22:28:48.542654: step 5738, loss 0.0155087, acc 1\n",
      "2017-04-03T22:28:48.743877: step 5739, loss 0.0515566, acc 1\n",
      "2017-04-03T22:28:48.934318: step 5740, loss 0.0624973, acc 0.96875\n",
      "2017-04-03T22:28:49.124491: step 5741, loss 0.0448121, acc 0.984375\n",
      "2017-04-03T22:28:49.318324: step 5742, loss 0.0389847, acc 0.984375\n",
      "2017-04-03T22:28:49.511241: step 5743, loss 0.0814541, acc 0.96875\n",
      "2017-04-03T22:28:49.700862: step 5744, loss 0.0517009, acc 0.984375\n",
      "2017-04-03T22:28:49.901379: step 5745, loss 0.0289505, acc 0.984375\n",
      "2017-04-03T22:28:50.091044: step 5746, loss 0.117208, acc 0.96875\n",
      "2017-04-03T22:28:50.284389: step 5747, loss 0.0355155, acc 1\n",
      "2017-04-03T22:28:50.472949: step 5748, loss 0.092467, acc 0.96875\n",
      "2017-04-03T22:28:50.666906: step 5749, loss 0.0974831, acc 0.953125\n",
      "2017-04-03T22:28:50.862533: step 5750, loss 0.0130295, acc 1\n",
      "2017-04-03T22:28:51.051346: step 5751, loss 0.0481428, acc 0.96875\n",
      "2017-04-03T22:28:51.243509: step 5752, loss 0.0369391, acc 0.984375\n",
      "2017-04-03T22:28:51.441946: step 5753, loss 0.0355824, acc 1\n",
      "2017-04-03T22:28:51.635571: step 5754, loss 0.0165338, acc 1\n",
      "2017-04-03T22:28:51.831033: step 5755, loss 0.143561, acc 0.96875\n",
      "2017-04-03T22:28:52.030460: step 5756, loss 0.035176, acc 0.984375\n",
      "2017-04-03T22:28:52.244676: step 5757, loss 0.0293734, acc 0.984375\n",
      "2017-04-03T22:28:52.439544: step 5758, loss 0.044996, acc 0.984375\n",
      "2017-04-03T22:28:52.635011: step 5759, loss 0.00598725, acc 1\n",
      "2017-04-03T22:28:52.830841: step 5760, loss 0.0364351, acc 0.984375\n",
      "2017-04-03T22:28:53.023564: step 5761, loss 0.0321671, acc 0.984375\n",
      "2017-04-03T22:28:53.219396: step 5762, loss 0.0751661, acc 0.984375\n",
      "2017-04-03T22:28:53.413714: step 5763, loss 0.0300257, acc 1\n",
      "2017-04-03T22:28:53.605037: step 5764, loss 0.00852889, acc 1\n",
      "2017-04-03T22:28:53.800435: step 5765, loss 0.0314267, acc 0.984375\n",
      "2017-04-03T22:28:53.994432: step 5766, loss 0.0306911, acc 0.984375\n",
      "2017-04-03T22:28:54.188230: step 5767, loss 0.072311, acc 0.984375\n",
      "2017-04-03T22:28:54.380726: step 5768, loss 0.0917968, acc 0.96875\n",
      "2017-04-03T22:28:54.578950: step 5769, loss 0.0390085, acc 1\n",
      "2017-04-03T22:28:54.792828: step 5770, loss 0.0176284, acc 1\n",
      "2017-04-03T22:28:54.995968: step 5771, loss 0.0266419, acc 1\n",
      "2017-04-03T22:28:55.192322: step 5772, loss 0.00809828, acc 1\n",
      "2017-04-03T22:28:55.390743: step 5773, loss 0.0578846, acc 0.984375\n",
      "2017-04-03T22:28:55.584916: step 5774, loss 0.019898, acc 1\n",
      "2017-04-03T22:28:55.782805: step 5775, loss 0.0691706, acc 0.984375\n",
      "2017-04-03T22:28:55.976704: step 5776, loss 0.0545568, acc 0.984375\n",
      "2017-04-03T22:28:56.168089: step 5777, loss 0.0412132, acc 0.984375\n",
      "2017-04-03T22:28:56.357525: step 5778, loss 0.0131687, acc 1\n",
      "2017-04-03T22:28:56.552122: step 5779, loss 0.0774021, acc 0.96875\n",
      "2017-04-03T22:28:56.741809: step 5780, loss 0.0649712, acc 0.96875\n",
      "2017-04-03T22:28:56.935543: step 5781, loss 0.00758639, acc 1\n",
      "2017-04-03T22:28:57.133901: step 5782, loss 0.0225295, acc 1\n",
      "2017-04-03T22:28:57.334480: step 5783, loss 0.0412215, acc 0.984375\n",
      "2017-04-03T22:28:57.528999: step 5784, loss 0.00298198, acc 1\n",
      "2017-04-03T22:28:57.736789: step 5785, loss 0.0204558, acc 1\n",
      "2017-04-03T22:28:57.929893: step 5786, loss 0.123098, acc 0.953125\n",
      "2017-04-03T22:28:58.136946: step 5787, loss 0.0658138, acc 0.984375\n",
      "2017-04-03T22:28:58.341656: step 5788, loss 0.0718675, acc 0.984375\n",
      "2017-04-03T22:28:58.556805: step 5789, loss 0.0211756, acc 1\n",
      "2017-04-03T22:28:58.765174: step 5790, loss 0.0959228, acc 0.96875\n",
      "2017-04-03T22:28:58.976592: step 5791, loss 0.00839999, acc 1\n",
      "2017-04-03T22:28:59.193021: step 5792, loss 0.0331727, acc 0.984375\n",
      "2017-04-03T22:28:59.402187: step 5793, loss 0.0890858, acc 0.96875\n",
      "2017-04-03T22:28:59.616029: step 5794, loss 0.116584, acc 0.953125\n",
      "2017-04-03T22:28:59.807021: step 5795, loss 0.0518114, acc 0.984375\n",
      "2017-04-03T22:28:59.995468: step 5796, loss 0.0944206, acc 0.96875\n",
      "2017-04-03T22:29:00.190490: step 5797, loss 0.0629304, acc 0.984375\n",
      "2017-04-03T22:29:00.389847: step 5798, loss 0.0355436, acc 0.984375\n",
      "2017-04-03T22:29:00.589215: step 5799, loss 0.011862, acc 1\n",
      "2017-04-03T22:29:00.798149: step 5800, loss 0.0691874, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:29:01.504232: step 5800, loss 2.20144, acc 0.575718\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5800\n",
      "\n",
      "2017-04-03T22:29:01.767393: step 5801, loss 0.0106787, acc 1\n",
      "2017-04-03T22:29:01.970306: step 5802, loss 0.0678884, acc 0.96875\n",
      "2017-04-03T22:29:02.178051: step 5803, loss 0.00944001, acc 1\n",
      "2017-04-03T22:29:02.370836: step 5804, loss 0.13374, acc 0.953125\n",
      "2017-04-03T22:29:02.566089: step 5805, loss 0.0279625, acc 1\n",
      "2017-04-03T22:29:02.773835: step 5806, loss 0.0201255, acc 1\n",
      "2017-04-03T22:29:02.976480: step 5807, loss 0.0749388, acc 0.984375\n",
      "2017-04-03T22:29:03.166653: step 5808, loss 0.042111, acc 0.984375\n",
      "2017-04-03T22:29:03.360418: step 5809, loss 0.0746755, acc 0.96875\n",
      "2017-04-03T22:29:03.559937: step 5810, loss 0.043634, acc 0.984375\n",
      "2017-04-03T22:29:03.752593: step 5811, loss 0.0598851, acc 0.984375\n",
      "2017-04-03T22:29:03.946163: step 5812, loss 0.0499656, acc 1\n",
      "2017-04-03T22:29:04.139011: step 5813, loss 0.151365, acc 0.96875\n",
      "2017-04-03T22:29:04.342407: step 5814, loss 0.060979, acc 0.984375\n",
      "2017-04-03T22:29:04.536945: step 5815, loss 0.016129, acc 1\n",
      "2017-04-03T22:29:04.729441: step 5816, loss 0.105148, acc 0.96875\n",
      "2017-04-03T22:29:04.917982: step 5817, loss 0.108657, acc 0.96875\n",
      "2017-04-03T22:29:05.115535: step 5818, loss 0.0171989, acc 1\n",
      "2017-04-03T22:29:05.312669: step 5819, loss 0.0630747, acc 0.984375\n",
      "2017-04-03T22:29:05.502425: step 5820, loss 0.0419468, acc 0.984375\n",
      "2017-04-03T22:29:05.694960: step 5821, loss 0.134096, acc 0.984375\n",
      "2017-04-03T22:29:05.892149: step 5822, loss 0.0888201, acc 0.953125\n",
      "2017-04-03T22:29:06.085503: step 5823, loss 0.0484294, acc 1\n",
      "2017-04-03T22:29:06.279463: step 5824, loss 0.0710281, acc 0.96875\n",
      "2017-04-03T22:29:06.471121: step 5825, loss 0.00684523, acc 1\n",
      "2017-04-03T22:29:06.668069: step 5826, loss 0.107756, acc 0.96875\n",
      "2017-04-03T22:29:06.884334: step 5827, loss 0.056488, acc 0.984375\n",
      "2017-04-03T22:29:07.081067: step 5828, loss 0.00536701, acc 1\n",
      "2017-04-03T22:29:07.278905: step 5829, loss 0.00739786, acc 1\n",
      "2017-04-03T22:29:07.470274: step 5830, loss 0.0441075, acc 0.984375\n",
      "2017-04-03T22:29:07.662874: step 5831, loss 0.113784, acc 0.953125\n",
      "2017-04-03T22:29:07.822467: step 5832, loss 0.0524506, acc 0.980769\n",
      "2017-04-03T22:29:08.015156: step 5833, loss 0.102397, acc 0.96875\n",
      "2017-04-03T22:29:08.208930: step 5834, loss 0.0333174, acc 1\n",
      "2017-04-03T22:29:08.406345: step 5835, loss 0.00688264, acc 1\n",
      "2017-04-03T22:29:08.601666: step 5836, loss 0.0645271, acc 0.96875\n",
      "2017-04-03T22:29:08.810955: step 5837, loss 0.0479995, acc 0.984375\n",
      "2017-04-03T22:29:09.011964: step 5838, loss 0.0639883, acc 0.984375\n",
      "2017-04-03T22:29:09.207278: step 5839, loss 0.0150564, acc 1\n",
      "2017-04-03T22:29:09.415893: step 5840, loss 0.0117698, acc 1\n",
      "2017-04-03T22:29:09.618887: step 5841, loss 0.0681348, acc 0.984375\n",
      "2017-04-03T22:29:09.812386: step 5842, loss 0.0358988, acc 1\n",
      "2017-04-03T22:29:10.005687: step 5843, loss 0.09331, acc 0.96875\n",
      "2017-04-03T22:29:10.195259: step 5844, loss 0.0353343, acc 1\n",
      "2017-04-03T22:29:10.391128: step 5845, loss 0.0931852, acc 0.953125\n",
      "2017-04-03T22:29:10.584557: step 5846, loss 0.0105692, acc 1\n",
      "2017-04-03T22:29:10.775998: step 5847, loss 0.127407, acc 0.96875\n",
      "2017-04-03T22:29:10.971602: step 5848, loss 0.0447014, acc 0.984375\n",
      "2017-04-03T22:29:11.167054: step 5849, loss 0.0182896, acc 1\n",
      "2017-04-03T22:29:11.362931: step 5850, loss 0.0825104, acc 0.96875\n",
      "2017-04-03T22:29:11.556947: step 5851, loss 0.0127644, acc 1\n",
      "2017-04-03T22:29:11.755363: step 5852, loss 0.013504, acc 1\n",
      "2017-04-03T22:29:11.951440: step 5853, loss 0.0459917, acc 0.984375\n",
      "2017-04-03T22:29:12.150462: step 5854, loss 0.12442, acc 0.953125\n",
      "2017-04-03T22:29:12.351013: step 5855, loss 0.0620048, acc 0.984375\n",
      "2017-04-03T22:29:12.542321: step 5856, loss 0.0156318, acc 1\n",
      "2017-04-03T22:29:12.734595: step 5857, loss 0.0370571, acc 0.984375\n",
      "2017-04-03T22:29:12.925560: step 5858, loss 0.0120728, acc 1\n",
      "2017-04-03T22:29:13.120745: step 5859, loss 0.0301709, acc 1\n",
      "2017-04-03T22:29:13.311630: step 5860, loss 0.0203482, acc 0.984375\n",
      "2017-04-03T22:29:13.506897: step 5861, loss 0.0307972, acc 0.984375\n",
      "2017-04-03T22:29:13.698874: step 5862, loss 0.0656228, acc 0.96875\n",
      "2017-04-03T22:29:13.893044: step 5863, loss 0.0737598, acc 0.984375\n",
      "2017-04-03T22:29:14.089231: step 5864, loss 0.0224539, acc 1\n",
      "2017-04-03T22:29:14.280409: step 5865, loss 0.078561, acc 0.96875\n",
      "2017-04-03T22:29:14.471928: step 5866, loss 0.0707872, acc 0.984375\n",
      "2017-04-03T22:29:14.669401: step 5867, loss 0.0277392, acc 1\n",
      "2017-04-03T22:29:14.877445: step 5868, loss 0.0295427, acc 0.984375\n",
      "2017-04-03T22:29:15.071778: step 5869, loss 0.0196078, acc 1\n",
      "2017-04-03T22:29:15.264083: step 5870, loss 0.0590889, acc 0.96875\n",
      "2017-04-03T22:29:15.459084: step 5871, loss 0.0788779, acc 0.96875\n",
      "2017-04-03T22:29:15.655470: step 5872, loss 0.0512299, acc 0.96875\n",
      "2017-04-03T22:29:15.850693: step 5873, loss 0.0794313, acc 0.96875\n",
      "2017-04-03T22:29:16.045489: step 5874, loss 0.0520938, acc 0.984375\n",
      "2017-04-03T22:29:16.239360: step 5875, loss 0.0162443, acc 1\n",
      "2017-04-03T22:29:16.440605: step 5876, loss 0.0709289, acc 0.96875\n",
      "2017-04-03T22:29:16.638906: step 5877, loss 0.0659757, acc 0.96875\n",
      "2017-04-03T22:29:16.833455: step 5878, loss 0.0933113, acc 0.96875\n",
      "2017-04-03T22:29:17.033073: step 5879, loss 0.023573, acc 1\n",
      "2017-04-03T22:29:17.228197: step 5880, loss 0.0469887, acc 0.984375\n",
      "2017-04-03T22:29:17.425557: step 5881, loss 0.0531305, acc 1\n",
      "2017-04-03T22:29:17.619197: step 5882, loss 0.00985244, acc 1\n",
      "2017-04-03T22:29:17.826566: step 5883, loss 0.0474497, acc 0.984375\n",
      "2017-04-03T22:29:18.020882: step 5884, loss 0.0687803, acc 0.96875\n",
      "2017-04-03T22:29:18.211790: step 5885, loss 0.0266266, acc 0.984375\n",
      "2017-04-03T22:29:18.403700: step 5886, loss 0.0687321, acc 0.984375\n",
      "2017-04-03T22:29:18.596677: step 5887, loss 0.0571155, acc 0.984375\n",
      "2017-04-03T22:29:18.791997: step 5888, loss 0.0154844, acc 1\n",
      "2017-04-03T22:29:18.985057: step 5889, loss 0.0896144, acc 0.96875\n",
      "2017-04-03T22:29:19.176384: step 5890, loss 0.00872963, acc 1\n",
      "2017-04-03T22:29:19.369952: step 5891, loss 0.0183142, acc 1\n",
      "2017-04-03T22:29:19.562936: step 5892, loss 0.0144004, acc 1\n",
      "2017-04-03T22:29:19.763063: step 5893, loss 0.123475, acc 0.953125\n",
      "2017-04-03T22:29:19.955599: step 5894, loss 0.00704948, acc 1\n",
      "2017-04-03T22:29:20.151284: step 5895, loss 0.0442007, acc 0.984375\n",
      "2017-04-03T22:29:20.351238: step 5896, loss 0.0289194, acc 0.984375\n",
      "2017-04-03T22:29:20.548921: step 5897, loss 0.00393514, acc 1\n",
      "2017-04-03T22:29:20.741103: step 5898, loss 0.0374061, acc 0.984375\n",
      "2017-04-03T22:29:20.939711: step 5899, loss 0.00810605, acc 1\n",
      "2017-04-03T22:29:21.133195: step 5900, loss 0.107064, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:29:21.793866: step 5900, loss 2.17621, acc 0.560052\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-5900\n",
      "\n",
      "2017-04-03T22:29:22.054368: step 5901, loss 0.0327488, acc 0.984375\n",
      "2017-04-03T22:29:22.252090: step 5902, loss 0.0664401, acc 0.984375\n",
      "2017-04-03T22:29:22.449117: step 5903, loss 0.0380279, acc 1\n",
      "2017-04-03T22:29:22.645138: step 5904, loss 0.0332136, acc 0.984375\n",
      "2017-04-03T22:29:22.834884: step 5905, loss 0.0528896, acc 0.984375\n",
      "2017-04-03T22:29:23.028145: step 5906, loss 0.127406, acc 0.953125\n",
      "2017-04-03T22:29:23.217402: step 5907, loss 0.0726487, acc 0.96875\n",
      "2017-04-03T22:29:23.407667: step 5908, loss 0.0264894, acc 1\n",
      "2017-04-03T22:29:23.597668: step 5909, loss 0.00967612, acc 1\n",
      "2017-04-03T22:29:23.816990: step 5910, loss 0.0303813, acc 0.984375\n",
      "2017-04-03T22:29:24.018353: step 5911, loss 0.00639378, acc 1\n",
      "2017-04-03T22:29:24.209895: step 5912, loss 0.0480607, acc 0.984375\n",
      "2017-04-03T22:29:24.403439: step 5913, loss 0.0205322, acc 1\n",
      "2017-04-03T22:29:24.598659: step 5914, loss 0.0434784, acc 1\n",
      "2017-04-03T22:29:24.796265: step 5915, loss 0.063626, acc 0.984375\n",
      "2017-04-03T22:29:24.999545: step 5916, loss 0.0828701, acc 0.96875\n",
      "2017-04-03T22:29:25.197146: step 5917, loss 0.0452408, acc 0.984375\n",
      "2017-04-03T22:29:25.405449: step 5918, loss 0.0109008, acc 1\n",
      "2017-04-03T22:29:25.604044: step 5919, loss 0.0139913, acc 1\n",
      "2017-04-03T22:29:25.811471: step 5920, loss 0.0261506, acc 0.984375\n",
      "2017-04-03T22:29:26.006166: step 5921, loss 0.0205583, acc 1\n",
      "2017-04-03T22:29:26.203555: step 5922, loss 0.0855549, acc 0.953125\n",
      "2017-04-03T22:29:26.398996: step 5923, loss 0.0460164, acc 0.984375\n",
      "2017-04-03T22:29:26.601912: step 5924, loss 0.0130064, acc 1\n",
      "2017-04-03T22:29:26.802668: step 5925, loss 0.079348, acc 0.984375\n",
      "2017-04-03T22:29:26.994641: step 5926, loss 0.058685, acc 0.96875\n",
      "2017-04-03T22:29:27.207051: step 5927, loss 0.160047, acc 0.96875\n",
      "2017-04-03T22:29:27.429350: step 5928, loss 0.0412035, acc 0.984375\n",
      "2017-04-03T22:29:27.624540: step 5929, loss 0.0934238, acc 0.953125\n",
      "2017-04-03T22:29:27.828349: step 5930, loss 0.00973592, acc 1\n",
      "2017-04-03T22:29:28.041463: step 5931, loss 0.0547219, acc 1\n",
      "2017-04-03T22:29:28.249678: step 5932, loss 0.0516964, acc 0.984375\n",
      "2017-04-03T22:29:28.467284: step 5933, loss 0.0468281, acc 0.984375\n",
      "2017-04-03T22:29:28.673339: step 5934, loss 0.0308431, acc 0.984375\n",
      "2017-04-03T22:29:28.888811: step 5935, loss 0.00945891, acc 1\n",
      "2017-04-03T22:29:29.102296: step 5936, loss 0.0129266, acc 1\n",
      "2017-04-03T22:29:29.325287: step 5937, loss 0.0354132, acc 0.984375\n",
      "2017-04-03T22:29:29.538744: step 5938, loss 0.0113466, acc 1\n",
      "2017-04-03T22:29:29.746552: step 5939, loss 0.143118, acc 0.9375\n",
      "2017-04-03T22:29:29.928459: step 5940, loss 0.158074, acc 0.942308\n",
      "2017-04-03T22:29:30.140561: step 5941, loss 0.0361264, acc 0.984375\n",
      "2017-04-03T22:29:30.350933: step 5942, loss 0.0624302, acc 0.984375\n",
      "2017-04-03T22:29:30.568192: step 5943, loss 0.0747157, acc 0.96875\n",
      "2017-04-03T22:29:30.776073: step 5944, loss 0.0394107, acc 1\n",
      "2017-04-03T22:29:30.974736: step 5945, loss 0.185107, acc 0.9375\n",
      "2017-04-03T22:29:31.172538: step 5946, loss 0.131394, acc 0.921875\n",
      "2017-04-03T22:29:31.385349: step 5947, loss 0.0512543, acc 0.984375\n",
      "2017-04-03T22:29:31.583417: step 5948, loss 0.0333544, acc 0.984375\n",
      "2017-04-03T22:29:31.789446: step 5949, loss 0.0037247, acc 1\n",
      "2017-04-03T22:29:32.013223: step 5950, loss 0.0208844, acc 1\n",
      "2017-04-03T22:29:32.230852: step 5951, loss 0.0204295, acc 0.984375\n",
      "2017-04-03T22:29:32.438513: step 5952, loss 0.0672209, acc 0.984375\n",
      "2017-04-03T22:29:32.653433: step 5953, loss 0.0113243, acc 1\n",
      "2017-04-03T22:29:32.863090: step 5954, loss 0.0338838, acc 0.984375\n",
      "2017-04-03T22:29:33.067945: step 5955, loss 0.124302, acc 0.953125\n",
      "2017-04-03T22:29:33.257638: step 5956, loss 0.105985, acc 0.96875\n",
      "2017-04-03T22:29:33.450870: step 5957, loss 0.035566, acc 1\n",
      "2017-04-03T22:29:33.644878: step 5958, loss 0.07389, acc 0.96875\n",
      "2017-04-03T22:29:33.847916: step 5959, loss 0.00788915, acc 1\n",
      "2017-04-03T22:29:34.056012: step 5960, loss 0.0579039, acc 1\n",
      "2017-04-03T22:29:34.252902: step 5961, loss 0.0354299, acc 0.984375\n",
      "2017-04-03T22:29:34.460242: step 5962, loss 0.0875317, acc 0.953125\n",
      "2017-04-03T22:29:34.653123: step 5963, loss 0.0105945, acc 1\n",
      "2017-04-03T22:29:34.852467: step 5964, loss 0.0598363, acc 0.984375\n",
      "2017-04-03T22:29:35.054249: step 5965, loss 0.00985864, acc 1\n",
      "2017-04-03T22:29:35.253929: step 5966, loss 0.0392469, acc 1\n",
      "2017-04-03T22:29:35.448962: step 5967, loss 0.00627034, acc 1\n",
      "2017-04-03T22:29:35.642583: step 5968, loss 0.0436359, acc 0.984375\n",
      "2017-04-03T22:29:35.845610: step 5969, loss 0.0167381, acc 1\n",
      "2017-04-03T22:29:36.054340: step 5970, loss 0.0309895, acc 0.984375\n",
      "2017-04-03T22:29:36.246750: step 5971, loss 0.0472348, acc 0.96875\n",
      "2017-04-03T22:29:36.444186: step 5972, loss 0.0201548, acc 1\n",
      "2017-04-03T22:29:36.644602: step 5973, loss 0.0507894, acc 0.953125\n",
      "2017-04-03T22:29:36.840887: step 5974, loss 0.102747, acc 0.96875\n",
      "2017-04-03T22:29:37.042589: step 5975, loss 0.0194573, acc 1\n",
      "2017-04-03T22:29:37.237795: step 5976, loss 0.0216466, acc 1\n",
      "2017-04-03T22:29:37.439028: step 5977, loss 0.0100884, acc 1\n",
      "2017-04-03T22:29:37.633494: step 5978, loss 0.0655339, acc 0.984375\n",
      "2017-04-03T22:29:37.839968: step 5979, loss 0.132446, acc 0.984375\n",
      "2017-04-03T22:29:38.040693: step 5980, loss 0.0416897, acc 1\n",
      "2017-04-03T22:29:38.245393: step 5981, loss 0.0284706, acc 1\n",
      "2017-04-03T22:29:38.446622: step 5982, loss 0.0576437, acc 0.96875\n",
      "2017-04-03T22:29:38.654223: step 5983, loss 0.0152332, acc 1\n",
      "2017-04-03T22:29:38.849708: step 5984, loss 0.0396277, acc 0.984375\n",
      "2017-04-03T22:29:39.053384: step 5985, loss 0.0630572, acc 0.984375\n",
      "2017-04-03T22:29:39.267572: step 5986, loss 0.0233918, acc 0.984375\n",
      "2017-04-03T22:29:39.464239: step 5987, loss 0.00609355, acc 1\n",
      "2017-04-03T22:29:39.668638: step 5988, loss 0.0274658, acc 1\n",
      "2017-04-03T22:29:39.862540: step 5989, loss 0.0222306, acc 1\n",
      "2017-04-03T22:29:40.058144: step 5990, loss 0.0182714, acc 1\n",
      "2017-04-03T22:29:40.267297: step 5991, loss 0.0453388, acc 0.984375\n",
      "2017-04-03T22:29:40.484083: step 5992, loss 0.0952054, acc 0.96875\n",
      "2017-04-03T22:29:40.706174: step 5993, loss 0.0369131, acc 0.984375\n",
      "2017-04-03T22:29:40.933059: step 5994, loss 0.00553797, acc 1\n",
      "2017-04-03T22:29:41.142423: step 5995, loss 0.0451427, acc 0.984375\n",
      "2017-04-03T22:29:41.341811: step 5996, loss 0.0965957, acc 0.96875\n",
      "2017-04-03T22:29:41.548276: step 5997, loss 0.0505382, acc 0.984375\n",
      "2017-04-03T22:29:41.760335: step 5998, loss 0.0163336, acc 1\n",
      "2017-04-03T22:29:41.964617: step 5999, loss 0.0141074, acc 1\n",
      "2017-04-03T22:29:42.169476: step 6000, loss 0.016615, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:29:42.891564: step 6000, loss 2.27521, acc 0.562663\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6000\n",
      "\n",
      "2017-04-03T22:29:43.169626: step 6001, loss 0.0600891, acc 0.984375\n",
      "2017-04-03T22:29:43.385378: step 6002, loss 0.0629154, acc 1\n",
      "2017-04-03T22:29:43.580163: step 6003, loss 0.00718189, acc 1\n",
      "2017-04-03T22:29:43.780159: step 6004, loss 0.0197773, acc 1\n",
      "2017-04-03T22:29:43.977531: step 6005, loss 0.0402523, acc 1\n",
      "2017-04-03T22:29:44.169356: step 6006, loss 0.0353721, acc 1\n",
      "2017-04-03T22:29:44.374986: step 6007, loss 0.00693266, acc 1\n",
      "2017-04-03T22:29:44.590277: step 6008, loss 0.0543856, acc 0.984375\n",
      "2017-04-03T22:29:44.797052: step 6009, loss 0.00858061, acc 1\n",
      "2017-04-03T22:29:45.006939: step 6010, loss 0.0179344, acc 1\n",
      "2017-04-03T22:29:45.213312: step 6011, loss 0.0463574, acc 0.96875\n",
      "2017-04-03T22:29:45.417996: step 6012, loss 0.0406633, acc 0.984375\n",
      "2017-04-03T22:29:45.624882: step 6013, loss 0.0101666, acc 1\n",
      "2017-04-03T22:29:45.826029: step 6014, loss 0.038327, acc 0.984375\n",
      "2017-04-03T22:29:46.022644: step 6015, loss 0.0380325, acc 0.984375\n",
      "2017-04-03T22:29:46.218507: step 6016, loss 0.0729605, acc 0.96875\n",
      "2017-04-03T22:29:46.426210: step 6017, loss 0.0568474, acc 0.984375\n",
      "2017-04-03T22:29:46.631885: step 6018, loss 0.0393729, acc 1\n",
      "2017-04-03T22:29:46.832348: step 6019, loss 0.0919154, acc 0.96875\n",
      "2017-04-03T22:29:47.030688: step 6020, loss 0.0153007, acc 1\n",
      "2017-04-03T22:29:47.223326: step 6021, loss 0.0310405, acc 0.984375\n",
      "2017-04-03T22:29:47.421914: step 6022, loss 0.0780521, acc 0.96875\n",
      "2017-04-03T22:29:47.614335: step 6023, loss 0.107292, acc 0.96875\n",
      "2017-04-03T22:29:47.810104: step 6024, loss 0.0270012, acc 0.984375\n",
      "2017-04-03T22:29:48.020952: step 6025, loss 0.0634149, acc 0.96875\n",
      "2017-04-03T22:29:48.216101: step 6026, loss 0.0125779, acc 1\n",
      "2017-04-03T22:29:48.423678: step 6027, loss 0.0650432, acc 0.96875\n",
      "2017-04-03T22:29:48.628565: step 6028, loss 0.0203891, acc 1\n",
      "2017-04-03T22:29:48.840002: step 6029, loss 0.158065, acc 0.953125\n",
      "2017-04-03T22:29:49.052505: step 6030, loss 0.0323003, acc 0.984375\n",
      "2017-04-03T22:29:49.264532: step 6031, loss 0.0699474, acc 0.96875\n",
      "2017-04-03T22:29:49.475351: step 6032, loss 0.104984, acc 0.96875\n",
      "2017-04-03T22:29:49.688492: step 6033, loss 0.0420869, acc 0.984375\n",
      "2017-04-03T22:29:49.896318: step 6034, loss 0.119864, acc 0.953125\n",
      "2017-04-03T22:29:50.125218: step 6035, loss 0.0518749, acc 0.984375\n",
      "2017-04-03T22:29:50.335573: step 6036, loss 0.125819, acc 0.953125\n",
      "2017-04-03T22:29:50.549217: step 6037, loss 0.0209279, acc 0.984375\n",
      "2017-04-03T22:29:50.760604: step 6038, loss 0.116326, acc 0.9375\n",
      "2017-04-03T22:29:50.973373: step 6039, loss 0.109455, acc 0.96875\n",
      "2017-04-03T22:29:51.186761: step 6040, loss 0.045024, acc 0.984375\n",
      "2017-04-03T22:29:51.404309: step 6041, loss 0.0076063, acc 1\n",
      "2017-04-03T22:29:51.622147: step 6042, loss 0.0428385, acc 0.984375\n",
      "2017-04-03T22:29:51.843494: step 6043, loss 0.0112442, acc 1\n",
      "2017-04-03T22:29:52.045675: step 6044, loss 0.00490072, acc 1\n",
      "2017-04-03T22:29:52.250310: step 6045, loss 0.0309402, acc 1\n",
      "2017-04-03T22:29:52.455145: step 6046, loss 0.0730319, acc 0.96875\n",
      "2017-04-03T22:29:52.651186: step 6047, loss 0.00869145, acc 1\n",
      "2017-04-03T22:29:52.816288: step 6048, loss 0.0844037, acc 0.961538\n",
      "2017-04-03T22:29:53.024377: step 6049, loss 0.0861302, acc 0.96875\n",
      "2017-04-03T22:29:53.233621: step 6050, loss 0.0276693, acc 0.984375\n",
      "2017-04-03T22:29:53.428436: step 6051, loss 0.0190287, acc 0.984375\n",
      "2017-04-03T22:29:53.633457: step 6052, loss 0.105557, acc 0.96875\n",
      "2017-04-03T22:29:53.824619: step 6053, loss 0.0069739, acc 1\n",
      "2017-04-03T22:29:54.022440: step 6054, loss 0.0601832, acc 0.984375\n",
      "2017-04-03T22:29:54.230327: step 6055, loss 0.0882659, acc 0.96875\n",
      "2017-04-03T22:29:54.426526: step 6056, loss 0.118802, acc 0.953125\n",
      "2017-04-03T22:29:54.618720: step 6057, loss 0.0430238, acc 0.984375\n",
      "2017-04-03T22:29:54.817254: step 6058, loss 0.0985534, acc 0.953125\n",
      "2017-04-03T22:29:55.016147: step 6059, loss 0.091526, acc 0.953125\n",
      "2017-04-03T22:29:55.224259: step 6060, loss 0.00411013, acc 1\n",
      "2017-04-03T22:29:55.426109: step 6061, loss 0.0976253, acc 0.984375\n",
      "2017-04-03T22:29:55.632962: step 6062, loss 0.0743727, acc 0.953125\n",
      "2017-04-03T22:29:55.838890: step 6063, loss 0.0223619, acc 1\n",
      "2017-04-03T22:29:56.032660: step 6064, loss 0.0765974, acc 1\n",
      "2017-04-03T22:29:56.235776: step 6065, loss 0.0107224, acc 1\n",
      "2017-04-03T22:29:56.446246: step 6066, loss 0.0315255, acc 0.984375\n",
      "2017-04-03T22:29:56.644335: step 6067, loss 0.108022, acc 0.953125\n",
      "2017-04-03T22:29:56.848577: step 6068, loss 0.0716841, acc 0.96875\n",
      "2017-04-03T22:29:57.043240: step 6069, loss 0.0449214, acc 0.984375\n",
      "2017-04-03T22:29:57.248686: step 6070, loss 0.0189997, acc 1\n",
      "2017-04-03T22:29:57.440365: step 6071, loss 0.0891791, acc 0.96875\n",
      "2017-04-03T22:29:57.632630: step 6072, loss 0.0201479, acc 1\n",
      "2017-04-03T22:29:57.829932: step 6073, loss 0.0338119, acc 0.984375\n",
      "2017-04-03T22:29:58.035965: step 6074, loss 0.0378233, acc 1\n",
      "2017-04-03T22:29:58.229384: step 6075, loss 0.0338298, acc 1\n",
      "2017-04-03T22:29:58.425726: step 6076, loss 0.00298139, acc 1\n",
      "2017-04-03T22:29:58.625347: step 6077, loss 0.008839, acc 1\n",
      "2017-04-03T22:29:58.837163: step 6078, loss 0.107194, acc 0.96875\n",
      "2017-04-03T22:29:59.035087: step 6079, loss 0.476912, acc 0.96875\n",
      "2017-04-03T22:29:59.236616: step 6080, loss 0.0400858, acc 0.984375\n",
      "2017-04-03T22:29:59.431499: step 6081, loss 0.0941992, acc 0.96875\n",
      "2017-04-03T22:29:59.649629: step 6082, loss 0.0313522, acc 0.984375\n",
      "2017-04-03T22:29:59.864273: step 6083, loss 0.0239975, acc 1\n",
      "2017-04-03T22:30:00.064139: step 6084, loss 0.0863155, acc 0.96875\n",
      "2017-04-03T22:30:00.260991: step 6085, loss 0.0375899, acc 0.984375\n",
      "2017-04-03T22:30:00.455813: step 6086, loss 0.086726, acc 0.96875\n",
      "2017-04-03T22:30:00.671414: step 6087, loss 0.0648892, acc 0.984375\n",
      "2017-04-03T22:30:00.883477: step 6088, loss 0.0151269, acc 1\n",
      "2017-04-03T22:30:01.076936: step 6089, loss 0.0691788, acc 0.96875\n",
      "2017-04-03T22:30:01.276383: step 6090, loss 0.00979702, acc 1\n",
      "2017-04-03T22:30:01.476083: step 6091, loss 0.0233428, acc 0.984375\n",
      "2017-04-03T22:30:01.692457: step 6092, loss 0.0790914, acc 0.953125\n",
      "2017-04-03T22:30:01.908405: step 6093, loss 0.0388236, acc 0.984375\n",
      "2017-04-03T22:30:02.118414: step 6094, loss 0.019412, acc 1\n",
      "2017-04-03T22:30:02.335058: step 6095, loss 0.028008, acc 0.984375\n",
      "2017-04-03T22:30:02.552786: step 6096, loss 0.0793083, acc 0.96875\n",
      "2017-04-03T22:30:02.762257: step 6097, loss 0.0335222, acc 1\n",
      "2017-04-03T22:30:02.971819: step 6098, loss 0.037676, acc 1\n",
      "2017-04-03T22:30:03.184980: step 6099, loss 0.0407786, acc 1\n",
      "2017-04-03T22:30:03.397600: step 6100, loss 0.0976233, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:30:04.075316: step 6100, loss 2.27589, acc 0.56658\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6100\n",
      "\n",
      "2017-04-03T22:30:04.337582: step 6101, loss 0.0749846, acc 0.96875\n",
      "2017-04-03T22:30:04.533933: step 6102, loss 0.0358561, acc 0.984375\n",
      "2017-04-03T22:30:04.727957: step 6103, loss 0.0994144, acc 0.953125\n",
      "2017-04-03T22:30:04.920944: step 6104, loss 0.0322675, acc 0.984375\n",
      "2017-04-03T22:30:05.122513: step 6105, loss 0.0809792, acc 0.96875\n",
      "2017-04-03T22:30:05.318072: step 6106, loss 0.127956, acc 0.96875\n",
      "2017-04-03T22:30:05.518837: step 6107, loss 0.0306213, acc 1\n",
      "2017-04-03T22:30:05.712862: step 6108, loss 0.0307217, acc 1\n",
      "2017-04-03T22:30:05.910999: step 6109, loss 0.0141238, acc 1\n",
      "2017-04-03T22:30:06.114769: step 6110, loss 0.0289614, acc 0.984375\n",
      "2017-04-03T22:30:06.323095: step 6111, loss 0.0193843, acc 1\n",
      "2017-04-03T22:30:06.529286: step 6112, loss 0.0154206, acc 1\n",
      "2017-04-03T22:30:06.755666: step 6113, loss 0.0107453, acc 1\n",
      "2017-04-03T22:30:06.982008: step 6114, loss 0.058722, acc 0.96875\n",
      "2017-04-03T22:30:07.204407: step 6115, loss 0.151769, acc 0.953125\n",
      "2017-04-03T22:30:07.403513: step 6116, loss 0.0473474, acc 0.984375\n",
      "2017-04-03T22:30:07.608668: step 6117, loss 0.0217689, acc 1\n",
      "2017-04-03T22:30:07.808408: step 6118, loss 0.0891373, acc 0.96875\n",
      "2017-04-03T22:30:08.004320: step 6119, loss 0.077623, acc 0.96875\n",
      "2017-04-03T22:30:08.197171: step 6120, loss 0.0334754, acc 0.96875\n",
      "2017-04-03T22:30:08.391908: step 6121, loss 0.0557737, acc 0.96875\n",
      "2017-04-03T22:30:08.588121: step 6122, loss 0.00974041, acc 1\n",
      "2017-04-03T22:30:08.778722: step 6123, loss 0.0415417, acc 0.984375\n",
      "2017-04-03T22:30:08.969388: step 6124, loss 0.0443255, acc 0.984375\n",
      "2017-04-03T22:30:09.161270: step 6125, loss 0.0577886, acc 0.984375\n",
      "2017-04-03T22:30:09.350450: step 6126, loss 0.0589569, acc 0.96875\n",
      "2017-04-03T22:30:09.542531: step 6127, loss 0.0624099, acc 0.96875\n",
      "2017-04-03T22:30:09.750460: step 6128, loss 0.0933615, acc 0.953125\n",
      "2017-04-03T22:30:09.952318: step 6129, loss 0.177827, acc 0.953125\n",
      "2017-04-03T22:30:10.142862: step 6130, loss 0.0667849, acc 0.96875\n",
      "2017-04-03T22:30:10.341276: step 6131, loss 0.0192787, acc 1\n",
      "2017-04-03T22:30:10.534582: step 6132, loss 0.0105514, acc 1\n",
      "2017-04-03T22:30:10.728804: step 6133, loss 0.0113186, acc 1\n",
      "2017-04-03T22:30:10.918740: step 6134, loss 0.0523691, acc 0.984375\n",
      "2017-04-03T22:30:11.110140: step 6135, loss 0.0534873, acc 0.984375\n",
      "2017-04-03T22:30:11.305209: step 6136, loss 0.0339827, acc 0.984375\n",
      "2017-04-03T22:30:11.500795: step 6137, loss 0.0209336, acc 1\n",
      "2017-04-03T22:30:11.698753: step 6138, loss 0.0575286, acc 0.984375\n",
      "2017-04-03T22:30:11.893277: step 6139, loss 0.0534885, acc 0.96875\n",
      "2017-04-03T22:30:12.085352: step 6140, loss 0.0605774, acc 0.96875\n",
      "2017-04-03T22:30:12.282954: step 6141, loss 0.00256567, acc 1\n",
      "2017-04-03T22:30:12.474034: step 6142, loss 0.105827, acc 0.96875\n",
      "2017-04-03T22:30:12.671446: step 6143, loss 0.0569462, acc 0.984375\n",
      "2017-04-03T22:30:12.862237: step 6144, loss 0.0154523, acc 1\n",
      "2017-04-03T22:30:13.055546: step 6145, loss 0.0124095, acc 1\n",
      "2017-04-03T22:30:13.249392: step 6146, loss 0.0826617, acc 0.96875\n",
      "2017-04-03T22:30:13.461259: step 6147, loss 0.0151508, acc 1\n",
      "2017-04-03T22:30:13.664990: step 6148, loss 0.0366783, acc 0.984375\n",
      "2017-04-03T22:30:13.865039: step 6149, loss 0.0766799, acc 0.96875\n",
      "2017-04-03T22:30:14.073961: step 6150, loss 0.0464919, acc 0.984375\n",
      "2017-04-03T22:30:14.294220: step 6151, loss 0.104098, acc 0.953125\n",
      "2017-04-03T22:30:14.493075: step 6152, loss 0.0761125, acc 0.984375\n",
      "2017-04-03T22:30:14.691239: step 6153, loss 0.0159044, acc 1\n",
      "2017-04-03T22:30:14.887890: step 6154, loss 0.0384069, acc 0.984375\n",
      "2017-04-03T22:30:15.101652: step 6155, loss 0.0306236, acc 0.984375\n",
      "2017-04-03T22:30:15.278633: step 6156, loss 0.097341, acc 0.961538\n",
      "2017-04-03T22:30:15.488472: step 6157, loss 0.0670779, acc 0.96875\n",
      "2017-04-03T22:30:15.719309: step 6158, loss 0.0839874, acc 0.96875\n",
      "2017-04-03T22:30:15.912447: step 6159, loss 0.0647578, acc 0.96875\n",
      "2017-04-03T22:30:16.104396: step 6160, loss 0.0929697, acc 0.953125\n",
      "2017-04-03T22:30:16.302984: step 6161, loss 0.0146047, acc 1\n",
      "2017-04-03T22:30:16.518311: step 6162, loss 0.0330774, acc 0.984375\n",
      "2017-04-03T22:30:16.717031: step 6163, loss 0.0495942, acc 0.984375\n",
      "2017-04-03T22:30:16.924669: step 6164, loss 0.0699405, acc 0.984375\n",
      "2017-04-03T22:30:17.128075: step 6165, loss 0.0497712, acc 0.984375\n",
      "2017-04-03T22:30:17.333823: step 6166, loss 0.0316024, acc 0.984375\n",
      "2017-04-03T22:30:17.536245: step 6167, loss 0.0221481, acc 1\n",
      "2017-04-03T22:30:17.729116: step 6168, loss 0.0440517, acc 0.96875\n",
      "2017-04-03T22:30:17.921594: step 6169, loss 0.0788248, acc 0.96875\n",
      "2017-04-03T22:30:18.112050: step 6170, loss 0.0330445, acc 0.984375\n",
      "2017-04-03T22:30:18.314803: step 6171, loss 0.00604709, acc 1\n",
      "2017-04-03T22:30:18.522697: step 6172, loss 0.0425863, acc 0.984375\n",
      "2017-04-03T22:30:18.715348: step 6173, loss 0.0991504, acc 0.953125\n",
      "2017-04-03T22:30:18.913996: step 6174, loss 0.0546461, acc 0.984375\n",
      "2017-04-03T22:30:19.114556: step 6175, loss 0.0476356, acc 1\n",
      "2017-04-03T22:30:19.319908: step 6176, loss 0.0252918, acc 1\n",
      "2017-04-03T22:30:19.516380: step 6177, loss 0.0305102, acc 1\n",
      "2017-04-03T22:30:19.724670: step 6178, loss 0.0104166, acc 1\n",
      "2017-04-03T22:30:19.918383: step 6179, loss 0.0573137, acc 0.984375\n",
      "2017-04-03T22:30:20.107662: step 6180, loss 0.00960592, acc 1\n",
      "2017-04-03T22:30:20.306057: step 6181, loss 0.0603221, acc 0.96875\n",
      "2017-04-03T22:30:20.520766: step 6182, loss 0.0050431, acc 1\n",
      "2017-04-03T22:30:20.709619: step 6183, loss 0.0135558, acc 1\n",
      "2017-04-03T22:30:20.911654: step 6184, loss 0.0918889, acc 0.96875\n",
      "2017-04-03T22:30:21.111795: step 6185, loss 0.0330536, acc 1\n",
      "2017-04-03T22:30:21.302757: step 6186, loss 0.0122185, acc 1\n",
      "2017-04-03T22:30:21.499653: step 6187, loss 0.0591786, acc 0.96875\n",
      "2017-04-03T22:30:21.708111: step 6188, loss 0.036399, acc 0.984375\n",
      "2017-04-03T22:30:21.916636: step 6189, loss 0.0757026, acc 0.96875\n",
      "2017-04-03T22:30:22.129471: step 6190, loss 0.0387708, acc 0.984375\n",
      "2017-04-03T22:30:22.344209: step 6191, loss 0.0294143, acc 0.984375\n",
      "2017-04-03T22:30:22.560696: step 6192, loss 0.121382, acc 0.953125\n",
      "2017-04-03T22:30:22.779893: step 6193, loss 0.0429673, acc 0.96875\n",
      "2017-04-03T22:30:22.995543: step 6194, loss 0.0369513, acc 0.984375\n",
      "2017-04-03T22:30:23.214201: step 6195, loss 0.00634253, acc 1\n",
      "2017-04-03T22:30:23.423638: step 6196, loss 0.0339472, acc 0.984375\n",
      "2017-04-03T22:30:23.630360: step 6197, loss 0.0411023, acc 0.96875\n",
      "2017-04-03T22:30:23.838148: step 6198, loss 0.0286189, acc 0.984375\n",
      "2017-04-03T22:30:24.045072: step 6199, loss 0.0152637, acc 1\n",
      "2017-04-03T22:30:24.264256: step 6200, loss 0.0411649, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:30:24.977553: step 6200, loss 2.26191, acc 0.56658\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6200\n",
      "\n",
      "2017-04-03T22:30:25.239731: step 6201, loss 0.0425903, acc 0.984375\n",
      "2017-04-03T22:30:25.438454: step 6202, loss 0.223469, acc 0.96875\n",
      "2017-04-03T22:30:25.636156: step 6203, loss 0.039054, acc 1\n",
      "2017-04-03T22:30:25.834966: step 6204, loss 0.0147237, acc 1\n",
      "2017-04-03T22:30:26.029250: step 6205, loss 0.0386052, acc 0.984375\n",
      "2017-04-03T22:30:26.221981: step 6206, loss 0.0258433, acc 1\n",
      "2017-04-03T22:30:26.414735: step 6207, loss 0.0516013, acc 0.96875\n",
      "2017-04-03T22:30:26.619934: step 6208, loss 0.0376656, acc 0.984375\n",
      "2017-04-03T22:30:26.831098: step 6209, loss 0.0478678, acc 0.984375\n",
      "2017-04-03T22:30:27.038467: step 6210, loss 0.0340563, acc 0.984375\n",
      "2017-04-03T22:30:27.248128: step 6211, loss 0.0103613, acc 1\n",
      "2017-04-03T22:30:27.454815: step 6212, loss 0.0352325, acc 0.984375\n",
      "2017-04-03T22:30:27.650556: step 6213, loss 0.0795783, acc 0.984375\n",
      "2017-04-03T22:30:27.844774: step 6214, loss 0.0127178, acc 1\n",
      "2017-04-03T22:30:28.039452: step 6215, loss 0.109266, acc 0.96875\n",
      "2017-04-03T22:30:28.235237: step 6216, loss 0.0023524, acc 1\n",
      "2017-04-03T22:30:28.438974: step 6217, loss 0.0260463, acc 1\n",
      "2017-04-03T22:30:28.660652: step 6218, loss 0.0419711, acc 0.984375\n",
      "2017-04-03T22:30:28.876296: step 6219, loss 0.0877603, acc 0.96875\n",
      "2017-04-03T22:30:29.090028: step 6220, loss 0.106798, acc 0.953125\n",
      "2017-04-03T22:30:29.308583: step 6221, loss 0.0950049, acc 0.9375\n",
      "2017-04-03T22:30:29.522947: step 6222, loss 0.0606024, acc 0.984375\n",
      "2017-04-03T22:30:29.716775: step 6223, loss 0.0515421, acc 0.984375\n",
      "2017-04-03T22:30:29.914198: step 6224, loss 0.0705014, acc 0.96875\n",
      "2017-04-03T22:30:30.114986: step 6225, loss 0.0161099, acc 0.984375\n",
      "2017-04-03T22:30:30.324563: step 6226, loss 0.0686178, acc 0.96875\n",
      "2017-04-03T22:30:30.534051: step 6227, loss 0.0888385, acc 0.96875\n",
      "2017-04-03T22:30:30.752933: step 6228, loss 0.0979021, acc 0.953125\n",
      "2017-04-03T22:30:30.939547: step 6229, loss 0.0632857, acc 0.96875\n",
      "2017-04-03T22:30:31.133911: step 6230, loss 0.0707007, acc 0.96875\n",
      "2017-04-03T22:30:31.330770: step 6231, loss 0.0375601, acc 0.984375\n",
      "2017-04-03T22:30:31.531370: step 6232, loss 0.0166899, acc 1\n",
      "2017-04-03T22:30:31.731060: step 6233, loss 0.0483053, acc 0.984375\n",
      "2017-04-03T22:30:31.932972: step 6234, loss 0.0468261, acc 1\n",
      "2017-04-03T22:30:32.141347: step 6235, loss 0.00939039, acc 1\n",
      "2017-04-03T22:30:32.348327: step 6236, loss 0.0244019, acc 1\n",
      "2017-04-03T22:30:32.558682: step 6237, loss 0.149373, acc 0.9375\n",
      "2017-04-03T22:30:32.756666: step 6238, loss 0.0848882, acc 0.984375\n",
      "2017-04-03T22:30:32.948340: step 6239, loss 0.0409833, acc 0.984375\n",
      "2017-04-03T22:30:33.150003: step 6240, loss 0.0061578, acc 1\n",
      "2017-04-03T22:30:33.342866: step 6241, loss 0.110574, acc 0.953125\n",
      "2017-04-03T22:30:33.540034: step 6242, loss 0.0470382, acc 0.984375\n",
      "2017-04-03T22:30:33.768486: step 6243, loss 0.0261081, acc 0.984375\n",
      "2017-04-03T22:30:33.983086: step 6244, loss 0.0390517, acc 1\n",
      "2017-04-03T22:30:34.202418: step 6245, loss 0.0291499, acc 1\n",
      "2017-04-03T22:30:34.403148: step 6246, loss 0.0238922, acc 0.984375\n",
      "2017-04-03T22:30:34.596898: step 6247, loss 0.00779492, acc 1\n",
      "2017-04-03T22:30:34.802055: step 6248, loss 0.00473711, acc 1\n",
      "2017-04-03T22:30:35.014564: step 6249, loss 0.0048189, acc 1\n",
      "2017-04-03T22:30:35.221598: step 6250, loss 0.0674438, acc 0.96875\n",
      "2017-04-03T22:30:35.424769: step 6251, loss 0.00996447, acc 1\n",
      "2017-04-03T22:30:35.621803: step 6252, loss 0.125046, acc 0.96875\n",
      "2017-04-03T22:30:35.811594: step 6253, loss 0.0584859, acc 0.984375\n",
      "2017-04-03T22:30:36.008271: step 6254, loss 0.0979096, acc 0.96875\n",
      "2017-04-03T22:30:36.215319: step 6255, loss 0.00968162, acc 1\n",
      "2017-04-03T22:30:36.413701: step 6256, loss 0.0515461, acc 0.984375\n",
      "2017-04-03T22:30:36.643747: step 6257, loss 0.0635653, acc 0.96875\n",
      "2017-04-03T22:30:36.857183: step 6258, loss 0.07819, acc 0.96875\n",
      "2017-04-03T22:30:37.069772: step 6259, loss 0.032078, acc 0.984375\n",
      "2017-04-03T22:30:37.284051: step 6260, loss 0.0151931, acc 1\n",
      "2017-04-03T22:30:37.479957: step 6261, loss 0.0468522, acc 0.984375\n",
      "2017-04-03T22:30:37.673398: step 6262, loss 0.0664046, acc 0.96875\n",
      "2017-04-03T22:30:37.889368: step 6263, loss 0.0257784, acc 1\n",
      "2017-04-03T22:30:38.080024: step 6264, loss 0.0745843, acc 0.961538\n",
      "2017-04-03T22:30:38.287939: step 6265, loss 0.089983, acc 0.984375\n",
      "2017-04-03T22:30:38.496087: step 6266, loss 0.0145756, acc 1\n",
      "2017-04-03T22:30:38.695858: step 6267, loss 0.0528874, acc 0.984375\n",
      "2017-04-03T22:30:38.905285: step 6268, loss 0.0343324, acc 0.984375\n",
      "2017-04-03T22:30:39.125281: step 6269, loss 0.0792517, acc 0.96875\n",
      "2017-04-03T22:30:39.346949: step 6270, loss 0.00920019, acc 1\n",
      "2017-04-03T22:30:39.545417: step 6271, loss 0.0582223, acc 0.984375\n",
      "2017-04-03T22:30:39.740910: step 6272, loss 0.0555415, acc 0.96875\n",
      "2017-04-03T22:30:39.936491: step 6273, loss 0.0147571, acc 1\n",
      "2017-04-03T22:30:40.128759: step 6274, loss 0.0520071, acc 0.984375\n",
      "2017-04-03T22:30:40.321314: step 6275, loss 0.104013, acc 0.953125\n",
      "2017-04-03T22:30:40.518448: step 6276, loss 0.00926358, acc 1\n",
      "2017-04-03T22:30:40.714847: step 6277, loss 0.0137227, acc 1\n",
      "2017-04-03T22:30:40.906186: step 6278, loss 0.0821326, acc 0.984375\n",
      "2017-04-03T22:30:41.101067: step 6279, loss 0.0491397, acc 0.984375\n",
      "2017-04-03T22:30:41.297297: step 6280, loss 0.0450678, acc 0.984375\n",
      "2017-04-03T22:30:41.493932: step 6281, loss 0.076502, acc 0.96875\n",
      "2017-04-03T22:30:41.710385: step 6282, loss 0.063563, acc 0.984375\n",
      "2017-04-03T22:30:41.925750: step 6283, loss 0.0212351, acc 1\n",
      "2017-04-03T22:30:42.126139: step 6284, loss 0.0193507, acc 1\n",
      "2017-04-03T22:30:42.326536: step 6285, loss 0.0135051, acc 1\n",
      "2017-04-03T22:30:42.518117: step 6286, loss 0.00683673, acc 1\n",
      "2017-04-03T22:30:42.715563: step 6287, loss 0.0352019, acc 0.984375\n",
      "2017-04-03T22:30:42.918593: step 6288, loss 0.0166911, acc 1\n",
      "2017-04-03T22:30:43.111677: step 6289, loss 0.0245424, acc 1\n",
      "2017-04-03T22:30:43.321669: step 6290, loss 0.0315498, acc 0.984375\n",
      "2017-04-03T22:30:43.511980: step 6291, loss 0.0411619, acc 1\n",
      "2017-04-03T22:30:43.706213: step 6292, loss 0.018931, acc 0.984375\n",
      "2017-04-03T22:30:43.898921: step 6293, loss 0.030507, acc 0.984375\n",
      "2017-04-03T22:30:44.096671: step 6294, loss 0.00955507, acc 1\n",
      "2017-04-03T22:30:44.290098: step 6295, loss 0.127356, acc 0.953125\n",
      "2017-04-03T22:30:44.492689: step 6296, loss 0.0955199, acc 0.953125\n",
      "2017-04-03T22:30:44.686923: step 6297, loss 0.075158, acc 0.984375\n",
      "2017-04-03T22:30:44.885034: step 6298, loss 0.0052225, acc 1\n",
      "2017-04-03T22:30:45.079612: step 6299, loss 0.0602777, acc 0.96875\n",
      "2017-04-03T22:30:45.277797: step 6300, loss 0.0826836, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:30:45.937062: step 6300, loss 2.27053, acc 0.55483\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6300\n",
      "\n",
      "2017-04-03T22:30:46.194489: step 6301, loss 0.00427198, acc 1\n",
      "2017-04-03T22:30:46.388975: step 6302, loss 0.119316, acc 0.9375\n",
      "2017-04-03T22:30:46.581020: step 6303, loss 0.037866, acc 0.984375\n",
      "2017-04-03T22:30:46.774771: step 6304, loss 0.0486421, acc 0.984375\n",
      "2017-04-03T22:30:46.967232: step 6305, loss 0.00450112, acc 1\n",
      "2017-04-03T22:30:47.159340: step 6306, loss 0.074187, acc 0.984375\n",
      "2017-04-03T22:30:47.352747: step 6307, loss 0.16114, acc 0.9375\n",
      "2017-04-03T22:30:47.567975: step 6308, loss 0.0394403, acc 0.984375\n",
      "2017-04-03T22:30:47.763933: step 6309, loss 0.0424492, acc 0.96875\n",
      "2017-04-03T22:30:47.955213: step 6310, loss 0.0162796, acc 1\n",
      "2017-04-03T22:30:48.147144: step 6311, loss 0.0241826, acc 0.984375\n",
      "2017-04-03T22:30:48.341800: step 6312, loss 0.0674364, acc 0.96875\n",
      "2017-04-03T22:30:48.534222: step 6313, loss 0.0464751, acc 0.984375\n",
      "2017-04-03T22:30:48.727100: step 6314, loss 0.0246885, acc 1\n",
      "2017-04-03T22:30:48.928617: step 6315, loss 0.041759, acc 0.984375\n",
      "2017-04-03T22:30:49.128011: step 6316, loss 0.0342725, acc 0.984375\n",
      "2017-04-03T22:30:49.327768: step 6317, loss 0.0371731, acc 0.984375\n",
      "2017-04-03T22:30:49.537204: step 6318, loss 0.00652657, acc 1\n",
      "2017-04-03T22:30:49.752491: step 6319, loss 0.0367031, acc 1\n",
      "2017-04-03T22:30:49.958080: step 6320, loss 0.0514901, acc 0.984375\n",
      "2017-04-03T22:30:50.156143: step 6321, loss 0.0150085, acc 1\n",
      "2017-04-03T22:30:50.347761: step 6322, loss 0.0407606, acc 0.984375\n",
      "2017-04-03T22:30:50.550515: step 6323, loss 0.0716537, acc 0.984375\n",
      "2017-04-03T22:30:50.747382: step 6324, loss 0.0692916, acc 0.984375\n",
      "2017-04-03T22:30:50.943100: step 6325, loss 0.073961, acc 0.96875\n",
      "2017-04-03T22:30:51.133720: step 6326, loss 0.0363727, acc 0.984375\n",
      "2017-04-03T22:30:51.328769: step 6327, loss 0.024196, acc 0.984375\n",
      "2017-04-03T22:30:51.526417: step 6328, loss 0.0364642, acc 0.984375\n",
      "2017-04-03T22:30:51.727188: step 6329, loss 0.0502164, acc 0.96875\n",
      "2017-04-03T22:30:51.923042: step 6330, loss 0.0499126, acc 0.984375\n",
      "2017-04-03T22:30:52.127771: step 6331, loss 0.00671237, acc 1\n",
      "2017-04-03T22:30:52.334985: step 6332, loss 0.0157752, acc 1\n",
      "2017-04-03T22:30:52.536849: step 6333, loss 0.0396175, acc 0.984375\n",
      "2017-04-03T22:30:52.747616: step 6334, loss 0.0607158, acc 0.96875\n",
      "2017-04-03T22:30:52.960709: step 6335, loss 0.0047389, acc 1\n",
      "2017-04-03T22:30:53.160836: step 6336, loss 0.04069, acc 0.984375\n",
      "2017-04-03T22:30:53.360766: step 6337, loss 0.132971, acc 0.96875\n",
      "2017-04-03T22:30:53.559691: step 6338, loss 0.0502252, acc 0.96875\n",
      "2017-04-03T22:30:53.752469: step 6339, loss 0.02402, acc 1\n",
      "2017-04-03T22:30:53.962646: step 6340, loss 0.0757766, acc 0.96875\n",
      "2017-04-03T22:30:54.151573: step 6341, loss 0.0385236, acc 0.984375\n",
      "2017-04-03T22:30:54.349897: step 6342, loss 0.0508191, acc 0.984375\n",
      "2017-04-03T22:30:54.547784: step 6343, loss 0.0272506, acc 0.984375\n",
      "2017-04-03T22:30:54.747202: step 6344, loss 0.0374094, acc 0.984375\n",
      "2017-04-03T22:30:54.940099: step 6345, loss 0.104678, acc 0.96875\n",
      "2017-04-03T22:30:55.136594: step 6346, loss 0.0245067, acc 1\n",
      "2017-04-03T22:30:55.339205: step 6347, loss 0.0618684, acc 0.984375\n",
      "2017-04-03T22:30:55.549682: step 6348, loss 0.0815404, acc 0.96875\n",
      "2017-04-03T22:30:55.764890: step 6349, loss 0.036006, acc 0.984375\n",
      "2017-04-03T22:30:55.980390: step 6350, loss 0.0702777, acc 0.96875\n",
      "2017-04-03T22:30:56.170814: step 6351, loss 0.0440859, acc 0.984375\n",
      "2017-04-03T22:30:56.370251: step 6352, loss 0.0490255, acc 1\n",
      "2017-04-03T22:30:56.566834: step 6353, loss 0.0360466, acc 0.984375\n",
      "2017-04-03T22:30:56.765942: step 6354, loss 0.0117446, acc 1\n",
      "2017-04-03T22:30:56.967652: step 6355, loss 0.100476, acc 0.96875\n",
      "2017-04-03T22:30:57.170989: step 6356, loss 0.0105276, acc 1\n",
      "2017-04-03T22:30:57.366183: step 6357, loss 0.0423036, acc 0.984375\n",
      "2017-04-03T22:30:57.567752: step 6358, loss 0.0730937, acc 0.96875\n",
      "2017-04-03T22:30:57.780392: step 6359, loss 0.0084923, acc 1\n",
      "2017-04-03T22:30:57.991126: step 6360, loss 0.0583201, acc 0.96875\n",
      "2017-04-03T22:30:58.203380: step 6361, loss 0.0872653, acc 0.953125\n",
      "2017-04-03T22:30:58.414585: step 6362, loss 0.015985, acc 1\n",
      "2017-04-03T22:30:58.630072: step 6363, loss 0.0638979, acc 0.984375\n",
      "2017-04-03T22:30:58.848541: step 6364, loss 0.0132098, acc 1\n",
      "2017-04-03T22:30:59.058806: step 6365, loss 0.00727012, acc 1\n",
      "2017-04-03T22:30:59.277488: step 6366, loss 0.0583384, acc 0.984375\n",
      "2017-04-03T22:30:59.491279: step 6367, loss 0.00480384, acc 1\n",
      "2017-04-03T22:30:59.701816: step 6368, loss 0.0399567, acc 0.984375\n",
      "2017-04-03T22:30:59.918534: step 6369, loss 0.0621159, acc 0.96875\n",
      "2017-04-03T22:31:00.132289: step 6370, loss 0.0308612, acc 0.984375\n",
      "2017-04-03T22:31:00.347982: step 6371, loss 0.0538644, acc 0.984375\n",
      "2017-04-03T22:31:00.534260: step 6372, loss 0.0580636, acc 0.980769\n",
      "2017-04-03T22:31:00.740520: step 6373, loss 0.0411846, acc 0.984375\n",
      "2017-04-03T22:31:00.955612: step 6374, loss 0.0824941, acc 0.96875\n",
      "2017-04-03T22:31:01.172602: step 6375, loss 0.0777579, acc 0.96875\n",
      "2017-04-03T22:31:01.390053: step 6376, loss 0.0050741, acc 1\n",
      "2017-04-03T22:31:01.585970: step 6377, loss 0.0350953, acc 0.984375\n",
      "2017-04-03T22:31:01.781681: step 6378, loss 0.00333105, acc 1\n",
      "2017-04-03T22:31:01.975484: step 6379, loss 0.0163739, acc 1\n",
      "2017-04-03T22:31:02.168451: step 6380, loss 0.0598444, acc 0.984375\n",
      "2017-04-03T22:31:02.356370: step 6381, loss 0.0282086, acc 1\n",
      "2017-04-03T22:31:02.567932: step 6382, loss 0.0687898, acc 0.96875\n",
      "2017-04-03T22:31:02.773642: step 6383, loss 0.00644407, acc 1\n",
      "2017-04-03T22:31:02.979953: step 6384, loss 0.0149933, acc 1\n",
      "2017-04-03T22:31:03.187179: step 6385, loss 0.036676, acc 0.984375\n",
      "2017-04-03T22:31:03.405637: step 6386, loss 0.037184, acc 0.984375\n",
      "2017-04-03T22:31:03.621410: step 6387, loss 0.0422106, acc 0.984375\n",
      "2017-04-03T22:31:03.833746: step 6388, loss 0.0257703, acc 1\n",
      "2017-04-03T22:31:04.045719: step 6389, loss 0.0228515, acc 1\n",
      "2017-04-03T22:31:04.263063: step 6390, loss 0.0620573, acc 0.984375\n",
      "2017-04-03T22:31:04.499173: step 6391, loss 0.0356856, acc 0.984375\n",
      "2017-04-03T22:31:04.710928: step 6392, loss 0.0345379, acc 0.984375\n",
      "2017-04-03T22:31:04.909484: step 6393, loss 0.0314456, acc 0.984375\n",
      "2017-04-03T22:31:05.118562: step 6394, loss 0.0374237, acc 1\n",
      "2017-04-03T22:31:05.312661: step 6395, loss 0.125532, acc 0.9375\n",
      "2017-04-03T22:31:05.508966: step 6396, loss 0.00668568, acc 1\n",
      "2017-04-03T22:31:05.705940: step 6397, loss 0.0194476, acc 1\n",
      "2017-04-03T22:31:05.905476: step 6398, loss 0.0538047, acc 0.984375\n",
      "2017-04-03T22:31:06.099593: step 6399, loss 0.0357616, acc 0.984375\n",
      "2017-04-03T22:31:06.297082: step 6400, loss 0.00587312, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:31:06.953501: step 6400, loss 2.2908, acc 0.569191\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6400\n",
      "\n",
      "2017-04-03T22:31:07.206182: step 6401, loss 0.0688853, acc 0.96875\n",
      "2017-04-03T22:31:07.395659: step 6402, loss 0.0123538, acc 1\n",
      "2017-04-03T22:31:07.588530: step 6403, loss 0.0496325, acc 0.96875\n",
      "2017-04-03T22:31:07.803770: step 6404, loss 0.108219, acc 0.953125\n",
      "2017-04-03T22:31:08.016480: step 6405, loss 0.0342267, acc 0.984375\n",
      "2017-04-03T22:31:08.223764: step 6406, loss 0.100024, acc 0.953125\n",
      "2017-04-03T22:31:08.421466: step 6407, loss 0.0482834, acc 0.96875\n",
      "2017-04-03T22:31:08.612461: step 6408, loss 0.0541163, acc 0.984375\n",
      "2017-04-03T22:31:08.815541: step 6409, loss 0.0736071, acc 0.96875\n",
      "2017-04-03T22:31:09.011182: step 6410, loss 0.00454408, acc 1\n",
      "2017-04-03T22:31:09.216496: step 6411, loss 0.105104, acc 0.96875\n",
      "2017-04-03T22:31:09.432305: step 6412, loss 0.0434233, acc 0.984375\n",
      "2017-04-03T22:31:09.625882: step 6413, loss 0.0188351, acc 1\n",
      "2017-04-03T22:31:09.821282: step 6414, loss 0.0112745, acc 1\n",
      "2017-04-03T22:31:10.029270: step 6415, loss 0.0469104, acc 0.984375\n",
      "2017-04-03T22:31:10.245504: step 6416, loss 0.0630535, acc 0.96875\n",
      "2017-04-03T22:31:10.465157: step 6417, loss 0.0386768, acc 0.984375\n",
      "2017-04-03T22:31:10.664959: step 6418, loss 0.0282825, acc 1\n",
      "2017-04-03T22:31:10.864367: step 6419, loss 0.0757689, acc 0.953125\n",
      "2017-04-03T22:31:11.061435: step 6420, loss 0.00643915, acc 1\n",
      "2017-04-03T22:31:11.278634: step 6421, loss 0.0609626, acc 0.96875\n",
      "2017-04-03T22:31:11.470745: step 6422, loss 0.00338929, acc 1\n",
      "2017-04-03T22:31:11.666256: step 6423, loss 0.0524066, acc 0.96875\n",
      "2017-04-03T22:31:11.875167: step 6424, loss 0.0350881, acc 0.984375\n",
      "2017-04-03T22:31:12.088745: step 6425, loss 0.010099, acc 1\n",
      "2017-04-03T22:31:12.309092: step 6426, loss 0.00226289, acc 1\n",
      "2017-04-03T22:31:12.522936: step 6427, loss 0.0627534, acc 0.984375\n",
      "2017-04-03T22:31:12.742297: step 6428, loss 0.0350004, acc 0.984375\n",
      "2017-04-03T22:31:12.936328: step 6429, loss 0.0557386, acc 0.984375\n",
      "2017-04-03T22:31:13.141745: step 6430, loss 0.0394525, acc 0.984375\n",
      "2017-04-03T22:31:13.336633: step 6431, loss 0.132904, acc 0.953125\n",
      "2017-04-03T22:31:13.535640: step 6432, loss 0.0319111, acc 1\n",
      "2017-04-03T22:31:13.755584: step 6433, loss 0.0595256, acc 0.984375\n",
      "2017-04-03T22:31:13.972186: step 6434, loss 0.0312826, acc 0.984375\n",
      "2017-04-03T22:31:14.165789: step 6435, loss 0.0103179, acc 1\n",
      "2017-04-03T22:31:14.366070: step 6436, loss 0.0709918, acc 0.96875\n",
      "2017-04-03T22:31:14.562323: step 6437, loss 0.0292569, acc 1\n",
      "2017-04-03T22:31:14.765542: step 6438, loss 0.0352258, acc 1\n",
      "2017-04-03T22:31:14.958318: step 6439, loss 0.0386799, acc 0.984375\n",
      "2017-04-03T22:31:15.156927: step 6440, loss 0.0301529, acc 0.984375\n",
      "2017-04-03T22:31:15.357871: step 6441, loss 0.0438998, acc 0.984375\n",
      "2017-04-03T22:31:15.557895: step 6442, loss 0.0642015, acc 0.96875\n",
      "2017-04-03T22:31:15.766732: step 6443, loss 0.00622182, acc 1\n",
      "2017-04-03T22:31:15.958891: step 6444, loss 0.0945258, acc 0.953125\n",
      "2017-04-03T22:31:16.155201: step 6445, loss 0.0927178, acc 0.96875\n",
      "2017-04-03T22:31:16.345500: step 6446, loss 0.0179173, acc 1\n",
      "2017-04-03T22:31:16.543312: step 6447, loss 0.0389385, acc 0.984375\n",
      "2017-04-03T22:31:16.741461: step 6448, loss 0.0743991, acc 0.953125\n",
      "2017-04-03T22:31:16.946354: step 6449, loss 0.0118203, acc 1\n",
      "2017-04-03T22:31:17.152304: step 6450, loss 0.193393, acc 0.9375\n",
      "2017-04-03T22:31:17.342709: step 6451, loss 0.0112076, acc 1\n",
      "2017-04-03T22:31:17.536087: step 6452, loss 0.0740683, acc 0.984375\n",
      "2017-04-03T22:31:17.735760: step 6453, loss 0.0120433, acc 1\n",
      "2017-04-03T22:31:17.954879: step 6454, loss 0.133687, acc 0.9375\n",
      "2017-04-03T22:31:18.150602: step 6455, loss 0.00901628, acc 1\n",
      "2017-04-03T22:31:18.352056: step 6456, loss 0.033683, acc 0.984375\n",
      "2017-04-03T22:31:18.543242: step 6457, loss 0.0766044, acc 0.96875\n",
      "2017-04-03T22:31:18.740562: step 6458, loss 0.0692309, acc 0.984375\n",
      "2017-04-03T22:31:18.944717: step 6459, loss 0.0187679, acc 1\n",
      "2017-04-03T22:31:19.134488: step 6460, loss 0.0225366, acc 0.984375\n",
      "2017-04-03T22:31:19.348320: step 6461, loss 0.00528461, acc 1\n",
      "2017-04-03T22:31:19.541599: step 6462, loss 0.036495, acc 0.984375\n",
      "2017-04-03T22:31:19.734301: step 6463, loss 0.115391, acc 0.9375\n",
      "2017-04-03T22:31:19.931245: step 6464, loss 0.0602939, acc 0.96875\n",
      "2017-04-03T22:31:20.140881: step 6465, loss 0.0332277, acc 1\n",
      "2017-04-03T22:31:20.345475: step 6466, loss 0.0153333, acc 1\n",
      "2017-04-03T22:31:20.546453: step 6467, loss 0.00777709, acc 1\n",
      "2017-04-03T22:31:20.748980: step 6468, loss 0.0741915, acc 0.984375\n",
      "2017-04-03T22:31:20.943414: step 6469, loss 0.0136878, acc 1\n",
      "2017-04-03T22:31:21.132245: step 6470, loss 0.0557613, acc 0.96875\n",
      "2017-04-03T22:31:21.323325: step 6471, loss 0.0249048, acc 0.984375\n",
      "2017-04-03T22:31:21.522004: step 6472, loss 0.0135346, acc 1\n",
      "2017-04-03T22:31:21.732024: step 6473, loss 0.0337784, acc 0.984375\n",
      "2017-04-03T22:31:21.935233: step 6474, loss 0.0364147, acc 1\n",
      "2017-04-03T22:31:22.128066: step 6475, loss 0.0168641, acc 1\n",
      "2017-04-03T22:31:22.327474: step 6476, loss 0.021678, acc 0.984375\n",
      "2017-04-03T22:31:22.521158: step 6477, loss 0.0745655, acc 0.96875\n",
      "2017-04-03T22:31:22.715865: step 6478, loss 0.0158029, acc 1\n",
      "2017-04-03T22:31:22.923544: step 6479, loss 0.0457143, acc 0.984375\n",
      "2017-04-03T22:31:23.103201: step 6480, loss 0.0690836, acc 1\n",
      "2017-04-03T22:31:23.309315: step 6481, loss 0.0434566, acc 0.984375\n",
      "2017-04-03T22:31:23.506280: step 6482, loss 0.0795745, acc 0.96875\n",
      "2017-04-03T22:31:23.705987: step 6483, loss 0.0662976, acc 0.96875\n",
      "2017-04-03T22:31:23.913554: step 6484, loss 0.0666072, acc 0.96875\n",
      "2017-04-03T22:31:24.124293: step 6485, loss 0.00301682, acc 1\n",
      "2017-04-03T22:31:24.345382: step 6486, loss 0.143971, acc 0.921875\n",
      "2017-04-03T22:31:24.558917: step 6487, loss 0.00872618, acc 1\n",
      "2017-04-03T22:31:24.786462: step 6488, loss 0.0343372, acc 0.984375\n",
      "2017-04-03T22:31:24.998284: step 6489, loss 0.00967246, acc 1\n",
      "2017-04-03T22:31:25.209280: step 6490, loss 0.0245458, acc 0.984375\n",
      "2017-04-03T22:31:25.419495: step 6491, loss 0.07559, acc 0.96875\n",
      "2017-04-03T22:31:25.635813: step 6492, loss 0.0928207, acc 0.96875\n",
      "2017-04-03T22:31:25.852980: step 6493, loss 0.086482, acc 0.984375\n",
      "2017-04-03T22:31:26.067403: step 6494, loss 0.0109245, acc 1\n",
      "2017-04-03T22:31:26.278572: step 6495, loss 0.0606445, acc 0.984375\n",
      "2017-04-03T22:31:26.486564: step 6496, loss 0.0340829, acc 0.984375\n",
      "2017-04-03T22:31:26.702732: step 6497, loss 0.00977427, acc 1\n",
      "2017-04-03T22:31:26.923544: step 6498, loss 0.238636, acc 0.96875\n",
      "2017-04-03T22:31:27.143019: step 6499, loss 0.018221, acc 1\n",
      "2017-04-03T22:31:27.358449: step 6500, loss 0.0401627, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:31:28.081953: step 6500, loss 2.33671, acc 0.577024\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6500\n",
      "\n",
      "2017-04-03T22:31:28.362006: step 6501, loss 0.0180026, acc 1\n",
      "2017-04-03T22:31:28.569104: step 6502, loss 0.0380531, acc 1\n",
      "2017-04-03T22:31:28.778237: step 6503, loss 0.0408904, acc 0.984375\n",
      "2017-04-03T22:31:28.992615: step 6504, loss 0.0193287, acc 0.984375\n",
      "2017-04-03T22:31:29.202187: step 6505, loss 0.0157715, acc 1\n",
      "2017-04-03T22:31:29.412100: step 6506, loss 0.0442988, acc 0.984375\n",
      "2017-04-03T22:31:29.629975: step 6507, loss 0.0365514, acc 0.984375\n",
      "2017-04-03T22:31:29.840187: step 6508, loss 0.0106384, acc 1\n",
      "2017-04-03T22:31:30.049129: step 6509, loss 0.0379094, acc 0.984375\n",
      "2017-04-03T22:31:30.258237: step 6510, loss 0.0607479, acc 0.96875\n",
      "2017-04-03T22:31:30.470945: step 6511, loss 0.0383606, acc 0.984375\n",
      "2017-04-03T22:31:30.684765: step 6512, loss 0.112954, acc 0.9375\n",
      "2017-04-03T22:31:30.903563: step 6513, loss 0.105979, acc 0.96875\n",
      "2017-04-03T22:31:31.116315: step 6514, loss 0.0570235, acc 0.984375\n",
      "2017-04-03T22:31:31.331917: step 6515, loss 0.038181, acc 0.984375\n",
      "2017-04-03T22:31:31.548232: step 6516, loss 0.028326, acc 1\n",
      "2017-04-03T22:31:31.764310: step 6517, loss 0.0422711, acc 0.984375\n",
      "2017-04-03T22:31:31.975142: step 6518, loss 0.0375424, acc 0.984375\n",
      "2017-04-03T22:31:32.200455: step 6519, loss 0.108379, acc 0.953125\n",
      "2017-04-03T22:31:32.409091: step 6520, loss 0.043047, acc 1\n",
      "2017-04-03T22:31:32.617336: step 6521, loss 0.0920642, acc 0.953125\n",
      "2017-04-03T22:31:32.841354: step 6522, loss 0.096668, acc 0.96875\n",
      "2017-04-03T22:31:33.057235: step 6523, loss 0.00345604, acc 1\n",
      "2017-04-03T22:31:33.267733: step 6524, loss 0.0473537, acc 0.984375\n",
      "2017-04-03T22:31:33.481325: step 6525, loss 0.0327397, acc 0.984375\n",
      "2017-04-03T22:31:33.691470: step 6526, loss 0.0137892, acc 1\n",
      "2017-04-03T22:31:33.908317: step 6527, loss 0.0309965, acc 0.984375\n",
      "2017-04-03T22:31:34.123386: step 6528, loss 0.0386995, acc 0.984375\n",
      "2017-04-03T22:31:34.332558: step 6529, loss 0.0282439, acc 1\n",
      "2017-04-03T22:31:34.547759: step 6530, loss 0.0512171, acc 0.96875\n",
      "2017-04-03T22:31:34.774229: step 6531, loss 0.0657807, acc 0.96875\n",
      "2017-04-03T22:31:34.986637: step 6532, loss 0.0317913, acc 0.984375\n",
      "2017-04-03T22:31:35.195996: step 6533, loss 0.0524374, acc 0.984375\n",
      "2017-04-03T22:31:35.406702: step 6534, loss 0.0682984, acc 0.96875\n",
      "2017-04-03T22:31:35.624765: step 6535, loss 0.0541643, acc 0.953125\n",
      "2017-04-03T22:31:35.837927: step 6536, loss 0.0864603, acc 0.953125\n",
      "2017-04-03T22:31:36.057764: step 6537, loss 0.0838188, acc 0.984375\n",
      "2017-04-03T22:31:36.275523: step 6538, loss 0.0104462, acc 1\n",
      "2017-04-03T22:31:36.489530: step 6539, loss 0.0939732, acc 0.953125\n",
      "2017-04-03T22:31:36.700170: step 6540, loss 0.0103708, acc 1\n",
      "2017-04-03T22:31:36.914683: step 6541, loss 0.0513067, acc 0.984375\n",
      "2017-04-03T22:31:37.125040: step 6542, loss 0.0313184, acc 0.984375\n",
      "2017-04-03T22:31:37.351858: step 6543, loss 0.0467895, acc 0.984375\n",
      "2017-04-03T22:31:37.558793: step 6544, loss 0.0978157, acc 0.984375\n",
      "2017-04-03T22:31:37.767473: step 6545, loss 0.0616302, acc 0.984375\n",
      "2017-04-03T22:31:37.975920: step 6546, loss 0.0614753, acc 0.984375\n",
      "2017-04-03T22:31:38.185562: step 6547, loss 0.0117402, acc 1\n",
      "2017-04-03T22:31:38.393854: step 6548, loss 0.0910926, acc 0.984375\n",
      "2017-04-03T22:31:38.602042: step 6549, loss 0.0618366, acc 0.984375\n",
      "2017-04-03T22:31:38.816251: step 6550, loss 0.0174268, acc 1\n",
      "2017-04-03T22:31:39.027564: step 6551, loss 0.0709961, acc 0.984375\n",
      "2017-04-03T22:31:39.235230: step 6552, loss 0.0664076, acc 0.96875\n",
      "2017-04-03T22:31:39.447298: step 6553, loss 0.0527579, acc 0.984375\n",
      "2017-04-03T22:31:39.660720: step 6554, loss 0.0242039, acc 0.984375\n",
      "2017-04-03T22:31:39.873500: step 6555, loss 0.0247105, acc 1\n",
      "2017-04-03T22:31:40.090780: step 6556, loss 0.0469447, acc 0.984375\n",
      "2017-04-03T22:31:40.305580: step 6557, loss 0.0886055, acc 0.96875\n",
      "2017-04-03T22:31:40.517891: step 6558, loss 0.0392468, acc 0.984375\n",
      "2017-04-03T22:31:40.739579: step 6559, loss 0.0671726, acc 0.96875\n",
      "2017-04-03T22:31:40.948640: step 6560, loss 0.0742406, acc 0.96875\n",
      "2017-04-03T22:31:41.171473: step 6561, loss 0.0340095, acc 0.984375\n",
      "2017-04-03T22:31:41.382718: step 6562, loss 0.0846271, acc 0.96875\n",
      "2017-04-03T22:31:41.598066: step 6563, loss 0.0351817, acc 0.984375\n",
      "2017-04-03T22:31:41.813301: step 6564, loss 0.0333671, acc 0.984375\n",
      "2017-04-03T22:31:42.036351: step 6565, loss 0.107436, acc 0.953125\n",
      "2017-04-03T22:31:42.259206: step 6566, loss 0.010498, acc 1\n",
      "2017-04-03T22:31:42.473242: step 6567, loss 0.0324109, acc 1\n",
      "2017-04-03T22:31:42.687681: step 6568, loss 0.0334194, acc 0.984375\n",
      "2017-04-03T22:31:42.896573: step 6569, loss 0.0387306, acc 0.984375\n",
      "2017-04-03T22:31:43.117094: step 6570, loss 0.018559, acc 1\n",
      "2017-04-03T22:31:43.318143: step 6571, loss 0.0346841, acc 1\n",
      "2017-04-03T22:31:43.515429: step 6572, loss 0.0215557, acc 0.984375\n",
      "2017-04-03T22:31:43.721035: step 6573, loss 0.00485444, acc 1\n",
      "2017-04-03T22:31:43.918449: step 6574, loss 0.0910785, acc 0.953125\n",
      "2017-04-03T22:31:44.148575: step 6575, loss 0.0456119, acc 0.96875\n",
      "2017-04-03T22:31:44.343397: step 6576, loss 0.0849131, acc 0.984375\n",
      "2017-04-03T22:31:44.544902: step 6577, loss 0.0341717, acc 0.984375\n",
      "2017-04-03T22:31:44.761204: step 6578, loss 0.0147742, acc 1\n",
      "2017-04-03T22:31:44.971124: step 6579, loss 0.0638046, acc 0.984375\n",
      "2017-04-03T22:31:45.183411: step 6580, loss 0.0199842, acc 1\n",
      "2017-04-03T22:31:45.407426: step 6581, loss 0.0848374, acc 0.96875\n",
      "2017-04-03T22:31:45.627932: step 6582, loss 0.0479002, acc 0.984375\n",
      "2017-04-03T22:31:45.839503: step 6583, loss 0.0356035, acc 0.984375\n",
      "2017-04-03T22:31:46.051889: step 6584, loss 0.0125326, acc 1\n",
      "2017-04-03T22:31:46.250858: step 6585, loss 0.0701345, acc 1\n",
      "2017-04-03T22:31:46.458152: step 6586, loss 0.0447134, acc 0.984375\n",
      "2017-04-03T22:31:46.675589: step 6587, loss 0.00374445, acc 1\n",
      "2017-04-03T22:31:46.858793: step 6588, loss 0.0992279, acc 0.961538\n",
      "2017-04-03T22:31:47.065646: step 6589, loss 0.0497084, acc 0.984375\n",
      "2017-04-03T22:31:47.278527: step 6590, loss 0.0265607, acc 1\n",
      "2017-04-03T22:31:47.494539: step 6591, loss 0.0306654, acc 0.984375\n",
      "2017-04-03T22:31:47.724441: step 6592, loss 0.080146, acc 0.984375\n",
      "2017-04-03T22:31:47.936887: step 6593, loss 0.0276428, acc 0.984375\n",
      "2017-04-03T22:31:48.147297: step 6594, loss 0.062846, acc 0.96875\n",
      "2017-04-03T22:31:48.357667: step 6595, loss 0.00291928, acc 1\n",
      "2017-04-03T22:31:48.571451: step 6596, loss 0.0671255, acc 0.96875\n",
      "2017-04-03T22:31:48.778310: step 6597, loss 0.00815888, acc 1\n",
      "2017-04-03T22:31:48.993610: step 6598, loss 0.149651, acc 0.921875\n",
      "2017-04-03T22:31:49.211995: step 6599, loss 0.0436636, acc 0.96875\n",
      "2017-04-03T22:31:49.422405: step 6600, loss 0.0436914, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:31:50.183786: step 6600, loss 2.36272, acc 0.565274\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6600\n",
      "\n",
      "2017-04-03T22:31:50.451837: step 6601, loss 0.0467753, acc 0.984375\n",
      "2017-04-03T22:31:50.670705: step 6602, loss 0.0335087, acc 0.984375\n",
      "2017-04-03T22:31:50.876374: step 6603, loss 0.0607402, acc 0.984375\n",
      "2017-04-03T22:31:51.095582: step 6604, loss 0.0471652, acc 0.984375\n",
      "2017-04-03T22:31:51.300398: step 6605, loss 0.00815479, acc 1\n",
      "2017-04-03T22:31:51.513123: step 6606, loss 0.117832, acc 0.9375\n",
      "2017-04-03T22:31:51.734858: step 6607, loss 0.062204, acc 0.96875\n",
      "2017-04-03T22:31:51.949793: step 6608, loss 0.0347875, acc 0.96875\n",
      "2017-04-03T22:31:52.171918: step 6609, loss 0.0820803, acc 0.96875\n",
      "2017-04-03T22:31:52.396005: step 6610, loss 0.0535111, acc 0.984375\n",
      "2017-04-03T22:31:52.602883: step 6611, loss 0.0328004, acc 0.984375\n",
      "2017-04-03T22:31:52.812165: step 6612, loss 0.0125794, acc 1\n",
      "2017-04-03T22:31:53.020799: step 6613, loss 0.0299478, acc 1\n",
      "2017-04-03T22:31:53.242901: step 6614, loss 0.0256511, acc 0.984375\n",
      "2017-04-03T22:31:53.462359: step 6615, loss 0.0222532, acc 0.984375\n",
      "2017-04-03T22:31:53.669652: step 6616, loss 0.0308776, acc 1\n",
      "2017-04-03T22:31:53.883672: step 6617, loss 0.050752, acc 0.984375\n",
      "2017-04-03T22:31:54.089810: step 6618, loss 0.038671, acc 0.984375\n",
      "2017-04-03T22:31:54.299324: step 6619, loss 0.0227196, acc 0.984375\n",
      "2017-04-03T22:31:54.505941: step 6620, loss 0.0216358, acc 1\n",
      "2017-04-03T22:31:54.722689: step 6621, loss 0.0691646, acc 0.96875\n",
      "2017-04-03T22:31:54.932535: step 6622, loss 0.0360599, acc 0.984375\n",
      "2017-04-03T22:31:55.143298: step 6623, loss 0.00984103, acc 1\n",
      "2017-04-03T22:31:55.351279: step 6624, loss 0.0529919, acc 0.984375\n",
      "2017-04-03T22:31:55.558989: step 6625, loss 0.0168714, acc 1\n",
      "2017-04-03T22:31:55.767018: step 6626, loss 0.0423575, acc 0.984375\n",
      "2017-04-03T22:31:55.971995: step 6627, loss 0.130919, acc 0.96875\n",
      "2017-04-03T22:31:56.180759: step 6628, loss 0.0147208, acc 1\n",
      "2017-04-03T22:31:56.396476: step 6629, loss 0.0467487, acc 0.96875\n",
      "2017-04-03T22:31:56.605929: step 6630, loss 0.0401266, acc 0.984375\n",
      "2017-04-03T22:31:56.827261: step 6631, loss 0.0446525, acc 0.984375\n",
      "2017-04-03T22:31:57.036015: step 6632, loss 0.205033, acc 0.953125\n",
      "2017-04-03T22:31:57.243723: step 6633, loss 0.0279364, acc 1\n",
      "2017-04-03T22:31:57.464519: step 6634, loss 0.0110801, acc 1\n",
      "2017-04-03T22:31:57.677291: step 6635, loss 0.0333897, acc 1\n",
      "2017-04-03T22:31:57.881402: step 6636, loss 0.0854353, acc 0.96875\n",
      "2017-04-03T22:31:58.090114: step 6637, loss 0.0693288, acc 0.953125\n",
      "2017-04-03T22:31:58.302149: step 6638, loss 0.0162123, acc 1\n",
      "2017-04-03T22:31:58.515501: step 6639, loss 0.0205348, acc 1\n",
      "2017-04-03T22:31:58.734544: step 6640, loss 0.0151009, acc 1\n",
      "2017-04-03T22:31:58.955770: step 6641, loss 0.106605, acc 0.984375\n",
      "2017-04-03T22:31:59.164867: step 6642, loss 0.172443, acc 0.984375\n",
      "2017-04-03T22:31:59.374792: step 6643, loss 0.0432881, acc 0.984375\n",
      "2017-04-03T22:31:59.599636: step 6644, loss 0.0114045, acc 1\n",
      "2017-04-03T22:31:59.816863: step 6645, loss 0.0644069, acc 0.96875\n",
      "2017-04-03T22:32:00.029683: step 6646, loss 0.143642, acc 0.953125\n",
      "2017-04-03T22:32:00.243869: step 6647, loss 0.063789, acc 0.96875\n",
      "2017-04-03T22:32:00.461228: step 6648, loss 0.0451282, acc 0.984375\n",
      "2017-04-03T22:32:00.679562: step 6649, loss 0.0213695, acc 0.984375\n",
      "2017-04-03T22:32:00.890419: step 6650, loss 0.0201862, acc 1\n",
      "2017-04-03T22:32:01.108913: step 6651, loss 0.0643924, acc 0.984375\n",
      "2017-04-03T22:32:01.320287: step 6652, loss 0.0385845, acc 0.984375\n",
      "2017-04-03T22:32:01.533446: step 6653, loss 0.0153681, acc 1\n",
      "2017-04-03T22:32:01.756397: step 6654, loss 0.0911207, acc 0.953125\n",
      "2017-04-03T22:32:01.969849: step 6655, loss 0.260979, acc 0.984375\n",
      "2017-04-03T22:32:02.177649: step 6656, loss 0.00525655, acc 1\n",
      "2017-04-03T22:32:02.385409: step 6657, loss 0.0116546, acc 1\n",
      "2017-04-03T22:32:02.598153: step 6658, loss 0.0078166, acc 1\n",
      "2017-04-03T22:32:02.820021: step 6659, loss 0.0834872, acc 0.953125\n",
      "2017-04-03T22:32:03.036174: step 6660, loss 0.0816716, acc 0.984375\n",
      "2017-04-03T22:32:03.248053: step 6661, loss 0.0651692, acc 0.96875\n",
      "2017-04-03T22:32:03.460162: step 6662, loss 0.0260437, acc 0.984375\n",
      "2017-04-03T22:32:03.673206: step 6663, loss 0.00563911, acc 1\n",
      "2017-04-03T22:32:03.885687: step 6664, loss 0.0677216, acc 0.96875\n",
      "2017-04-03T22:32:04.097568: step 6665, loss 0.0415687, acc 0.984375\n",
      "2017-04-03T22:32:04.307665: step 6666, loss 0.0202814, acc 1\n",
      "2017-04-03T22:32:04.528820: step 6667, loss 0.0342681, acc 0.984375\n",
      "2017-04-03T22:32:04.742245: step 6668, loss 0.0606473, acc 0.96875\n",
      "2017-04-03T22:32:04.967977: step 6669, loss 0.0331405, acc 0.984375\n",
      "2017-04-03T22:32:05.177219: step 6670, loss 0.0596844, acc 0.984375\n",
      "2017-04-03T22:32:05.397992: step 6671, loss 0.126588, acc 0.9375\n",
      "2017-04-03T22:32:05.607687: step 6672, loss 0.039351, acc 0.984375\n",
      "2017-04-03T22:32:05.816521: step 6673, loss 0.0148175, acc 1\n",
      "2017-04-03T22:32:06.031487: step 6674, loss 0.0601008, acc 0.984375\n",
      "2017-04-03T22:32:06.250940: step 6675, loss 0.0482928, acc 0.984375\n",
      "2017-04-03T22:32:06.459301: step 6676, loss 0.147805, acc 0.9375\n",
      "2017-04-03T22:32:06.671878: step 6677, loss 0.0849726, acc 0.96875\n",
      "2017-04-03T22:32:06.884888: step 6678, loss 0.00602775, acc 1\n",
      "2017-04-03T22:32:07.097040: step 6679, loss 0.0459777, acc 0.984375\n",
      "2017-04-03T22:32:07.312475: step 6680, loss 0.0179193, acc 1\n",
      "2017-04-03T22:32:07.528535: step 6681, loss 0.0311709, acc 0.984375\n",
      "2017-04-03T22:32:07.739309: step 6682, loss 0.030616, acc 0.984375\n",
      "2017-04-03T22:32:07.945770: step 6683, loss 0.0525203, acc 0.96875\n",
      "2017-04-03T22:32:08.167672: step 6684, loss 0.0706301, acc 0.984375\n",
      "2017-04-03T22:32:08.371288: step 6685, loss 0.0347763, acc 0.984375\n",
      "2017-04-03T22:32:08.584741: step 6686, loss 0.0309781, acc 1\n",
      "2017-04-03T22:32:08.793377: step 6687, loss 0.0137054, acc 1\n",
      "2017-04-03T22:32:09.006375: step 6688, loss 0.0807363, acc 0.96875\n",
      "2017-04-03T22:32:09.223013: step 6689, loss 0.00813077, acc 1\n",
      "2017-04-03T22:32:09.444739: step 6690, loss 0.022031, acc 1\n",
      "2017-04-03T22:32:09.660595: step 6691, loss 0.0597429, acc 0.984375\n",
      "2017-04-03T22:32:09.868793: step 6692, loss 0.0415837, acc 1\n",
      "2017-04-03T22:32:10.081619: step 6693, loss 0.0073087, acc 1\n",
      "2017-04-03T22:32:10.289305: step 6694, loss 0.010458, acc 1\n",
      "2017-04-03T22:32:10.501232: step 6695, loss 0.0608786, acc 0.96875\n",
      "2017-04-03T22:32:10.679577: step 6696, loss 0.0931384, acc 0.961538\n",
      "2017-04-03T22:32:10.900946: step 6697, loss 0.0933115, acc 0.984375\n",
      "2017-04-03T22:32:11.110970: step 6698, loss 0.0430517, acc 1\n",
      "2017-04-03T22:32:11.331439: step 6699, loss 0.0147601, acc 1\n",
      "2017-04-03T22:32:11.553333: step 6700, loss 0.0223216, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:32:12.311253: step 6700, loss 2.39846, acc 0.55483\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6700\n",
      "\n",
      "2017-04-03T22:32:12.574809: step 6701, loss 0.0597962, acc 0.984375\n",
      "2017-04-03T22:32:12.781967: step 6702, loss 0.00848127, acc 1\n",
      "2017-04-03T22:32:12.994685: step 6703, loss 0.192732, acc 0.921875\n",
      "2017-04-03T22:32:13.204214: step 6704, loss 0.0509445, acc 0.984375\n",
      "2017-04-03T22:32:13.410274: step 6705, loss 0.0269731, acc 0.984375\n",
      "2017-04-03T22:32:13.620071: step 6706, loss 0.00152601, acc 1\n",
      "2017-04-03T22:32:13.835058: step 6707, loss 0.0897181, acc 0.96875\n",
      "2017-04-03T22:32:14.043690: step 6708, loss 0.0879221, acc 0.953125\n",
      "2017-04-03T22:32:14.253421: step 6709, loss 0.0267144, acc 0.984375\n",
      "2017-04-03T22:32:14.464625: step 6710, loss 0.0582238, acc 0.96875\n",
      "2017-04-03T22:32:14.674785: step 6711, loss 0.0112855, acc 1\n",
      "2017-04-03T22:32:14.891182: step 6712, loss 0.0133802, acc 1\n",
      "2017-04-03T22:32:15.107768: step 6713, loss 0.0810378, acc 0.984375\n",
      "2017-04-03T22:32:15.321608: step 6714, loss 0.00389238, acc 1\n",
      "2017-04-03T22:32:15.538839: step 6715, loss 0.0183871, acc 0.984375\n",
      "2017-04-03T22:32:15.750475: step 6716, loss 0.0125652, acc 1\n",
      "2017-04-03T22:32:15.961595: step 6717, loss 0.0178514, acc 1\n",
      "2017-04-03T22:32:16.172860: step 6718, loss 0.0216153, acc 1\n",
      "2017-04-03T22:32:16.390430: step 6719, loss 0.036539, acc 0.984375\n",
      "2017-04-03T22:32:16.601603: step 6720, loss 0.0376974, acc 0.984375\n",
      "2017-04-03T22:32:16.821392: step 6721, loss 0.0282892, acc 1\n",
      "2017-04-03T22:32:17.031445: step 6722, loss 0.0360456, acc 0.984375\n",
      "2017-04-03T22:32:17.249095: step 6723, loss 0.0982393, acc 0.96875\n",
      "2017-04-03T22:32:17.459015: step 6724, loss 0.0402071, acc 0.984375\n",
      "2017-04-03T22:32:17.682229: step 6725, loss 0.0370739, acc 0.984375\n",
      "2017-04-03T22:32:17.905883: step 6726, loss 0.0346041, acc 0.984375\n",
      "2017-04-03T22:32:18.114635: step 6727, loss 0.019483, acc 1\n",
      "2017-04-03T22:32:18.323818: step 6728, loss 0.0124783, acc 1\n",
      "2017-04-03T22:32:18.530322: step 6729, loss 0.0315965, acc 1\n",
      "2017-04-03T22:32:18.734693: step 6730, loss 0.0601813, acc 1\n",
      "2017-04-03T22:32:18.943062: step 6731, loss 0.0120403, acc 1\n",
      "2017-04-03T22:32:19.150692: step 6732, loss 0.0314433, acc 0.984375\n",
      "2017-04-03T22:32:19.378785: step 6733, loss 0.0975331, acc 0.953125\n",
      "2017-04-03T22:32:19.590882: step 6734, loss 0.0471492, acc 0.984375\n",
      "2017-04-03T22:32:19.796618: step 6735, loss 0.0147995, acc 1\n",
      "2017-04-03T22:32:20.011908: step 6736, loss 0.0295591, acc 1\n",
      "2017-04-03T22:32:20.219687: step 6737, loss 0.0147452, acc 1\n",
      "2017-04-03T22:32:20.429928: step 6738, loss 0.0475285, acc 1\n",
      "2017-04-03T22:32:20.641473: step 6739, loss 0.00850702, acc 1\n",
      "2017-04-03T22:32:20.860265: step 6740, loss 0.00736542, acc 1\n",
      "2017-04-03T22:32:21.073134: step 6741, loss 0.0821362, acc 0.96875\n",
      "2017-04-03T22:32:21.290564: step 6742, loss 0.0824716, acc 0.96875\n",
      "2017-04-03T22:32:21.504766: step 6743, loss 0.0120714, acc 1\n",
      "2017-04-03T22:32:21.718652: step 6744, loss 0.0743259, acc 0.96875\n",
      "2017-04-03T22:32:21.925979: step 6745, loss 0.00595121, acc 1\n",
      "2017-04-03T22:32:22.135544: step 6746, loss 0.00425324, acc 1\n",
      "2017-04-03T22:32:22.344339: step 6747, loss 0.00663957, acc 1\n",
      "2017-04-03T22:32:22.561270: step 6748, loss 0.0599822, acc 0.96875\n",
      "2017-04-03T22:32:22.773563: step 6749, loss 0.0781465, acc 0.96875\n",
      "2017-04-03T22:32:22.988190: step 6750, loss 0.191643, acc 0.9375\n",
      "2017-04-03T22:32:23.197102: step 6751, loss 0.0538043, acc 0.96875\n",
      "2017-04-03T22:32:23.415609: step 6752, loss 0.0383985, acc 1\n",
      "2017-04-03T22:32:23.627857: step 6753, loss 0.0079852, acc 1\n",
      "2017-04-03T22:32:23.838291: step 6754, loss 0.0172127, acc 1\n",
      "2017-04-03T22:32:24.045395: step 6755, loss 0.00310765, acc 1\n",
      "2017-04-03T22:32:24.254240: step 6756, loss 0.0428254, acc 0.984375\n",
      "2017-04-03T22:32:24.473097: step 6757, loss 0.0309559, acc 1\n",
      "2017-04-03T22:32:24.687890: step 6758, loss 0.046154, acc 0.984375\n",
      "2017-04-03T22:32:24.897656: step 6759, loss 0.0184047, acc 1\n",
      "2017-04-03T22:32:25.107479: step 6760, loss 0.00581136, acc 1\n",
      "2017-04-03T22:32:25.313892: step 6761, loss 0.0667148, acc 0.984375\n",
      "2017-04-03T22:32:25.532144: step 6762, loss 0.0798128, acc 0.96875\n",
      "2017-04-03T22:32:25.757438: step 6763, loss 0.0237942, acc 1\n",
      "2017-04-03T22:32:25.984556: step 6764, loss 0.00573919, acc 1\n",
      "2017-04-03T22:32:26.201155: step 6765, loss 0.111975, acc 0.96875\n",
      "2017-04-03T22:32:26.428648: step 6766, loss 0.0159384, acc 1\n",
      "2017-04-03T22:32:26.646389: step 6767, loss 0.0166093, acc 1\n",
      "2017-04-03T22:32:26.855071: step 6768, loss 0.090477, acc 0.984375\n",
      "2017-04-03T22:32:27.074658: step 6769, loss 0.0850456, acc 0.96875\n",
      "2017-04-03T22:32:27.285953: step 6770, loss 0.00388696, acc 1\n",
      "2017-04-03T22:32:27.499633: step 6771, loss 0.037746, acc 0.984375\n",
      "2017-04-03T22:32:27.712881: step 6772, loss 0.0858343, acc 0.96875\n",
      "2017-04-03T22:32:27.929784: step 6773, loss 0.0828246, acc 0.953125\n",
      "2017-04-03T22:32:28.137656: step 6774, loss 0.145714, acc 0.953125\n",
      "2017-04-03T22:32:28.352594: step 6775, loss 0.112845, acc 0.96875\n",
      "2017-04-03T22:32:28.564061: step 6776, loss 0.0332244, acc 0.984375\n",
      "2017-04-03T22:32:28.771290: step 6777, loss 0.0705782, acc 0.96875\n",
      "2017-04-03T22:32:28.978704: step 6778, loss 0.0942545, acc 0.953125\n",
      "2017-04-03T22:32:29.193581: step 6779, loss 0.0338374, acc 0.984375\n",
      "2017-04-03T22:32:29.404819: step 6780, loss 0.0943097, acc 0.96875\n",
      "2017-04-03T22:32:29.615063: step 6781, loss 0.0281985, acc 1\n",
      "2017-04-03T22:32:29.827282: step 6782, loss 0.0515047, acc 0.984375\n",
      "2017-04-03T22:32:30.038877: step 6783, loss 0.0347753, acc 0.984375\n",
      "2017-04-03T22:32:30.246932: step 6784, loss 0.0430577, acc 1\n",
      "2017-04-03T22:32:30.459616: step 6785, loss 0.00377362, acc 1\n",
      "2017-04-03T22:32:30.669700: step 6786, loss 0.103855, acc 0.953125\n",
      "2017-04-03T22:32:30.885663: step 6787, loss 0.05057, acc 0.984375\n",
      "2017-04-03T22:32:31.093040: step 6788, loss 0.0840349, acc 0.984375\n",
      "2017-04-03T22:32:31.306033: step 6789, loss 0.0769957, acc 0.96875\n",
      "2017-04-03T22:32:31.515885: step 6790, loss 0.0141606, acc 1\n",
      "2017-04-03T22:32:31.723880: step 6791, loss 0.0756171, acc 0.96875\n",
      "2017-04-03T22:32:31.931658: step 6792, loss 0.150534, acc 0.9375\n",
      "2017-04-03T22:32:32.160274: step 6793, loss 0.0196036, acc 1\n",
      "2017-04-03T22:32:32.395296: step 6794, loss 0.0328896, acc 0.984375\n",
      "2017-04-03T22:32:32.605783: step 6795, loss 0.0140978, acc 1\n",
      "2017-04-03T22:32:32.822473: step 6796, loss 0.0651248, acc 0.96875\n",
      "2017-04-03T22:32:33.037601: step 6797, loss 0.292974, acc 0.96875\n",
      "2017-04-03T22:32:33.247845: step 6798, loss 0.0674122, acc 0.96875\n",
      "2017-04-03T22:32:33.456448: step 6799, loss 0.0100436, acc 1\n",
      "2017-04-03T22:32:33.671546: step 6800, loss 0.00463014, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-04-03T22:32:34.389100: step 6800, loss 2.40897, acc 0.561358\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/cnn-text-classification-tf/runs/1491237569/checkpoints/model-6800\n",
      "\n",
      "2017-04-03T22:32:34.688978: step 6801, loss 0.0258219, acc 0.984375\n",
      "2017-04-03T22:32:34.896230: step 6802, loss 0.0213825, acc 1\n",
      "2017-04-03T22:32:35.128561: step 6803, loss 0.0852132, acc 0.96875\n",
      "2017-04-03T22:32:35.306341: step 6804, loss 0.0843768, acc 0.942308\n",
      "2017-04-03T22:32:35.513347: step 6805, loss 0.039681, acc 0.984375\n",
      "2017-04-03T22:32:35.721427: step 6806, loss 0.0738216, acc 0.96875\n",
      "2017-04-03T22:32:35.931262: step 6807, loss 0.00878807, acc 1\n",
      "2017-04-03T22:32:36.139433: step 6808, loss 0.0282003, acc 0.984375\n",
      "2017-04-03T22:32:36.349711: step 6809, loss 0.012041, acc 1\n",
      "2017-04-03T22:32:36.555388: step 6810, loss 0.0245917, acc 0.984375\n",
      "2017-04-03T22:32:36.773178: step 6811, loss 0.0525646, acc 0.984375\n",
      "2017-04-03T22:32:36.986412: step 6812, loss 0.00829033, acc 1\n",
      "2017-04-03T22:32:37.197449: step 6813, loss 0.00492276, acc 1\n",
      "2017-04-03T22:32:37.404063: step 6814, loss 0.0622676, acc 0.984375\n",
      "2017-04-03T22:32:37.616260: step 6815, loss 0.0539962, acc 0.96875\n",
      "2017-04-03T22:32:37.825802: step 6816, loss 0.012772, acc 1\n",
      "2017-04-03T22:32:38.040946: step 6817, loss 0.103149, acc 0.953125\n",
      "2017-04-03T22:32:38.258615: step 6818, loss 0.0386648, acc 0.984375\n",
      "2017-04-03T22:32:38.466369: step 6819, loss 0.00823742, acc 1\n",
      "2017-04-03T22:32:38.679201: step 6820, loss 0.0117203, acc 1\n",
      "2017-04-03T22:32:38.897212: step 6821, loss 0.0186465, acc 1\n",
      "2017-04-03T22:32:39.111611: step 6822, loss 0.00679781, acc 1\n",
      "2017-04-03T22:32:39.325368: step 6823, loss 0.0160596, acc 1\n",
      "2017-04-03T22:32:39.540094: step 6824, loss 0.0731897, acc 0.984375\n",
      "2017-04-03T22:32:39.759913: step 6825, loss 0.00936031, acc 1\n",
      "2017-04-03T22:32:39.970173: step 6826, loss 0.037558, acc 0.984375\n",
      "2017-04-03T22:32:40.179644: step 6827, loss 0.00534824, acc 1\n",
      "2017-04-03T22:32:40.390778: step 6828, loss 0.0680504, acc 0.96875\n",
      "2017-04-03T22:32:40.603744: step 6829, loss 0.0984109, acc 0.953125\n",
      "2017-04-03T22:32:40.815850: step 6830, loss 0.0735616, acc 0.96875\n",
      "2017-04-03T22:32:41.031121: step 6831, loss 0.0124027, acc 1\n",
      "2017-04-03T22:32:41.235933: step 6832, loss 0.0211106, acc 0.984375\n",
      "2017-04-03T22:32:41.445306: step 6833, loss 0.0288082, acc 0.984375\n",
      "2017-04-03T22:32:41.656875: step 6834, loss 0.00243984, acc 1\n",
      "2017-04-03T22:32:41.869123: step 6835, loss 0.0378284, acc 0.984375\n",
      "2017-04-03T22:32:42.079792: step 6836, loss 0.310086, acc 0.96875\n",
      "2017-04-03T22:32:42.285220: step 6837, loss 0.0377059, acc 0.984375\n",
      "2017-04-03T22:32:42.495835: step 6838, loss 0.00107533, acc 1\n",
      "2017-04-03T22:32:42.706270: step 6839, loss 0.0211526, acc 1\n",
      "2017-04-03T22:32:42.920071: step 6840, loss 0.00999627, acc 1\n",
      "2017-04-03T22:32:43.134331: step 6841, loss 0.137997, acc 0.9375\n",
      "2017-04-03T22:32:43.355202: step 6842, loss 0.0120289, acc 1\n",
      "2017-04-03T22:32:43.573932: step 6843, loss 0.0124655, acc 1\n",
      "2017-04-03T22:32:43.781906: step 6844, loss 0.010419, acc 1\n",
      "2017-04-03T22:32:43.990639: step 6845, loss 0.0185671, acc 1\n",
      "2017-04-03T22:32:44.199775: step 6846, loss 0.0142577, acc 1\n",
      "2017-04-03T22:32:44.407465: step 6847, loss 0.00701537, acc 1\n",
      "2017-04-03T22:32:44.618639: step 6848, loss 0.205951, acc 0.984375\n",
      "2017-04-03T22:32:44.828417: step 6849, loss 0.12523, acc 0.921875\n",
      "2017-04-03T22:32:45.040281: step 6850, loss 0.0306805, acc 1\n",
      "2017-04-03T22:32:45.248942: step 6851, loss 0.0345768, acc 1\n",
      "2017-04-03T22:32:45.456585: step 6852, loss 0.0164008, acc 1\n",
      "2017-04-03T22:32:45.666646: step 6853, loss 0.174446, acc 0.96875\n",
      "2017-04-03T22:32:45.888770: step 6854, loss 0.0402312, acc 0.984375\n",
      "2017-04-03T22:32:46.101456: step 6855, loss 0.0394149, acc 0.984375\n",
      "2017-04-03T22:32:46.333048: step 6856, loss 0.0123105, acc 1\n",
      "2017-04-03T22:32:46.539744: step 6857, loss 0.0765499, acc 0.984375\n",
      "2017-04-03T22:32:46.743417: step 6858, loss 0.0128007, acc 1\n",
      "2017-04-03T22:32:46.955814: step 6859, loss 0.0857053, acc 0.96875\n",
      "2017-04-03T22:32:47.166117: step 6860, loss 0.0715179, acc 0.96875\n",
      "2017-04-03T22:32:47.378734: step 6861, loss 0.0711646, acc 0.984375\n",
      "2017-04-03T22:32:47.588980: step 6862, loss 0.0404166, acc 0.984375\n",
      "2017-04-03T22:32:47.797536: step 6863, loss 0.0145128, acc 1\n",
      "2017-04-03T22:32:48.005272: step 6864, loss 0.0158784, acc 1\n",
      "2017-04-03T22:32:48.217619: step 6865, loss 0.0127122, acc 1\n",
      "2017-04-03T22:32:48.425531: step 6866, loss 0.0338846, acc 1\n",
      "2017-04-03T22:32:48.636060: step 6867, loss 0.0120814, acc 1\n",
      "2017-04-03T22:32:48.843103: step 6868, loss 0.116676, acc 0.9375\n",
      "2017-04-03T22:32:49.061548: step 6869, loss 0.0495602, acc 0.984375\n",
      "2017-04-03T22:32:49.269600: step 6870, loss 0.0428772, acc 0.984375\n",
      "2017-04-03T22:32:49.485260: step 6871, loss 0.0531459, acc 0.96875\n",
      "2017-04-03T22:32:49.702529: step 6872, loss 0.0609583, acc 0.96875\n",
      "2017-04-03T22:32:49.925479: step 6873, loss 0.0649089, acc 0.984375\n",
      "2017-04-03T22:32:50.146962: step 6874, loss 0.0111986, acc 1\n",
      "2017-04-03T22:32:50.360453: step 6875, loss 0.0855406, acc 0.984375\n",
      "2017-04-03T22:32:50.582732: step 6876, loss 0.0630911, acc 0.96875\n",
      "2017-04-03T22:32:50.787597: step 6877, loss 0.144805, acc 0.9375\n",
      "2017-04-03T22:32:50.999052: step 6878, loss 0.0575808, acc 0.96875\n",
      "2017-04-03T22:32:51.223301: step 6879, loss 0.0983432, acc 0.953125\n",
      "2017-04-03T22:32:51.437300: step 6880, loss 0.0138232, acc 1\n",
      "2017-04-03T22:32:51.645760: step 6881, loss 0.111822, acc 0.96875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c0eff3b9602e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-c0eff3b9602e>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     76\u001b[0m             _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m     77\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=128,\n",
    "            filter_sizes=[3 4 5],\n",
    "            num_filters=128,\n",
    "            l2_reg_lambda=0.0)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                \n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), 64, 200)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % 100 == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % 100 == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
